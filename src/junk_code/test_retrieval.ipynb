{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from typing import List\n",
    "\n",
    "# Download the punkt tokenizer if not already available\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 300) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk the input text using a sliding window approach with overlap,\n",
    "    while respecting sentence boundaries.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text to be chunked.\n",
    "    chunk_size (int): The target size of each chunk in characters.\n",
    "    overlap (int): The number of characters to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence)\n",
    "\n",
    "        if current_length + sentence_length <= chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            # If the current chunk is not empty, add it to the list of chunks\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            # Start a new chunk, including the overlap\n",
    "            overlap_text = \" \".join(\n",
    "                current_chunk[-2:]\n",
    "            )  # Include last 2 sentences for context\n",
    "            current_chunk = [overlap_text, sentence]\n",
    "            current_length = len(overlap_text) + sentence_length\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 9\n",
      "\n",
      "Chunk 1:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "Chunk 2:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them.\n",
      "\n",
      "Chunk 3:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "\n",
      "Chunk 4:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
      "\n",
      "Chunk 5:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\n",
      "\n",
      "Chunk 6:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data.\n",
      "\n",
      "Chunk 7:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
      "\n",
      "Chunk 8:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature.\n",
      "\n",
      "Chunk 9:\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
    "\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ubuntu/nltk_data'\n    - '/home/ubuntu/miniconda3/envs/nvembed/nltk_data'\n    - '/home/ubuntu/miniconda3/envs/nvembed/share/nltk_data'\n    - '/home/ubuntu/miniconda3/envs/nvembed/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sent_text \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this gives us a list of sentences\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nvembed/lib/python3.9/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/nvembed/lib/python3.9/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nvembed/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nvembed/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/miniconda3/envs/nvembed/lib/python3.9/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ubuntu/nltk_data'\n    - '/home/ubuntu/miniconda3/envs/nvembed/nltk_data'\n    - '/home/ubuntu/miniconda3/envs/nvembed/share/nltk_data'\n    - '/home/ubuntu/miniconda3/envs/nvembed/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sent_text = nltk.sent_tokenize(text)  # this gives us a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = pd.read_excel(\"retriever_eval_nvidia_NV-Embed-v2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>standalone_score</th>\n",
       "      <th>standalone_eval</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>relatedness_eval</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>relevance_eval</th>\n",
       "      <th>Retrieved_Doc_IDs</th>\n",
       "      <th>Retrieved_Contexts</th>\n",
       "      <th>LLM_Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>`tokenizers-linux-x64-musl`\\n\\nThis is the **...</td>\n",
       "      <td>What architecture is the `tokenizers-linux-x64...</td>\n",
       "      <td>x86_64-unknown-linux-musl</td>\n",
       "      <td>huggingface/tokenizers/blob/main/bindings/node...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is asking about the specific arch...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context directly specifies the architectur...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question is asking for specific technical ...</td>\n",
       "      <td>['huggingface/tokenizers/blob/main/bindings/no...</td>\n",
       "      <td>`tokenizers-linux-x64-musl`\\n\\nThis is the **x...</td>\n",
       "      <td>The `tokenizers-linux-x64-musl` binary is desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!--Copyright 2023 The HuggingFace Team. All ri...</td>\n",
       "      <td>What is the purpose of the BLIP-Diffusion mode...</td>\n",
       "      <td>The BLIP-Diffusion model is designed for contr...</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is asking for the purpose of a sp...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context provides a detailed description of...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question asks about the purpose of the BLI...</td>\n",
       "      <td>['huggingface/diffusers/blob/main/docs/source/...</td>\n",
       "      <td>!--Copyright 2023 The HuggingFace Team. All ri...</td>\n",
       "      <td>According to the provided context, the purpose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paper Pages\\n\\nPaper pages allow people to fi...</td>\n",
       "      <td>How can a user claim authorship of a paper on ...</td>\n",
       "      <td>By clicking their name on the corresponding Pa...</td>\n",
       "      <td>huggingface/hub-docs/blob/main/docs/hub/paper-...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is clear and does not depend on a...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context provides a clear explanation of ho...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question is specific to the Hugging Face H...</td>\n",
       "      <td>['huggingface/hub-docs/blob/main/docs/hub/pape...</td>\n",
       "      <td>If your paper is not linked to your account, y...</td>\n",
       "      <td>To claim authorship of a paper on the Hugging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Datasets server API\\n\\n&gt; API on 🤗 datasets\\n\\...</td>\n",
       "      <td>What is the purpose of the /healthcheck endpoi...</td>\n",
       "      <td>Ensure the app is running</td>\n",
       "      <td>huggingface/datasets-server/blob/main/services...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is asking for the purpose of a sp...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context directly states the purpose of the...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is specific and technical, asking...</td>\n",
       "      <td>['huggingface/datasets-server/blob/main/servic...</td>\n",
       "      <td>Datasets server API\\n\\n&gt; API on 🤗 datasets\\n\\n...</td>\n",
       "      <td>According to the provided context, the `/healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!--Copyright 2022 The HuggingFace Team. All ri...</td>\n",
       "      <td>What is the default context window size for Lo...</td>\n",
       "      <td>127 tokens</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is asking for a specific paramete...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context provides a specific detail about t...</td>\n",
       "      <td>3</td>\n",
       "      <td>This question is specific and technical, askin...</td>\n",
       "      <td>['huggingface/transformers/blob/main/docs/sour...</td>\n",
       "      <td>- [`LongT5ForConditionalGeneration`] is an ext...</td>\n",
       "      <td>The default context window size for Local Atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>!--Copyright 2022 The HuggingFace Team. All ri...</td>\n",
       "      <td>What is the maximum size of a model checkpoint...</td>\n",
       "      <td>10GB</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is asking for a specific piece of...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context explicitly states that since versi...</td>\n",
       "      <td>3</td>\n",
       "      <td>This question is specific and technical, focus...</td>\n",
       "      <td>['huggingface/transformers/blob/main/docs/sour...</td>\n",
       "      <td>&lt;/Tip&gt;\\n\\nIn this guide, we explore the soluti...</td>\n",
       "      <td>According to the provided context, the maximum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Gradio and W&amp;B Integration\\n\\nRelated spaces:...</td>\n",
       "      <td>What is the purpose of Weights and Biases (W&amp;B...</td>\n",
       "      <td>To track their machine learning experiments at...</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/06_integrat...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question asks about the purpose of a speci...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context provides a clear explanation of wh...</td>\n",
       "      <td>4</td>\n",
       "      <td>The question is asking about the purpose of We...</td>\n",
       "      <td>['gradio-app/gradio/blob/main/guides/06_integr...</td>\n",
       "      <td>Gradio and W&amp;B Integration\\n\\nRelated spaces: ...</td>\n",
       "      <td>According to the context, Weights and Biases (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>--\\ntitle: \"Intel and Hugging Face Partner to ...</td>\n",
       "      <td>What is the name of the open-source library cr...</td>\n",
       "      <td>Optimum</td>\n",
       "      <td>huggingface/blog/blob/main/intel.md</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is asking for the name of a speci...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context provided includes a detailed expla...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question asks for the name of a specific o...</td>\n",
       "      <td>['huggingface/blog/blob/main/intel.md', 'huggi...</td>\n",
       "      <td>“*We’re excited to work with Hugging Face to b...</td>\n",
       "      <td>The name of the open-source library created by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>控制布局 (Controlling Layout)\\n\\n默认情况下，块中的组件是垂直排列...</td>\n",
       "      <td>What parameter is used to ensure that elements...</td>\n",
       "      <td>equal_height</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/cn/03_build...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is clear and does not rely on a s...</td>\n",
       "      <td>5</td>\n",
       "      <td>The context provides a clear explanation of ho...</td>\n",
       "      <td>3</td>\n",
       "      <td>The question is specific to the Gradio library...</td>\n",
       "      <td>['huggingface/course/blob/main/chapters/en/cha...</td>\n",
       "      <td>Here's what you should keep in mind: any compo...</td>\n",
       "      <td>According to the text, the `equal_height` para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>!--Copyright 2023 The HuggingFace Team. All ri...</td>\n",
       "      <td>What is the command to install the latest vers...</td>\n",
       "      <td>pip install --upgrade-strategy eager optimum[\"...</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en...</td>\n",
       "      <td>5</td>\n",
       "      <td>The question is clear and context-independent....</td>\n",
       "      <td>5</td>\n",
       "      <td>The context provides the exact command needed ...</td>\n",
       "      <td>3</td>\n",
       "      <td>This question is quite specific and technical,...</td>\n",
       "      <td>['huggingface/optimum/blob/main/docs/source/no...</td>\n",
       "      <td>## Optimum Intel\\n\\n### OpenVINO\\n\\n!--Copyrig...</td>\n",
       "      <td>The command to install the latest version of O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context  \\\n",
       "0    `tokenizers-linux-x64-musl`\\n\\nThis is the **...   \n",
       "1   !--Copyright 2023 The HuggingFace Team. All ri...   \n",
       "2    Paper Pages\\n\\nPaper pages allow people to fi...   \n",
       "3    Datasets server API\\n\\n> API on 🤗 datasets\\n\\...   \n",
       "4   !--Copyright 2022 The HuggingFace Team. All ri...   \n",
       "..                                                ...   \n",
       "60  !--Copyright 2022 The HuggingFace Team. All ri...   \n",
       "61   Gradio and W&B Integration\\n\\nRelated spaces:...   \n",
       "62  --\\ntitle: \"Intel and Hugging Face Partner to ...   \n",
       "63   控制布局 (Controlling Layout)\\n\\n默认情况下，块中的组件是垂直排列...   \n",
       "64  !--Copyright 2023 The HuggingFace Team. All ri...   \n",
       "\n",
       "                                             question  \\\n",
       "0   What architecture is the `tokenizers-linux-x64...   \n",
       "1   What is the purpose of the BLIP-Diffusion mode...   \n",
       "2   How can a user claim authorship of a paper on ...   \n",
       "3   What is the purpose of the /healthcheck endpoi...   \n",
       "4   What is the default context window size for Lo...   \n",
       "..                                                ...   \n",
       "60  What is the maximum size of a model checkpoint...   \n",
       "61  What is the purpose of Weights and Biases (W&B...   \n",
       "62  What is the name of the open-source library cr...   \n",
       "63  What parameter is used to ensure that elements...   \n",
       "64  What is the command to install the latest vers...   \n",
       "\n",
       "                                               answer  \\\n",
       "0                           x86_64-unknown-linux-musl   \n",
       "1   The BLIP-Diffusion model is designed for contr...   \n",
       "2   By clicking their name on the corresponding Pa...   \n",
       "3                           Ensure the app is running   \n",
       "4                                          127 tokens   \n",
       "..                                                ...   \n",
       "60                                               10GB   \n",
       "61  To track their machine learning experiments at...   \n",
       "62                                            Optimum   \n",
       "63                                       equal_height   \n",
       "64  pip install --upgrade-strategy eager optimum[\"...   \n",
       "\n",
       "                                           source_doc  standalone_score  \\\n",
       "0   huggingface/tokenizers/blob/main/bindings/node...                 5   \n",
       "1   huggingface/diffusers/blob/main/docs/source/en...                 5   \n",
       "2   huggingface/hub-docs/blob/main/docs/hub/paper-...                 5   \n",
       "3   huggingface/datasets-server/blob/main/services...                 5   \n",
       "4   huggingface/transformers/blob/main/docs/source...                 5   \n",
       "..                                                ...               ...   \n",
       "60  huggingface/transformers/blob/main/docs/source...                 5   \n",
       "61  gradio-app/gradio/blob/main/guides/06_integrat...                 5   \n",
       "62                huggingface/blog/blob/main/intel.md                 5   \n",
       "63  gradio-app/gradio/blob/main/guides/cn/03_build...                 5   \n",
       "64  huggingface/diffusers/blob/main/docs/source/en...                 5   \n",
       "\n",
       "                                      standalone_eval  relatedness_score  \\\n",
       "0   The question is asking about the specific arch...                  5   \n",
       "1   The question is asking for the purpose of a sp...                  5   \n",
       "2   The question is clear and does not depend on a...                  5   \n",
       "3   The question is asking for the purpose of a sp...                  5   \n",
       "4   The question is asking for a specific paramete...                  5   \n",
       "..                                                ...                ...   \n",
       "60  The question is asking for a specific piece of...                  5   \n",
       "61  The question asks about the purpose of a speci...                  5   \n",
       "62  The question is asking for the name of a speci...                  5   \n",
       "63  The question is clear and does not rely on a s...                  5   \n",
       "64  The question is clear and context-independent....                  5   \n",
       "\n",
       "                                     relatedness_eval  relevance_score  \\\n",
       "0   The context directly specifies the architectur...                3   \n",
       "1   The context provides a detailed description of...                3   \n",
       "2   The context provides a clear explanation of ho...                3   \n",
       "3   The context directly states the purpose of the...                4   \n",
       "4   The context provides a specific detail about t...                3   \n",
       "..                                                ...              ...   \n",
       "60  The context explicitly states that since versi...                3   \n",
       "61  The context provides a clear explanation of wh...                4   \n",
       "62  The context provided includes a detailed expla...                3   \n",
       "63  The context provides a clear explanation of ho...                3   \n",
       "64  The context provides the exact command needed ...                3   \n",
       "\n",
       "                                       relevance_eval  \\\n",
       "0   The question is asking for specific technical ...   \n",
       "1   The question asks about the purpose of the BLI...   \n",
       "2   The question is specific to the Hugging Face H...   \n",
       "3   The question is specific and technical, asking...   \n",
       "4   This question is specific and technical, askin...   \n",
       "..                                                ...   \n",
       "60  This question is specific and technical, focus...   \n",
       "61  The question is asking about the purpose of We...   \n",
       "62  The question asks for the name of a specific o...   \n",
       "63  The question is specific to the Gradio library...   \n",
       "64  This question is quite specific and technical,...   \n",
       "\n",
       "                                    Retrieved_Doc_IDs  \\\n",
       "0   ['huggingface/tokenizers/blob/main/bindings/no...   \n",
       "1   ['huggingface/diffusers/blob/main/docs/source/...   \n",
       "2   ['huggingface/hub-docs/blob/main/docs/hub/pape...   \n",
       "3   ['huggingface/datasets-server/blob/main/servic...   \n",
       "4   ['huggingface/transformers/blob/main/docs/sour...   \n",
       "..                                                ...   \n",
       "60  ['huggingface/transformers/blob/main/docs/sour...   \n",
       "61  ['gradio-app/gradio/blob/main/guides/06_integr...   \n",
       "62  ['huggingface/blog/blob/main/intel.md', 'huggi...   \n",
       "63  ['huggingface/course/blob/main/chapters/en/cha...   \n",
       "64  ['huggingface/optimum/blob/main/docs/source/no...   \n",
       "\n",
       "                                   Retrieved_Contexts  \\\n",
       "0   `tokenizers-linux-x64-musl`\\n\\nThis is the **x...   \n",
       "1   !--Copyright 2023 The HuggingFace Team. All ri...   \n",
       "2   If your paper is not linked to your account, y...   \n",
       "3   Datasets server API\\n\\n> API on 🤗 datasets\\n\\n...   \n",
       "4   - [`LongT5ForConditionalGeneration`] is an ext...   \n",
       "..                                                ...   \n",
       "60  </Tip>\\n\\nIn this guide, we explore the soluti...   \n",
       "61  Gradio and W&B Integration\\n\\nRelated spaces: ...   \n",
       "62  “*We’re excited to work with Hugging Face to b...   \n",
       "63  Here's what you should keep in mind: any compo...   \n",
       "64  ## Optimum Intel\\n\\n### OpenVINO\\n\\n!--Copyrig...   \n",
       "\n",
       "                                           LLM_Answer  \n",
       "0   The `tokenizers-linux-x64-musl` binary is desi...  \n",
       "1   According to the provided context, the purpose...  \n",
       "2   To claim authorship of a paper on the Hugging ...  \n",
       "3   According to the provided context, the `/healt...  \n",
       "4   The default context window size for Local Atte...  \n",
       "..                                                ...  \n",
       "60  According to the provided context, the maximum...  \n",
       "61  According to the context, Weights and Biases (...  \n",
       "62  The name of the open-source library created by...  \n",
       "63  According to the text, the `equal_height` para...  \n",
       "64  The command to install the latest version of O...  \n",
       "\n",
       "[65 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md\n",
      "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "\n",
      "x86_64-unknown-linux-musl\n",
      "x86_64-unknown-linux-musl\n",
      "-----------------Context------------\n",
      "['huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md', 'huggingface/course/blob/main/chapters/en/chapter2/4.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx']\n",
      "`tokenizers-linux-x64-musl`\n",
      "\n",
      "This is the **x86_64-unknown-linux-musl** binary for `tokenizers`\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.\n",
      "Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\n",
      "the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\n",
      "can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\n",
      "removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\n",
      "sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\n",
      "operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\n",
      "minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\n",
      "training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\n",
      "counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\n",
      "tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\n",
      "pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\n",
      "experiments.*\n",
      "\n",
      "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\n",
      "found [here](https://github.com/google-research/byt5).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "ByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\n",
      "only differ in how inputs should be prepared for the model, see the code examples below.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "FrameworkSwitchCourse {fw} />\n",
      "\n",
      "# Tokenizers[[tokenizers]]\n",
      "\n",
      "{#if fw === 'pt'}\n",
      "\n",
      "<CourseFloatingBanner chapter={2}\n",
      "  classNames=\"absolute z-10 right-0 top-0\"\n",
      "  notebooks={[\n",
      "    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb\"},\n",
      "    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb\"},\n",
      "]} />\n",
      "\n",
      "{:else}\n",
      "\n",
      "<CourseFloatingBanner chapter={2}\n",
      "  classNames=\"absolute z-10 right-0 top-0\"\n",
      "  notebooks={[\n",
      "    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\n",
      "    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\n",
      "]} />\n",
      "\n",
      "{/if}\n",
      "\n",
      "<Youtube id=\"VFp38yj8h3A\"/>\n",
      "\n",
      "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \n",
      "\n",
      "In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n",
      "\n",
      "```\n",
      "Jim Henson was a puppeteer\n",
      "```\n",
      "\n",
      "However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation.\n",
      "\n",
      "Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.\n",
      "\n",
      "## Word-based[[word-based]]\n",
      "\n",
      "<Youtube id=\"nhJxYji1aho\"/>\n",
      "\n",
      "## Models\n",
      "\n",
      "Models are the core algorithms used to actually tokenize, and therefore,\n",
      "they are the only mandatory component of a Tokenizer.\n",
      "\n",
      "| Name | Description |\n",
      "| :--- | :--- |\n",
      "| WordLevel | This is the “classic” tokenization algorithm. It let’s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs.  |\n",
      "| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having “unk” (unknown) tokens.  |\n",
      "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\n",
      "| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\n",
      "\n",
      "## Post-Processors\n",
      "\n",
      "<tokenizerslangcontent>\n",
      "<python>\n",
      "| Name | Description | Example |\n",
      "| :--- | :--- | :--- |\n",
      "| ByteLevel | Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties: <ul> <li>Since it maps on bytes, a tokenizer using this only requires **256** characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.</li> <li>A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! 🎉🎉)</li> <li>For non ascii characters, it gets completely unreadable, but it works nonetheless!</li> </ul> | Input: `\"Hello my friend, how are you?\"` <br> Ouput: `\"Hello\", \"Ġmy\", Ġfriend\", \",\", \"Ġhow\", \"Ġare\", \"Ġyou\", \"?\"`  |\n",
      "| Whitespace | Splits on word boundaries (using the following regular expression: `\\w+&#124;[^\\w\\s]+` | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there\", \"!\"`  |\n",
      "| WhitespaceSplit | Splits on any whitespace character | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there!\"`  |\n",
      "| Punctuation | Will isolate all punctuation characters | Input: `\"Hello?\"` <br> Ouput: `\"Hello\", \"?\"`  |\n",
      "| Metaspace | Splits on whitespaces and replaces them with a special char “▁” (U+2581) | Input: `\"Hello there\"` <br> Ouput: `\"Hello\", \"▁there\"`  |\n",
      "| CharDelimiterSplit | Splits on a given character | Example with `x`: <br> Input: `\"Helloxthere\"` <br> Ouput: `\"Hello\", \"there\"`  |\n",
      "| Digits | Splits the numbers from any other characters. | Input: `\"Hello123there\"` <br>  Output: ``\"Hello\", \"123\", \"there\"``  |\n",
      "-----------------LLm ANSWER------------\n",
      "The `tokenizers-linux-x64-musl` binary is designed for the Transformer architecture.\n",
      "-----------------------------\n",
      "1\n",
      "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md\n",
      "What is the purpose of the BLIP-Diffusion model?\n",
      "\n",
      "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n",
      "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# BLIP-Diffusion\n",
      "\n",
      "BLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n",
      "\n",
      "\n",
      "The abstract from the paper is:\n",
      "\n",
      "The abstract from the paper is:\n",
      "\n",
      "*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*\n",
      "\n",
      "The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\n",
      "\n",
      "`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "\n",
      "## BlipDiffusionPipeline\n",
      "[[autodoc]] BlipDiffusionPipeline\n",
      "    - all\n",
      "    - __call__\n",
      "\n",
      "## BlipDiffusionControlNetPipeline\n",
      "[[autodoc]] BlipDiffusionControlNetPipeline\n",
      "    - all\n",
      "    - __call__\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# unCLIP\n",
      "\n",
      "[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo).\n",
      "\n",
      "The abstract from the paper is following:\n",
      "\n",
      "*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Philosophy\n",
      "\n",
      "🧨 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\n",
      "Its purpose is to serve as a **modular toolbox** for both inference and training.\n",
      "\n",
      "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
      "\n",
      "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n",
      "\n",
      "## Usability over Performance\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided context, the purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation. It is a new subject-driven image generation model that supports multimodal control and consumes inputs of subject images and text prompts.\n",
      "-----------------------------\n",
      "2\n",
      "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md\n",
      "How can a user claim authorship of a paper on the Hugging Face Hub?\n",
      "\n",
      "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n",
      "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n",
      "-----------------Context------------\n",
      "['huggingface/hub-docs/blob/main/docs/hub/paper-pages.md', 'huggingface/hub-docs/blob/main/docs/hub/paper-pages.md', 'huggingface/blog/blob/main/spacy.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md']\n",
      "If your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n",
      "<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n",
      "</div>\n",
      "\n",
      "\n",
      "## Frequently Asked Questions\n",
      "\n",
      "### Can I control which Paper pages show in my profile?\n",
      "\n",
      "Yes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile. \n",
      "\n",
      "### Do you support ACL anthology?\n",
      "\n",
      "We're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n",
      "\n",
      "### Can I have a Paper page even if I have no model/dataset/Space?\n",
      "\n",
      "Yes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index.\n",
      "\n",
      "Paper Pages\n",
      "\n",
      "Paper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png\"/>\n",
      "<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png\"/>\n",
      "</div>\n",
      "\n",
      "## Linking a Paper to a model, dataset or Space\n",
      "\n",
      "If the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n",
      "\n",
      "* Visit the Paper page.\n",
      "* Filter for other models or datasets on the Hub that cite the same paper.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n",
      "<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n",
      "</div>\n",
      "\n",
      "## Claiming authorship to a Paper\n",
      "\n",
      "The Hub will attempt to automatically match paper to users based on their email. \n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n",
      "<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n",
      "</div>\n",
      "\n",
      "Try it out and share your models with the community!\n",
      "\n",
      "## Would you like to integrate your library to the Hub?\n",
      "\n",
      "This integration is possible thanks to the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you!\n",
      "\n",
      "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Interact with Discussions and Pull Requests \n",
      "\n",
      "The `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\n",
      "Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\n",
      "for a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n",
      "\n",
      "## Retrieve Discussions and Pull Requests from the Hub\n",
      "\n",
      "The `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import get_repo_discussions\n",
      ">>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n",
      "...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")\n",
      "\n",
      "# 11 - Add Flax weights, pr: True\n",
      "# 10 - Update README.md, pr: True\n",
      "# 9 - Training languages in the model card, pr: True\n",
      "# 8 - Update tokenizer_config.json, pr: True\n",
      "# 7 - Slurm training script, pr: False\n",
      "[...]\n",
      "```\n",
      "\n",
      "`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import get_repo_discussions\n",
      ">>> for discussion in get_repo_discussions(\n",
      "...    repo_id=\"bigscience/bloom\",\n",
      "...    author=\"ArthurZ\",\n",
      "...    discussion_type=\"pull_request\",\n",
      "...    discussion_status=\"open\",\n",
      "... ):\n",
      "...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")\n",
      "\n",
      "# 19 - Add Flax weights by ArthurZ, pr: True\n",
      "```\n",
      "\n",
      "`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\n",
      "[`Discussion`] objects. To get all the Discussions in a single list, run:\n",
      "\n",
      "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Create and manage a repository\n",
      "\n",
      "The Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\n",
      "development to easily version projects when working collaboratively. This guide will show you how to interact with the\n",
      "repositories on the Hub, especially:\n",
      "\n",
      "- Create and delete a repository.\n",
      "- Manage branches and tags. \n",
      "- Rename your repository.\n",
      "- Update your repository visibility.\n",
      "- Manage a local copy of your repository.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\n",
      "might be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n",
      "(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\n",
      "not share the same requirements and workflows. Model repositories might maintain large model weight files for different\n",
      "frameworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\n",
      "a result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\n",
      "explanation page for more details.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n",
      "[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\n",
      "\n",
      "## Repo creation and deletion\n",
      "\n",
      "The first step is to know how to create and delete repositories. You can only manage repositories that you own (under\n",
      "your username namespace) or from organizations in which you have write permissions.\n",
      "\n",
      "### Create a repository\n",
      "-----------------LLm ANSWER------------\n",
      "To claim authorship of a paper on the Hugging Face Hub, you can click on your name in the corresponding Paper page and click \"claim authorship\". This will automatically redirect you to your paper settings where you can confirm the request. Once confirmed, the Paper page will show as verified.\n",
      "-----------------------------\n",
      "3\n",
      "huggingface/datasets-server/blob/main/services/api/README.md\n",
      "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n",
      "\n",
      "Ensure the app is running\n",
      "Ensure the app is running\n",
      "-----------------Context------------\n",
      "['huggingface/datasets-server/blob/main/services/api/README.md', 'huggingface/datasets-server/blob/main/docs/source/splits.mdx', 'huggingface/datasets-server/blob/main/docs/source/splits.mdx', 'huggingface/course/blob/main/chapters/en/chapter5/1.mdx', 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx']\n",
      "Datasets server API\n",
      "\n",
      "> API on 🤗 datasets\n",
      "\n",
      "## Configuration\n",
      "\n",
      "The service can be configured using environment variables. They are grouped by scope.\n",
      "\n",
      "### API service\n",
      "\n",
      "See [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n",
      "\n",
      "### Common\n",
      "\n",
      "See [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n",
      "\n",
      "## Endpoints\n",
      "\n",
      "See https://huggingface.co/docs/datasets-server\n",
      "\n",
      "- /healthcheck: Ensure the app is running\n",
      "- /metrics: Return a list of metrics in the Prometheus format\n",
      "- /webhook: Add, update or remove a dataset\n",
      "- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n",
      "- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n",
      "- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n",
      "- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset\n",
      "\n",
      "The `/splits` endpoint accepts the dataset name as its query parameter:\n",
      "\n",
      "<inferencesnippet>\n",
      "<python>\n",
      "```python\n",
      "import requests\n",
      "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
      "API_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\n",
      "def query():\n",
      "    response = requests.get(API_URL, headers=headers)\n",
      "    return response.json()\n",
      "data = query()\n",
      "```\n",
      "</python>\n",
      "<js>\n",
      "```js\n",
      "import fetch from \"node-fetch\";\n",
      "async function query(data) {\n",
      "    const response = await fetch(\n",
      "        \"https://datasets-server.huggingface.co/splits?dataset=duorc\",\n",
      "        {\n",
      "            headers: { Authorization: `Bearer ${API_TOKEN}` },\n",
      "            method: \"GET\"\n",
      "        }\n",
      "    );\n",
      "    const result = await response.json();\n",
      "    return result;\n",
      "}\n",
      "query().then((response) => {\n",
      "    console.log(JSON.stringify(response));\n",
      "});\n",
      "```\n",
      "</js>\n",
      "<curl>\n",
      "```curl\n",
      "curl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n",
      "        -X GET \\\n",
      "        -H \"Authorization: Bearer ${API_TOKEN}\"\n",
      "```\n",
      "</curl>\n",
      "</inferencesnippet>\n",
      "\n",
      "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"splits\": [\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\n",
      "  ],\n",
      "  \"pending\": [],\n",
      "  \"failed\": []\n",
      "}\n",
      "```\n",
      "\n",
      "List splits and configurations\n",
      "\n",
      "Datasets typically have splits and may also have configurations. A _split_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A _configuration_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load_hub)!\n",
      "\n",
      "This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\n",
      "\n",
      "The `/splits` endpoint accepts the dataset name as its query parameter:\n",
      "\n",
      "Introduction[[introduction]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={5}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "In [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n",
      "\n",
      "1. Load a dataset from the Hugging Face Hub.\n",
      "2. Preprocess the data with `Dataset.map()`.\n",
      "3. Load and compute metrics.\n",
      "\n",
      "But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n",
      "\n",
      "* What do you do when your dataset is not on the Hub?\n",
      "* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n",
      "* What do you do when your dataset is huge and will melt your laptop's RAM?\n",
      "* What the heck are \"memory mapping\" and Apache Arrow?\n",
      "* How can you create your own dataset and push it to the Hub?\n",
      "\n",
      "The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!\n",
      "\n",
      "Access and read Logs\n",
      "\n",
      "Hugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your Endpoint. \n",
      "\n",
      "You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n",
      "\n",
      "The Container Logs are only available when your Endpoint is in the “Running” state. \n",
      "\n",
      "_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n",
      "\n",
      "**Build Logs:**\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n",
      "\n",
      "**Container Logs:**\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided context, the `/healthcheck` endpoint in the Datasets server API is used to \"Ensure the app is running\". This endpoint is likely used for monitoring and maintenance purposes, such as checking the status of the API service or detecting issues with the application.\n",
      "-----------------------------\n",
      "4\n",
      "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md\n",
      "What is the default context window size for Local Attention in the LongT5 model?\n",
      "\n",
      "127 tokens\n",
      "127 tokens\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md']\n",
      "- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\n",
      "encoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n",
      "- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\n",
      "inspired by the pre-training of [`PegasusForConditionalGeneration`].\n",
      "- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\n",
      "input sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n",
      "- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\n",
      "tokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\n",
      "to the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n",
      "- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\n",
      "interact with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\n",
      "length `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\n",
      "in the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\n",
      "also every global token like in the case of standard global attention (*transient* represents the fact the global tokens\n",
      "are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\n",
      "a few new parameters -- global relative position biases and a layer normalization for global token's embedding.\n",
      "The complexity of this mechanism is `O(l(r + l/k))`.\n",
      "\n",
      "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# LongT5\n",
      "\n",
      "## Overview\n",
      "\n",
      "The LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\n",
      "by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\n",
      "encoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\n",
      "T5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\n",
      "Transient-Global attention.\n",
      "\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\n",
      "performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\n",
      "explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\n",
      "attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n",
      "(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n",
      "(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\n",
      "able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\n",
      "question answering tasks.*\n",
      "\n",
      "This model was contributed by [stancld](https://huggingface.co/stancld).\n",
      "The original code can be found [here](https://github.com/google-research/longt5).\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "```python\n",
      ">>> import evaluate\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
      "\n",
      ">>> dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n",
      ">>> model = (\n",
      "...     LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n",
      "...     .to(\"cuda\")\n",
      "...     .half()\n",
      "... )\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n",
      "\n",
      "\n",
      ">>> def generate_answers(batch):\n",
      "...     inputs_dict = tokenizer(\n",
      "...         batch[\"article\"], max_length=16384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
      "...     )\n",
      "...     input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
      "...     attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
      "...     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)\n",
      "...     batch[\"predicted_abstract\"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
      "...     return batch\n",
      "\n",
      "\n",
      ">>> result = dataset.map(generate_answer, batched=True, batch_size=2)\n",
      ">>> rouge = evaluate.load(\"rouge\")\n",
      ">>> rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])\n",
      "```\n",
      "\n",
      "\n",
      "## Resources\n",
      "\n",
      "- [Translation task guide](../tasks/translation)\n",
      "- [Summarization task guide](../tasks/summarization)\n",
      "\n",
      "## LongT5Config\n",
      "\n",
      "[[autodoc]] LongT5Config\n",
      "\n",
      "<frameworkcontent>\n",
      "<pt>\n",
      "\n",
      "## LongT5Model\n",
      "\n",
      "[[autodoc]] LongT5Model\n",
      "    - forward\n",
      "\n",
      "## LongT5ForConditionalGeneration\n",
      "\n",
      "[[autodoc]] LongT5ForConditionalGeneration\n",
      "    - forward\n",
      "\n",
      "## LongT5EncoderModel\n",
      "\n",
      "[[autodoc]] LongT5EncoderModel\n",
      "    - forward\n",
      "\n",
      "</pt>\n",
      "<jax>\n",
      "\n",
      "## FlaxLongT5Model\n",
      "\n",
      "[[autodoc]] FlaxLongT5Model\n",
      "    - __call__\n",
      "    - encode\n",
      "    - decode\n",
      "\n",
      "## FlaxLongT5ForConditionalGeneration\n",
      "\n",
      "[[autodoc]] FlaxLongT5ForConditionalGeneration\n",
      "    - __call__\n",
      "    - encode\n",
      "    - decode\n",
      "\n",
      "</jax>\n",
      "</frameworkcontent>\n",
      "\n",
      "a few new parameters -- global relative position biases and a layer normalization for global token's embedding.\n",
      "The complexity of this mechanism is `O(l(r + l/k))`.\n",
      "- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below.\n",
      "-----------------LLm ANSWER------------\n",
      "The default context window size for Local Attention in the LongT5 model is `r=127`.\n",
      "-----------------------------\n",
      "5\n",
      "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md\n",
      "What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "\n",
      "from_pretrained()\n",
      "from_pretrained()\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# AutoPipeline\n",
      "\n",
      "`AutoPipeline` is designed to:\n",
      "\n",
      "1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n",
      "2. use multiple pipelines in your workflow\n",
      "\n",
      "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n",
      "\n",
      "To seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n",
      "\n",
      "```py\n",
      "from diffusers import AutoPipelineForText2Image\n",
      "import torch\n",
      "\n",
      "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
      "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
      ").to(\"cuda\")\n",
      "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
      "\n",
      "image = pipeline(prompt, num_inference_steps=25).images[0]\n",
      "```\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Check out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n",
      "\n",
      "</Tip>\n",
      "\n",
      "`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n",
      "\n",
      "- [Stable Diffusion](./stable_diffusion/overview)\n",
      "- [ControlNet](./controlnet)\n",
      "- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n",
      "- [DeepFloyd IF](./deepfloyd_if)\n",
      "- [Kandinsky 2.1](./kandinsky)\n",
      "- [Kandinsky 2.2](./kandinsky_v22)\n",
      "\n",
      "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n",
      "\n",
      "```py\n",
      ">>> from transformers.modeling_utils import load_sharded_checkpoint\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     load_sharded_checkpoint(model, tmp_dir)\n",
      "```\n",
      "\n",
      "## Low memory loading\n",
      "\n",
      "Sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n",
      "\n",
      "Please read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
      "\n",
      "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n",
      "\n",
      "```py\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     new_model = AutoModel.from_pretrained(tmp_dir)\n",
      "```\n",
      "\n",
      "The main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n",
      "\n",
      "Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n",
      "\n",
      "```py\n",
      ">>> import json\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n",
      "...         index = json.load(f)\n",
      "\n",
      ">>> print(index.keys())\n",
      "dict_keys(['metadata', 'weight_map'])\n",
      "```\n",
      "\n",
      "The metadata just consists of the total size of the model for now. We plan to add other information in the future:\n",
      "\n",
      "```py\n",
      ">>> index[\"metadata\"]\n",
      "{'total_size': 433245184}\n",
      "```\n",
      "\n",
      "The weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n",
      "\n",
      "```py\n",
      ">>> index[\"weight_map\"]\n",
      "{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n",
      " 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n",
      " ...\n",
      "```\n",
      "\n",
      "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n",
      "\n",
      "</Tip>\n",
      "\n",
      "In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n",
      "\n",
      "## Sharded checkpoints\n",
      "\n",
      "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n",
      "\n",
      "You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n",
      "\n",
      "```py\n",
      "from transformers import AutoModel\n",
      "\n",
      "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
      "```\n",
      "\n",
      "If you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n",
      "\n",
      "```py\n",
      ">>> import os\n",
      ">>> import tempfile\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir)\n",
      "...     print(sorted(os.listdir(tmp_dir)))\n",
      "['config.json', 'pytorch_model.bin']\n",
      "```\n",
      "\n",
      "Now let's use a maximum shard size of 200MB:\n",
      "\n",
      "```py\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     print(sorted(os.listdir(tmp_dir)))\n",
      "['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n",
      "```\n",
      "\n",
      "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n",
      "\n",
      "if training_args.gradient_checkpointing:\n",
      "    model.gradient_checkpointing_enable()\n",
      "\n",
      "accelerator = Accelerator(fp16=training_args.fp16)\n",
      "model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n",
      "\n",
      "model.train()\n",
      "for step, batch in enumerate(dataloader, start=1):\n",
      "    loss = model(**batch).loss\n",
      "    loss = loss / training_args.gradient_accumulation_steps\n",
      "    accelerator.backward(loss)\n",
      "    if step % training_args.gradient_accumulation_steps == 0:\n",
      "        optimizer.step()\n",
      "        optimizer.zero_grad()\n",
      "```\n",
      "\n",
      "First we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \n",
      "Then we can enable gradient checkpointing by calling the model's [`~PreTrainedModel.gradient_checkpointing_enable`] method. \n",
      "When we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator) \n",
      "we can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. \n",
      "During the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare) \n",
      "call the dataloader will also be distributed across workers should we use multiple GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example.\n",
      "\n",
      "Finally, we can add the main training loop. Note that the `backward` call is handled by 🤗 Accelerate. We can also see\n",
      "how gradient accumulation works: we normalize the loss, so we get the average at the end of accumulation and once we have \n",
      "enough steps we run the optimization. \n",
      "\n",
      "Implementing these optimization techniques with 🤗 Accelerate only takes a handful of lines of code and comes with the \n",
      "benefit of more flexibility in the training loop. For a full documentation of all features have a look at the \n",
      "[Accelerate documentation](https://huggingface.co/docs/accelerate/index).\n",
      "\n",
      "\n",
      "## Efficient Software Prebuilds\n",
      "-----------------LLm ANSWER------------\n",
      "The method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method. This method allows you to load a checkpoint for a task without knowing the specific pipeline class to use.\n",
      "-----------------------------\n",
      "6\n",
      "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md\n",
      "What is the purpose of Diffusers library?\n",
      "\n",
      "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n",
      "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Philosophy\n",
      "\n",
      "🧨 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\n",
      "Its purpose is to serve as a **modular toolbox** for both inference and training.\n",
      "\n",
      "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
      "\n",
      "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n",
      "\n",
      "## Usability over Performance\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Overview\n",
      "\n",
      "Welcome to 🧨 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is meant to be used.\n",
      "\n",
      "You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want.\n",
      "\n",
      "After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\n",
      "\n",
      "Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\n",
      "\n",
      "Let's start diffusing! 🧨\n",
      "\n",
      "At Hugging Face, we call this design the **single-file policy** which means that almost all of the code of a certain class should be written in a single, self-contained file. To read more about the philosophy, you can have a look\n",
      "at [this blog post](https://huggingface.co/blog/transformers-design-philosophy).\n",
      "\n",
      "In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffusion models. The reason we don't follow this design fully for diffusion models is because almost all diffusion pipelines, such\n",
      "as [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm), [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview#stable-diffusion-pipelines), [unCLIP (DALL·E 2)](https://huggingface.co/docs/diffusers/api/pipelines/unclip) and [Imagen](https://imagen.research.google/) all rely on the same diffusion model, the [UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond).\n",
      "\n",
      "Great, now you should have generally understood why 🧨 Diffusers is designed the way it is 🤗.\n",
      "We try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would ❤️  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\n",
      "\n",
      "## Design Philosophy in Details\n",
      "\n",
      "Now, let's look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\n",
      "Let's walk through more in-detail design decisions for each class.\n",
      "\n",
      "### Pipelines\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# 🧨 Diffusers’ Ethical Guidelines\n",
      "\n",
      "## Preamble\n",
      "\n",
      "[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n",
      "\n",
      "Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library.\n",
      "\n",
      "The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\n",
      "We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n",
      "\n",
      "\n",
      "## Scope\n",
      "\n",
      "The Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n",
      "\n",
      "\n",
      "## Ethical guidelines\n",
      "\n",
      "## Simple over easy\n",
      "\n",
      "As PyTorch states, **explicit is better than implicit** and **simple is better than complex**. This design philosophy is reflected in multiple parts of the library:\n",
      "- We follow PyTorch's API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management.\n",
      "- Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible.\n",
      "- Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers.\n",
      "- Separately trained components of the diffusion pipeline, *e.g.* the text encoder, the unet, and the variational autoencoder, each have their own model class. This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\n",
      "is very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline.\n",
      "\n",
      "## Tweakable, contributor-friendly over abstraction\n",
      "-----------------LLm ANSWER------------\n",
      "The purpose of the Diffusers library is to provide a modular toolbox for both inference and training of diffusion models across multiple modalities, aiming to be a natural extension of PyTorch.\n",
      "-----------------------------\n",
      "7\n",
      "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md\n",
      "What method does the EulerAncestralDiscreteScheduler use for sampling?\n",
      "\n",
      "Ancestral sampling with Euler method steps.\n",
      "Ancestral sampling with Euler method steps.\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md', 'huggingface/blog/blob/main/nystromformer.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# EulerAncestralDiscreteScheduler\n",
      "\n",
      "A scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n",
      "\n",
      "## EulerAncestralDiscreteScheduler\n",
      "[[autodoc]] EulerAncestralDiscreteScheduler\n",
      "\n",
      "## EulerAncestralDiscreteSchedulerOutput\n",
      "[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput\n",
      "\n",
      "### Schedulers\n",
      "\n",
      "Schedulers are responsible to guide the denoising process for inference as well as to define a noise schedule for training. They are designed as individual classes with loadable configuration files and strongly follow the **single-file policy**.\n",
      "\n",
      "The following design principles are followed:\n",
      "- All schedulers are found in [`src/diffusers/schedulers`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\n",
      "- Schedulers are **not** allowed to import from large utils files and shall be kept very self-contained.\n",
      "- One scheduler Python file corresponds to one scheduler algorithm (as might be defined in a paper).\n",
      "- If schedulers share similar functionalities, we can make use of the `#Copied from` mechanism.\n",
      "- Schedulers all inherit from `SchedulerMixin` and `ConfigMixin`.\n",
      "- Schedulers can be easily swapped out with the [`ConfigMixin.from_config`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config) method as explained in detail [here](../using-diffusers/schedulers.md).\n",
      "- Every scheduler has to have a `set_num_inference_steps`, and a `step` function. `set_num_inference_steps(...)` has to be called before every denoising process, *i.e.* before `step(...)` is called.\n",
      "- Every scheduler exposes the timesteps to be \"looped over\" via a `timesteps` attribute, which is an array of timesteps the model will be called upon.\n",
      "- The `step(...)` function takes a predicted model output and the \"current\" sample (x_t) and returns the \"previous\", slightly more denoised sample (x_t-1).\n",
      "- Given the complexity of diffusion schedulers, the `step` function does not expose all the complexity and can be a bit of a \"black box\".\n",
      "- In almost all cases, novel schedulers shall be implemented in a new scheduling file.\n",
      "\n",
      "You can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "## UnCLIPPipeline\n",
      "[[autodoc]] UnCLIPPipeline\n",
      "\t- all\n",
      "\t- __call__\n",
      "\n",
      "## UnCLIPImageVariationPipeline\n",
      "[[autodoc]] UnCLIPImageVariationPipeline\n",
      "\t- all\n",
      "\t- __call__\n",
      "\n",
      "## ImagePipelineOutput\n",
      "[[autodoc]] pipelines.ImagePipelineOutput\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Representing P as a block matrix\" src=\"assets/86_nystromformer/p_block.png\"></medium-zoom>\n",
      "  <figcaption>Representing P as a block matrix</figcaption>\n",
      "</figure>\n",
      "\n",
      "We now have four submatrices: \\\\(A_P, B_P, F_P,\\\\) and \\\\(C_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and \n",
      "\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A_P\\\\) and \\\\(F_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A_P\\\\) and \\\\(B_P\\\\). So, the entries of \\\\(A_P, B_P,\\\\) and \\\\(F_P\\\\) are known to us, and we will estimate \\\\(C_P\\\\). According to the Nyström method, \\\\(C_P\\\\) is given by:\n",
      "\n",
      "$$C_P = F_P A_P^+ B_P$$\n",
      "\n",
      "Here, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). \n",
      "Thus, the Nyström approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nyström approximation of P\" src=\"assets/86_nystromformer/p_hat.png\"></medium-zoom>\n",
      "  <figcaption>Nyström approximation of P</figcaption>\n",
      "</figure>\n",
      "\n",
      "As shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices. The reason for doing so will become clear later.\n",
      "\n",
      "\n",
      "## Can we approximate self-attention with the Nyström method?\n",
      "\n",
      "Our goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\n",
      "\n",
      "Here, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nyström approximation of S\" src=\"assets/86_nystromformer/s_hat.png\"></medium-zoom>\n",
      "  <figcaption>Nyström approximation of S</figcaption>\n",
      "</figure>\n",
      "\n",
      "`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "\n",
      "## BlipDiffusionPipeline\n",
      "[[autodoc]] BlipDiffusionPipeline\n",
      "    - all\n",
      "    - __call__\n",
      "\n",
      "## BlipDiffusionControlNetPipeline\n",
      "[[autodoc]] BlipDiffusionControlNetPipeline\n",
      "    - all\n",
      "    - __call__\n",
      "-----------------LLm ANSWER------------\n",
      "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps.\n",
      "-----------------------------\n",
      "8\n",
      "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md\n",
      "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n",
      "\n",
      "IDEFICS\n",
      "IDEFICS\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md', 'huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md', 'huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md', 'huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# Image tasks with IDEFICS\n",
      "\n",
      "[[open-in-colab]]\n",
      "\n",
      "While individual tasks can be tackled by fine-tuning specialized models, an alternative approach \n",
      "that has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \n",
      "For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \n",
      "This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \n",
      "solve image-text tasks with a large multimodal model called IDEFICS.\n",
      "\n",
      "[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), \n",
      "a state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image \n",
      "and text inputs and generates coherent text as output. It can answer questions about images, describe visual content, \n",
      "create stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) \n",
      "and [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub. For each variant, you can also find fine-tuned instructed \n",
      "versions of the model adapted for conversational use cases.\n",
      "\n",
      "This model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However, \n",
      "being a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether \n",
      "this approach suits your use case better than fine-tuning specialized models for each individual task. \n",
      "\n",
      "In this guide, you'll learn how to: \n",
      "- [Load IDEFICS](#loading-the-model) and [load the quantized version of the model](#loading-the-quantized-version-of-the-model)\n",
      "- Use IDEFICS for: \n",
      "  - [Image captioning](#image-captioning)\n",
      "  - [Prompted image captioning](#prompted-image-captioning)\n",
      "  - [Few-shot prompting](#few-shot-prompting)\n",
      "  - [Visual question answering](#visual-question-answering)\n",
      "  - [Image classificaiton](#image-classification)\n",
      "  - [Image-guided text generation](#image-guided-text-generation)\n",
      "- [Run inference in batch mode](#running-inference-in-batch-mode)\n",
      "- [Run IDEFICS instruct for conversational use](#idefics-instruct-for-conversational-use)\n",
      "\n",
      "Before you begin, make sure you have all the necessary libraries installed. \n",
      "\n",
      "```bash\n",
      "pip install -q bitsandbytes sentencepiece accelerate transformers\n",
      "```\n",
      "\n",
      ">>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
      ">>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n",
      "\n",
      ">>> generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)\n",
      ">>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
      ">>> print(generated_text[0])\n",
      "User: Describe this image.\n",
      "Assistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building. \n",
      "User: Describe this image.\n",
      "Assistant: An image of the Statue of Liberty. Fun fact: the Statue of Liberty is 151 feet tall.\n",
      "```\n",
      "\n",
      "Notice that just from a single example (i.e., 1-shot) the model has learned how to perform the task. For more complex tasks, \n",
      "feel free to experiment with a larger number of examples (e.g., 3-shot, 5-shot, etc.).\n",
      "\n",
      "## Visual question answering\n",
      "\n",
      "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. Similar to image \n",
      "captioning it can be used in accessibility applications, but also in education (reasoning about visual materials), customer \n",
      "service (questions about products based on images), and image retrieval.\n",
      "\n",
      "Let's get a new image for this task: \n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-vqa.jpg\" alt=\"Image of a couple having a picnic\"/>\n",
      "</div>\n",
      "\n",
      "Photo by [Jarritos Mexican Soda](https://unsplash.com/@jarritos). \n",
      "\n",
      "You can steer the model from image captioning to visual question answering by prompting it with appropriate instructions:\n",
      "\n",
      "1:\n",
      "This is an image of a couple on a picnic blanket.\n",
      "\n",
      "2:\n",
      "This is an image of a vegetable stand.\n",
      "```\n",
      "\n",
      "## IDEFICS instruct for conversational use\n",
      "\n",
      "For conversational use cases, you can find fine-tuned instructed versions of the model on the 🤗 Hub: \n",
      "`HuggingFaceM4/idefics-80b-instruct` and `HuggingFaceM4/idefics-9b-instruct`.\n",
      "\n",
      "These checkpoints are the result of fine-tuning the respective base models on a mixture of supervised and instruction \n",
      "fine-tuning datasets, which boosts the downstream performance while making the models more usable in conversational settings.\n",
      "\n",
      "The use and prompting for the conversational use is very similar to using the base models: \n",
      "\n",
      "```py\n",
      ">>> import torch\n",
      ">>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n",
      "\n",
      ">>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "\n",
      ">>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n",
      ">>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n",
      ">>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
      "\n",
      ">>> prompts = [\n",
      "...     [\n",
      "...         \"User: What is in this image?\",\n",
      "...         \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\n",
      "...         \"<end_of_utterance>\",\n",
      "\n",
      "...         \"\\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>\",\n",
      "\n",
      "...         \"\\nUser:\",\n",
      "...         \"https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052\",\n",
      "...         \"And who is that?<end_of_utterance>\",\n",
      "\n",
      "...         \"\\nAssistant:\",\n",
      "...     ],\n",
      "... ]\n",
      "\n",
      ">>> # --batched mode\n",
      ">>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(device)\n",
      ">>> # --single sample mode\n",
      ">>> # inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n",
      "\n",
      "To explain how tasks are solved, we'll walk through what goes on inside the model to output useful predictions.\n",
      "\n",
      "- [Wav2Vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)\n",
      "- [Vision Transformer (ViT)](model_doc/vit) and [ConvNeXT](model_doc/convnext) for image classification\n",
      "- [DETR](model_doc/detr) for object detection\n",
      "- [Mask2Former](model_doc/mask2former) for image segmentation\n",
      "- [GLPN](model_doc/glpn) for depth estimation\n",
      "- [BERT](model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder\n",
      "- [GPT2](model_doc/gpt2) for NLP tasks like text generation that use a decoder\n",
      "- [BART](model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Before you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information! \n",
      "\n",
      "</Tip>\n",
      "\n",
      "## Speech and audio\n",
      "\n",
      "[Wav2Vec2](model_doc/wav2vec2) is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. \n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png\"/>\n",
      "</div>\n",
      "\n",
      "This model has four main components:\n",
      "\n",
      "1. A *feature encoder* takes the raw audio waveform, normalizes it to zero mean and unit variance, and converts it into a sequence of feature vectors that are each 20ms long.\n",
      "-----------------LLm ANSWER------------\n",
      "The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS.\n",
      "-----------------------------\n",
      "9\n",
      "huggingface/course/blob/main/chapters/en/chapter9/7.mdx\n",
      "What is the purpose of the `gradio.Blocks` API?\n",
      "\n",
      "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n",
      "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/chapters/en/chapter9/7.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/7.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/7.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/9.mdx', 'gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md']\n",
      "Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\n",
      "\n",
      "<CourseFloatingBanner chapter={9}\n",
      "  classNames=\"absolute z-10 right-0 top-0\"\n",
      "  notebooks={[\n",
      "    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n",
      "    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n",
      "]} />\n",
      "\n",
      "In the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\n",
      "\n",
      "Now, what's the difference between `Interface` and `Blocks`?\n",
      "\n",
      "- ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\n",
      "\n",
      "- 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\").\n",
      "\n",
      "\n",
      "### Why Blocks 🧱?[[why-blocks-]]\n",
      "\n",
      "As we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n",
      "\n",
      "- Group together related demos as multiple tabs in one web application\n",
      "- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n",
      "- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n",
      "- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\n",
      "\n",
      "We will explore all of these concepts below.\n",
      "\n",
      "### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\n",
      "\n",
      "We will explore all of these concepts below.\n",
      "\n",
      "### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\n",
      "\n",
      "After you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook.\n",
      "\n",
      "```py\n",
      "import gradio as gr\n",
      "\n",
      "\n",
      "def flip_text(x):\n",
      "    return x[::-1]\n",
      "\n",
      "\n",
      "demo = gr.Blocks()\n",
      "\n",
      "with demo:\n",
      "    gr.Markdown(\n",
      "        \"\"\"\n",
      "    # Flip Text!\n",
      "    Start typing below to see the output.\n",
      "    \"\"\"\n",
      "    )\n",
      "    input = gr.Textbox(placeholder=\"Flip this text\")\n",
      "    output = gr.Textbox()\n",
      "\n",
      "    input.change(fn=flip_text, inputs=input, outputs=output)\n",
      "\n",
      "demo.launch()\n",
      "```\n",
      "\n",
      "<iframe src=\"https://course-demos-flip-text.hf.space\" frameBorder=\"0\" height=\"400\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n",
      "\n",
      "This simple example above introduces 4 concepts that underlie Blocks:\n",
      "\n",
      "1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\n",
      "<Tip>\n",
      "🙋If you're not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that 🤗\n",
      "</Tip>\n",
      "The order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)\n",
      "\n",
      "2. You can define regular Python functions anywhere in your code and run them with user input using `Blocks`. In our example, we have a simple function that \"flips\" the input text, but you can write any Python function, from a simple calculation to processing the predictions from a machine learning model.\n",
      "\n",
      "3. You can assign events to any `Blocks` component. This will run your function when the component is clicked, changed, etc. When you assign an event, you pass in three parameters: `fn`: the function that should be called, `inputs`: the (list) of input component(s), and `outputs`: the (list) of output components that should be called.\n",
      "\n",
      "   In the example above, we run the `flip_text()` function when the value in the `Textbox` named input `input` changes. The event reads the value in `input`, passes it as the name parameter to `flip_text()`, which then returns a value that gets assigned to our second `Textbox` named `output`.\n",
      "\n",
      "   To see a list of events that each component supports, see the Gradio [documentation](https://www.gradio.app/docs/).\n",
      "\n",
      "4. Blocks automatically figures out whether a component should be interactive (accept user input) or not, based on the event triggers you define. In our example, the first textbox is interactive, since its value is used by the `flip_text()` function. The second textbox is not interactive, since its value is never used as an input. In some cases, you might want to override this, which you can do by passing a boolean to the `interactive` parameter of the component (e.g. `gr.Textbox(placeholder=\"Flip this text\", interactive=True)`).\n",
      "\n",
      "### Customizing the layout of your demo[[customizing-the-layout-of-your-demo]]\n",
      "\n",
      "How can we use `Blocks` to customize the layout of our demo? By default, `Blocks` renders the components that you create vertically in one column. You can change that by creating additional columns `with gradio.Column():` or rows `with gradio.Row():` and creating components within those contexts.\n",
      "\n",
      "### 8. Which of the following are components included in the Gradio library?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Textbox.\",\n",
      "\t\t\texplain: \"Yes, you can create textboxes with the Textbox component.\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Graph.\",\n",
      "\t\t\texplain: \"There is currently no Graph component.\",\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Image.\",\n",
      "\t\t\texplain: \"Yes, you can create an image upload widget with the Image component.\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Audio.\",\n",
      "\t\t\texplain: \"Yes, you can create an audio upload widget with the Audio component.\",\n",
      "            correct: true\n",
      "        },\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 9. What does Gradio `Blocks` allow you to do?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Combine multiple demos into one web app\",\n",
      "\t\t\texplain: \"You can use the `with gradio.Tabs():` to add tabs for multiple demos\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Assign event triggers such as clicked/changed/etc to `Blocks` components\",\n",
      "\t\t\texplain: \"When you assign an event, you pass in three parameters: fn: the function that should be called, inputs: the (list) of input component(s), and outputs: the (list) of output components that should be called.\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Automatically determine which `Blocks` component should be interactive vs. static\",\n",
      "\t\t\texplain: \"Based on the event triggers you define, `Blocks` automatically figures out whether a component should accept user input or not.\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "\t\t {\n",
      "\t\t\ttext: \"Create multi-step demos; meaning allowing you to reuse the output of one component as the input to the next\",\n",
      "\t\t\texplain: \"You can use a component for the input of one event trigger but the output of another.\",\n",
      "            correct: true\n",
      "        },\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces.\n",
      "\n",
      "```py\n",
      "%%blocks\n",
      "\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    gr.Markdown(f\"# Greetings {args.name}!\")\n",
      "    inp = gr.Textbox()\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n",
      "```\n",
      "\n",
      "Notice that:\n",
      "\n",
      "- You do not need to launch your demo — Gradio does that for you automatically!\n",
      "\n",
      "- Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server. This means you'll see your changes _much, much faster_ than if you were rerunning the cell normally.\n",
      "\n",
      "Here's what it looks like in a jupyter notebook:\n",
      "\n",
      "![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter_reload.gif)\n",
      "\n",
      "🪄 This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code!\n",
      "\n",
      "The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio.\n",
      "\n",
      "---\n",
      "\n",
      "## Next Steps\n",
      "\n",
      "Now that you know how to develop quickly using Gradio, start building your own!\n",
      "\n",
      "If you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) 🤗\n",
      "-----------------LLm ANSWER------------\n",
      "The `gradio.Blocks` API is a low-level API that allows you to have full control over the data flows and layout of your application. It provides a way to build complex, multi-step applications, whereas the `Interface` API is a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\n",
      "-----------------------------\n",
      "10\n",
      "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md\n",
      "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n",
      "\n",
      "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n",
      "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md', 'huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md', 'huggingface/blog/blob/main/clipseg-zero-shot.md', 'huggingface/blog/blob/main/clipseg-zero-shot.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# unCLIP\n",
      "\n",
      "[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo).\n",
      "\n",
      "The abstract from the paper is following:\n",
      "\n",
      "*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\n",
      "\n",
      "Distillation for quantization on Textual Inversion models to personalize text2image\n",
      "\n",
      "[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\n",
      "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
      "We have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n",
      "\n",
      "## Installing the dependencies\n",
      "\n",
      "Before running the scripts, make sure to install the library's training dependencies:\n",
      "\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "## Prepare Datasets\n",
      "\n",
      "One picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n",
      "\n",
      "<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n",
      "    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n",
      "</a>\n",
      "\n",
      "## Get a FP32 Textual Inversion model\n",
      "\n",
      "Use the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n",
      "\n",
      "```bash\n",
      "export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
      "export DATA_DIR=\"./dicoo\"\n",
      "\n",
      "accelerate launch textual_inversion.py \\\n",
      "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
      "  --train_data_dir=$DATA_DIR \\\n",
      "  --learnable_property=\"object\" \\\n",
      "  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n",
      "  --resolution=512 \\\n",
      "  --train_batch_size=1 \\\n",
      "  --gradient_accumulation_steps=4 \\\n",
      "  --max_train_steps=3000 \\\n",
      "  --learning_rate=5.0e-04 --scale_lr \\\n",
      "  --lr_scheduler=\"constant\" \\\n",
      "  --lr_warmup_steps=0 \\\n",
      "  --output_dir=\"dicoo_model\"\n",
      "```\n",
      "\n",
      "What’s more, CLIP is not only useful for classification, but it can also be used for [image search](https://huggingface.co/spaces/DrishtiSharma/Text-to-Image-search-using-CLIP) (can you see how this is similar to classification?), [text-to-image models](https://huggingface.co/spaces/kamiyamai/stable-diffusion-webui) ([DALL-E 2](https://openai.com/dall-e-2/) is powered by CLIP), [object detection](https://segments.ai/zeroshot?utm_source=hf&utm_medium=blog&utm_campaign=clipseg) ([OWL-ViT](https://arxiv.org/abs/2205.06230)), and most importantly for us: image segmentation. Now you see why CLIP was truly a breakthrough in machine learning.\n",
      "\n",
      "The reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them.\n",
      "\n",
      "## CLIPSeg: image segmentation with CLIP\n",
      "\n",
      "[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo Lüddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn’t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP.\n",
      "\n",
      "## CLIP: the magic model behind CLIPSeg\n",
      "\n",
      "[CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip), which stands for **C**ontrastive **L**anguage–**I**mage **P**re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract *representation* of your input. This abstract representation, also called an *embedding*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).\n",
      "\n",
      "At first, this might not seem very useful, but it is actually very powerful. As an example, let’s take a quick look at how CLIP can be used to classify images without ever having been trained on that task. To classify an image, we input the image and the different categories we want to choose from to CLIP (e.g. we input an image and the words “apple”, “orange”, …).  CLIP then gives us back an embedding of the image and of each category. Now, we simply have to check which category embedding is closest to the embedding of the image, et voilà! Feels like magic, doesn’t it? \n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\n",
      "  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\n",
      "</figure>\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# BLIP-Diffusion\n",
      "\n",
      "BLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n",
      "\n",
      "\n",
      "The abstract from the paper is:\n",
      "-----------------LLm ANSWER------------\n",
      "The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to leverage the robust representations of images learned by contrastive models like CLIP for image generation. The model consists of a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. The authors show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity.\n",
      "-----------------------------\n",
      "11\n",
      "huggingface/transformers/blob/main/examples/research_projects/README.md\n",
      "What command is used to install the requirements for a research project using 🤗 Transformers?\n",
      "\n",
      "pip install -r requirements.txt\n",
      "pip install -r requirements.txt\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/examples/research_projects/README.md', 'huggingface/blog/blob/main/accelerate-library.md', 'huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md', 'huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md', 'huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md']\n",
      "!---\n",
      "Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "-->\n",
      "\n",
      "# Research projects\n",
      "\n",
      "This folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\n",
      "version of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\n",
      "\n",
      "To use any of them, just run the command\n",
      "```\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "inside the folder of your choice.\n",
      "\n",
      "If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.\n",
      "\n",
      "### One launcher to rule them all\n",
      "\n",
      "The scripts using Accelerate will be completely compatible with your traditional launchers, such as `torch.distributed.launch`. But remembering all the arguments to them is a bit annoying and when you've setup your instance with 4 GPUs, you'll run most of your trainings using them all. Accelerate comes with a handy CLI that works in two steps:\n",
      "\n",
      "```bash\n",
      "accelerate config\n",
      "```\n",
      "\n",
      "This will trigger a little questionnaire about your setup, which will create a config file you can edit with all the defaults for your training commands. Then\n",
      "\n",
      "```bash\n",
      "accelerate launch path_to_script.py --args_to_the_script\n",
      "```\n",
      "\n",
      "will launch your training script using those default. The only thing you have to do is provide all the arguments needed by your training script.\n",
      "\n",
      "To make this launcher even more awesome, you can use it to spawn an AWS instance using SageMaker. Look at [this guide](https://huggingface.co/docs/accelerate/sagemaker.html) to discover how!\n",
      "\n",
      "### How to get involved?\n",
      "\n",
      "To get started, just `pip install accelerate` or see the [documentation](https://huggingface.co/docs/accelerate/installation.html) for more install options.\n",
      "\n",
      "Accelerate is a fully open-sourced project, you can find it on [GitHub](https://github.com/huggingface/accelerate), have a look at its [documentation](https://huggingface.co/docs/accelerate/) or skim through our [basic examples](https://github.com/huggingface/accelerate/tree/main/examples). Please let us know if you have any issue or feature you would like the library to support. For all questions, the [forums](https://discuss.huggingface.co/c/accelerate) is the place to check!\n",
      "\n",
      "For more complex examples in situation, you can look at the official [Transformers examples](https://github.com/huggingface/transformers/tree/master/examples). Each folder contains a `run_task_no_trainer.py` that leverages the Accelerate library!\n",
      "\n",
      "Before you begin, make sure you have all the necessary libraries installed. \n",
      "\n",
      "```bash\n",
      "pip install -q bitsandbytes sentencepiece accelerate transformers\n",
      "```\n",
      "\n",
      "<Tip>\n",
      "To run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of GPU memory.\n",
      "</Tip>\n",
      "\n",
      "## Loading the model\n",
      "\n",
      "Let's start by loading the model's 9 billion parameters checkpoint: \n",
      "\n",
      "```py\n",
      ">>> checkpoint = \"HuggingFaceM4/idefics-9b\"\n",
      "```\n",
      "\n",
      "Just like for other Transformers models, you need to load a processor and the model itself from the checkpoint. \n",
      "The IDEFICS processor wraps a [`LlamaTokenizer`] and IDEFICS image processor into a single processor to take care of \n",
      "preparing text and image inputs for the model.\n",
      "\n",
      "```py\n",
      ">>> import torch\n",
      "\n",
      ">>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n",
      "\n",
      ">>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
      "\n",
      ">>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
      "```\n",
      "\n",
      "Setting `device_map` to `\"auto\"` will automatically determine how to load and store the model weights in the most optimized \n",
      "manner given existing devices.\n",
      "\n",
      "### Quantized model\n",
      "\n",
      "If high-memory GPU availability is an issue, you can load the quantized version of the model. To load the model and the \n",
      "processor in 4bit precision, pass a `BitsAndBytesConfig` to the `from_pretrained` method and the model will be compressed \n",
      "on the fly while loading.\n",
      "\n",
      "```py\n",
      ">>> import torch\n",
      ">>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig\n",
      "\n",
      ">>> quantization_config = BitsAndBytesConfig(\n",
      "...     load_in_4bit=True,\n",
      "...     bnb_4bit_compute_dtype=torch.float16,\n",
      "... )\n",
      "\n",
      ">>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
      "\n",
      ">>> model = IdeficsForVisionText2Text.from_pretrained(\n",
      "...     checkpoint,\n",
      "...     quantization_config=quantization_config,\n",
      "...     device_map=\"auto\"\n",
      "... )\n",
      "```\n",
      "\n",
      "Or:\n",
      "\n",
      "```python\n",
      "notebook_launcher(train_ddp_accelerate, args=(), num_processes=2)\n",
      "```\n",
      "\n",
      "## Using 🤗 Trainer\n",
      "\n",
      "Finally, we arrive at the highest level of API -- the Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).\n",
      "\n",
      "This wraps as much training as possible while still being able to train on distributed systems without the user needing to do anything at all.\n",
      "\n",
      "First we need to import the Trainer:\n",
      "\n",
      "```python\n",
      "from transformers import Trainer\n",
      "```\n",
      "\n",
      "Then we define some `TrainingArguments` to control all the usual hyper-parameters. The trainer also works through dictionaries, so a custom collate function needs to be made.\n",
      "\n",
      "Finally, we subclass the trainer and write our own `compute_loss`.\n",
      "\n",
      "Afterwards, this code will also work on a distributed setup without any training code needing to be written whatsoever!\n",
      "\n",
      "```python\n",
      "from transformers import Trainer, TrainingArguments\n",
      "\n",
      "model = BasicNet()\n",
      "\n",
      "training_args = TrainingArguments(\n",
      "    \"basic-trainer\",\n",
      "    per_device_train_batch_size=64,\n",
      "    per_device_eval_batch_size=64,\n",
      "    num_train_epochs=1,\n",
      "    evaluation_strategy=\"epoch\",\n",
      "    remove_unused_columns=False\n",
      ")\n",
      "\n",
      "def collate_fn(examples):\n",
      "    pixel_values = torch.stack([example[0] for example in examples])\n",
      "    labels = torch.tensor([example[1] for example in examples])\n",
      "    return {\"x\":pixel_values, \"labels\":labels}\n",
      "\n",
      "class MyTrainer(Trainer):\n",
      "    def compute_loss(self, model, inputs, return_outputs=False):\n",
      "        outputs = model(inputs[\"x\"])\n",
      "        target = inputs[\"labels\"]\n",
      "        loss = F.nll_loss(outputs, target)\n",
      "        return (loss, outputs) if return_outputs else loss\n",
      "\n",
      "trainer = MyTrainer(\n",
      "    model,\n",
      "    training_args,\n",
      "    train_dataset=train_dset,\n",
      "    eval_dataset=test_dset,\n",
      "    data_collator=collate_fn,\n",
      ")\n",
      "```\n",
      "\n",
      "```python\n",
      "trainer.train()\n",
      "```\n",
      "\n",
      "trainer.train()\n",
      "\n",
      "notebook_launcher(train_trainer_ddp, args=(), num_processes=2)\n",
      "```\n",
      "\n",
      "## Resources\n",
      "\n",
      "To learn more about PyTorch Distributed Data Parallelism, check out the documentation [here](https://pytorch.org/docs/stable/distributed.html)\n",
      "\n",
      "To learn more about 🤗 Accelerate, check out the documentation [here](https://huggingface.co/docs/accelerate)\n",
      "\n",
      "To learn more about 🤗 Transformers, check out the documentation [here](https://huggingface.co/docs/transformers)\n",
      "-----------------LLm ANSWER------------\n",
      "According to the given context, the command used to install the requirements for a research project using 🤗 Transformers is:\n",
      "\n",
      "```\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "This command should be run inside the folder of your choice.\n",
      "-----------------------------\n",
      "12\n",
      "huggingface/course/blob/main/chapters/en/chapter1/10.mdx\n",
      "What task does the `roberta-large-mnli` checkpoint perform?\n",
      "\n",
      "Text classification\n",
      "Text classification\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/chapters/en/chapter1/10.mdx', 'huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md', 'huggingface/course/blob/main/subtitles/en/raw/tasks.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/course/blob/main/subtitles/en/raw/tasks.md']\n",
      "!-- DISABLE-FRONTMATTER-SECTIONS -->\n",
      "\n",
      "# End-of-chapter quiz[[end-of-chapter-quiz]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={1}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "This chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n",
      "\n",
      "First, though, let's test what you learned in this chapter!\n",
      "\n",
      "\n",
      "### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\n",
      "\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"Summarization\",\n",
      "\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Text classification\",\n",
      "\t\t\texplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called <em>natural language inference</em>.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Text generation\",\n",
      "\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 2. What will the following code return?\n",
      "\n",
      "```py\n",
      "from transformers import pipeline\n",
      "\n",
      "ner = pipeline(\"ner\", grouped_entities=True)\n",
      "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
      "```\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return classification scores for this sentence, with labels \\\"positive\\\" or \\\"negative\\\".\",\n",
      "\t\t\texplain: \"This is incorrect — this would be a <code>sentiment-analysis</code> pipeline.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return a generated text completing this sentence.\",\n",
      "\t\t\texplain: \"This is incorrect — it would be a <code>text-generation</code> pipeline.\",\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return the words representing persons, organizations or locations.\",\n",
      "\t\t\texplain: \"Furthermore, with <code>grouped_entities=True</code>, it will group together the words belonging to the same entity, like \\\"Hugging Face\\\".\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "## Resources\n",
      "\n",
      "- [Text classification task guide](../tasks/sequence_classification)\n",
      "- [Token classification task guide](../tasks/token_classification)\n",
      "- [Question answering task guide](../tasks/question_answering)\n",
      "- [Masked language modeling task guide](../tasks/masked_language_modeling)\n",
      "- [Multiple choice task guide](../tasks/multiple_choice)\n",
      "\n",
      "## FNetConfig\n",
      "\n",
      "[[autodoc]] FNetConfig\n",
      "\n",
      "## FNetTokenizer\n",
      "\n",
      "[[autodoc]] FNetTokenizer\n",
      "    - build_inputs_with_special_tokens\n",
      "    - get_special_tokens_mask\n",
      "    - create_token_type_ids_from_sequences\n",
      "    - save_vocabulary\n",
      "\n",
      "## FNetTokenizerFast\n",
      "\n",
      "[[autodoc]] FNetTokenizerFast\n",
      "\n",
      "## FNetModel\n",
      "\n",
      "[[autodoc]] FNetModel\n",
      "    - forward\n",
      "\n",
      "## FNetForPreTraining\n",
      "\n",
      "[[autodoc]] FNetForPreTraining\n",
      "    - forward\n",
      "\n",
      "## FNetForMaskedLM\n",
      "\n",
      "[[autodoc]] FNetForMaskedLM\n",
      "    - forward\n",
      "\n",
      "## FNetForNextSentencePrediction\n",
      "\n",
      "[[autodoc]] FNetForNextSentencePrediction\n",
      "    - forward\n",
      "\n",
      "## FNetForSequenceClassification\n",
      "\n",
      "[[autodoc]] FNetForSequenceClassification\n",
      "    - forward\n",
      "\n",
      "## FNetForMultipleChoice\n",
      "\n",
      "[[autodoc]] FNetForMultipleChoice\n",
      "    - forward\n",
      "\n",
      "## FNetForTokenClassification\n",
      "\n",
      "[[autodoc]] FNetForTokenClassification\n",
      "    - forward\n",
      "\n",
      "## FNetForQuestionAnswering\n",
      "\n",
      "[[autodoc]] FNetForQuestionAnswering\n",
      "    - forward\n",
      "\n",
      "ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI_JMQtyEr-sLJSy5_SnLCb4vtQf\n",
      "\n",
      "Token Classification video\n",
      "\n",
      "Welcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task.\n",
      "Token classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\n",
      "Let’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\n",
      "Another token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\n",
      "Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\n",
      "An example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\n",
      "You can extract important information from invoices using named entity recognition models, such as date, organization name or address.\n",
      "For more information about the Token classification task, check out the Hugging Face course.\n",
      "\n",
      "\n",
      "Question Answering video\n",
      "\n",
      "3. The input embeddings are passed through multiple encoder layers to output some final hidden states.\n",
      "\n",
      "To use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label.\n",
      "\n",
      "Ready to try your hand at text classification? Check out our complete [text classification guide](tasks/sequence_classification) to learn how to finetune DistilBERT and use it for inference!\n",
      "\n",
      "### Token classification\n",
      "\n",
      "To use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.\n",
      "\n",
      "Ready to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!\n",
      "\n",
      "### Question answering\n",
      "\n",
      "To use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer.\n",
      "\n",
      "Ready to try your hand at question answering? Check out our complete [question answering guide](tasks/question_answering) to learn how to finetune DistilBERT and use it for inference!\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Masked Language Modeling video\n",
      "\n",
      "Welcome to the Hugging Face tasks series! In this video we’ll take a look at Masked Language Modeling.\n",
      "Masked language modeling is the task of predicting which words should fill in the blanks of a sentence.\n",
      "These models take a masked text as the input and output the possible values for that mask.\n",
      "Masked language modeling is handy before fine-tuning your model for your task. For example, if you need to use a model in a specific domain, say, biomedical documents, models like BERT will treat your domain-specific words as rare tokens. If you train a masked language model using your biomedical corpus and then fine tune your model on a downstream task, you will have a better performance.\n",
      "Classification metrics can’t be used as there’s no single correct answer to mask values. Instead, we evaluate the distribution of the mask values.\n",
      "A common metric to do so is the cross-entropy loss. Perplexity is also a widely used metric and it is calculated as the exponential of the cross-entropy loss.\n",
      "You can use any dataset with plain text and tokenize the text to mask the data.\n",
      "For more information about the Masked Language Modeling, check out the Hugging Face course.\n",
      "\n",
      "\n",
      "Summarization video\n",
      "-----------------LLm ANSWER------------\n",
      "The `roberta-large-mnli` checkpoint performs the task of Natural Language Inference (NLI), which is a task also called \"entailment\" or \"text classification\".\n",
      "-----------------------------\n",
      "13\n",
      "huggingface/blog/blob/main/pricing-update.md\n",
      "What service is replacing the Paid tier of the Inference API at Hugging Face?\n",
      "\n",
      "Inference Endpoints\n",
      "Inference Endpoints\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/pricing-update.md', 'huggingface/blog/blob/main/infinity-cpu-performance.md', 'huggingface/blog/blob/main/infinity-cpu-performance.md', 'huggingface/blog/blob/main/infinity-cpu-performance.md', 'huggingface/blog/blob/main/infinity-cpu-performance.md']\n",
      "--\n",
      "title: Introducing our new pricing\n",
      "thumbnail: /blog/assets/114_pricing-update/thumbnail.png\n",
      "authors:\n",
      "- user: sbrandeis\n",
      "- user: pierric\n",
      "---\n",
      "\n",
      "# Introducing our new pricing\n",
      "\n",
      "\n",
      "As you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n",
      "\n",
      "First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\n",
      "\n",
      "Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).\n",
      "\n",
      "Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records.\n",
      "\n",
      "**TL;DR**: **At HF we monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system.\n",
      "\n",
      "If you have any questions, feel free to reach out. We welcome your feedback 🔥\n",
      "\n",
      "If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n",
      "\n",
      "\n",
      "## Resources\n",
      "\n",
      "* [Hugging Face Infinity](https://huggingface.co/infinity)\n",
      "* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n",
      "* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i) \n",
      "* [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
      "* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n",
      "* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n",
      "* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)\n",
      "\n",
      "--\n",
      "title: \"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\"\n",
      "thumbnail: /blog/assets/46_infinity_cpu_performance/thumbnail.png\n",
      "authors:\n",
      "- user: philschmid\n",
      "- user: jeffboudier\n",
      "- user: mfuntowicz\n",
      "---\n",
      "# Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\n",
      "\n",
      "\n",
      "<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n",
      "\n",
      "<br>\n",
      "<div style=\"background-color: #e6f9e6; padding: 16px 32px; outline: 2px solid; border-radius: 10px;\">\n",
      "  December 2022 Update: Infinity is no longer offered by Hugging Face as a commercial inference solution. To deploy and accelerate your models, we recommend the following new solutions:\n",
      "\n",
      "  * [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) to easily deploy models on dedicated infrastructure managed by Hugging Face.\n",
      "\n",
      "  * Our open-source optimization libraries, [🤗 Optimum Intel](https://huggingface.co/blog/openvino) and [🤗 Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference.\n",
      "\n",
      "  * Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.\n",
      "</div>\n",
      "\n",
      "\n",
      "## Introduction \n",
      "\n",
      "Transfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.\n",
      "\n",
      "The main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n",
      "\n",
      "With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n",
      "\n",
      "\n",
      "## What is Hugging Face Infinity\n",
      "\n",
      "Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.\n",
      "\n",
      "Hugging Face Infinity consists of 2 main services:\n",
      "* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.\n",
      "* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.\n",
      "\n",
      "The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.\n",
      "\n",
      "<br>\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Product overview\" src=\"assets/46_infinity_cpu_performance/overview.png\"></medium-zoom>\n",
      "  <figcaption>Figure 1. Infinity Overview</figcaption>\n",
      "</figure>\n",
      "<br>\n",
      "\n",
      "An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification.\n",
      "\n",
      "You can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).\n",
      "\n",
      "---\n",
      "\n",
      "## Benchmark \n",
      "\n",
      "Inference performance benchmarks often only measure the execution of the model. In this blog post, and when discussing the performance of Infinity, we always measure the end-to-end pipeline including pre-processing, prediction, post-processing. Please keep this in mind when comparing these results with other latency measurements. \n",
      "\n",
      "<br>\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pipeline\" src=\"assets/46_infinity_cpu_performance/pipeline.png\"></medium-zoom>\n",
      "  <figcaption>Figure 2. Infinity End-to-End Pipeline</figcaption>\n",
      "</figure>\n",
      "<br>\n",
      "\n",
      "### Environment\n",
      "\n",
      "As a benchmark environment, we are going to use the [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i), which are compute-optimized instances powered by the 3rd generation of Intel Xeon Scalable processors. These new Intel-based instances are using the ice-lake Process Technology and support Intel AVX-512, Intel Turbo Boost, and Intel Deep Learning Boost.\n",
      "\n",
      "In addition to superior performance for machine learning workloads, the Intel Ice Lake C6i instances offer great cost-performance and are our recommendation to deploy Infinity on Amazon Web Services.  To learn more, visit the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) page.\n",
      "-----------------LLm ANSWER------------\n",
      "According to the blog post, the service replacing the Paid tier of the Inference API at Hugging Face is the Inference Endpoints.\n",
      "-----------------------------\n",
      "14\n",
      "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md\n",
      "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n",
      "\n",
      "Grouped convolutions\n",
      "Grouped convolutions\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md']\n",
      "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# SqueezeBERT\n",
      "\n",
      "## Overview\n",
      "\n",
      "The SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\n",
      "bidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\n",
      "SqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\n",
      "instead of fully-connected layers for the Q, K, V and FFN layers.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "2. The encoder accepts the patch embeddings, and passes them through several encoder blocks. Each block consists of attention and Mix-FFN layers. The purpose of the latter is to provide positional information. At the end of each encoder block is a *patch merging* layer for creating hierarchical representations. The features of each group of neighboring patches are concatenated, and a linear layer is applied to the concatenated features to reduce the number of patches to a resolution of 1/4. This becomes the input to the next encoder block, where this whole process is repeated until you have image features with resolutions of 1/8, 1/16, and 1/32.\n",
      "\n",
      "3. A lightweight decoder takes the last feature map (1/32 scale) from the encoder and upsamples it to 1/16 scale. From here, the feature is passed into a *Selective Feature Fusion (SFF)* module, which selects and combines local and global features from an attention map for each feature and then upsamples it to 1/8th. This process is repeated until the decoded features are the same size as the original image. The output is passed through two convolution layers and then a sigmoid activation is applied to predict the depth of each pixel.\n",
      "\n",
      "## Natural language processing\n",
      "\n",
      "The Transformer was initially designed for machine translation, and since then, it has practically become the default architecture for solving all NLP tasks. Some tasks lend themselves to the Transformer's encoder structure, while others are better suited for the decoder. Still, other tasks make use of both the Transformer's encoder-decoder structure.\n",
      "\n",
      "### Text classification\n",
      "\n",
      "[BERT](model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.\n",
      "\n",
      "This model was contributed by [forresti](https://huggingface.co/forresti).\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n",
      "  rather than the left.\n",
      "- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n",
      "  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n",
      "  with a causal language modeling (CLM) objective are better in that regard.\n",
      "- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n",
      "  *squeezebert/squeezebert-mnli-headless* checkpoint.\n",
      "\n",
      "## Resources\n",
      "\n",
      "- [Text classification task guide](../tasks/sequence_classification)\n",
      "- [Token classification task guide](../tasks/token_classification)\n",
      "- [Question answering task guide](../tasks/question_answering)\n",
      "- [Masked language modeling task guide](../tasks/masked_language_modeling)\n",
      "- [Multiple choice task guide](../tasks/multiple_choice)\n",
      "\n",
      "## SqueezeBertConfig\n",
      "\n",
      "[[autodoc]] SqueezeBertConfig\n",
      "\n",
      "## SqueezeBertTokenizer\n",
      "\n",
      "[[autodoc]] SqueezeBertTokenizer\n",
      "    - build_inputs_with_special_tokens\n",
      "    - get_special_tokens_mask\n",
      "    - create_token_type_ids_from_sequences\n",
      "    - save_vocabulary\n",
      "\n",
      "## SqueezeBertTokenizerFast\n",
      "\n",
      "[[autodoc]] SqueezeBertTokenizerFast\n",
      "\n",
      "## SqueezeBertModel\n",
      "\n",
      "[[autodoc]] SqueezeBertModel\n",
      "\n",
      "## SqueezeBertForMaskedLM\n",
      "\n",
      "[[autodoc]] SqueezeBertForMaskedLM\n",
      "\n",
      "## SqueezeBertForSequenceClassification\n",
      "\n",
      "[[autodoc]] SqueezeBertForSequenceClassification\n",
      "\n",
      "## SqueezeBertForMultipleChoice\n",
      "\n",
      "[[autodoc]] SqueezeBertForMultipleChoice\n",
      "\n",
      "## SqueezeBertForTokenClassification\n",
      "\n",
      "[[autodoc]] SqueezeBertForTokenClassification\n",
      "\n",
      "## SqueezeBertForQuestionAnswering\n",
      "\n",
      "[[autodoc]] SqueezeBertForQuestionAnswering\n",
      "\n",
      "- Splitting the embedding matrix into two smaller matrices.\n",
      "- Using repeating layers split among groups.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "*Increasing model size when pretraining natural language representations often results in improved performance on\n",
      "downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\n",
      "longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\n",
      "techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\n",
      "that our proposed methods lead to models that scale much better compared to the original BERT. We also use a\n",
      "self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\n",
      "with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\n",
      "SQuAD benchmarks while having fewer parameters compared to BERT-large.*\n",
      "\n",
      "This model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n",
      "[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# FNet\n",
      "\n",
      "## Overview\n",
      "\n",
      "The FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\n",
      "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\n",
      "model with a fourier transform which returns only the real parts of the transform. The model is significantly faster\n",
      "than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\n",
      "accuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\n",
      "paper is the following:\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.\n",
      "-----------------------------\n",
      "15\n",
      "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx\n",
      "What type of license is the HuggingFace Team's software distributed under?\n",
      "\n",
      "Apache License, Version 2.0\n",
      "Apache License, Version 2.0\n",
      "-----------------Context------------\n",
      "['huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx', 'huggingface/transformers/blob/main/examples/research_projects/README.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md']\n",
      "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Under construction\n",
      "\n",
      "!---\n",
      "Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "-->\n",
      "\n",
      "# Research projects\n",
      "\n",
      "This folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\n",
      "version of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\n",
      "\n",
      "To use any of them, just run the command\n",
      "```\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "inside the folder of your choice.\n",
      "\n",
      "If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# 🧨 Diffusers’ Ethical Guidelines\n",
      "\n",
      "## Preamble\n",
      "\n",
      "[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n",
      "\n",
      "Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library.\n",
      "\n",
      "The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\n",
      "We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n",
      "\n",
      "\n",
      "## Scope\n",
      "\n",
      "The Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n",
      "\n",
      "\n",
      "## Ethical guidelines\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Philosophy\n",
      "\n",
      "🧨 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\n",
      "Its purpose is to serve as a **modular toolbox** for both inference and training.\n",
      "\n",
      "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
      "\n",
      "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n",
      "\n",
      "## Usability over Performance\n",
      "\n",
      "- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n",
      "\n",
      "- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n",
      "\n",
      "- **Encouraging safety in deployment**\n",
      "\n",
      "  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n",
      "\n",
      "  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n",
      "\n",
      "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository’s authors to have more control over its use.\n",
      "\n",
      "- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
      "-----------------LLm ANSWER------------\n",
      "The HuggingFace Team's software is distributed under the Apache License, Version 2.0 (the \"License\").\n",
      "-----------------------------\n",
      "16\n",
      "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md\n",
      "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n",
      "\n",
      "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n",
      "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md']\n",
      "- Splitting the embedding matrix into two smaller matrices.\n",
      "- Using repeating layers split among groups.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "*Increasing model size when pretraining natural language representations often results in improved performance on\n",
      "downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\n",
      "longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\n",
      "techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\n",
      "that our proposed methods lead to models that scale much better compared to the original BERT. We also use a\n",
      "self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\n",
      "with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\n",
      "SQuAD benchmarks while having fewer parameters compared to BERT-large.*\n",
      "\n",
      "This model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n",
      "[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# ALBERT\n",
      "\n",
      "<div class=\"flex flex-wrap space-x-1\">\n",
      "<a href=\"https://huggingface.co/models?filter=albert\">\n",
      "<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\n",
      "</a>\n",
      "<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\n",
      "<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n",
      "</a>\n",
      "</div>\n",
      "\n",
      "## Overview\n",
      "\n",
      "The ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\n",
      "Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\n",
      "speed of BERT:\n",
      "\n",
      "- Splitting the embedding matrix into two smaller matrices.\n",
      "- Using repeating layers split among groups.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "Since it has been discovered that more parameters lead to better performance, this technique allows to increase the \n",
      "number of parameters by an order of magnitude without increasing training costs.\n",
      "\n",
      "In this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function \n",
      "that trains each expert in a balanced way depending on the input token's position in a sequence.\n",
      "\n",
      "![MoE Transformer 2x block](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perf-moe-transformer.png)\n",
      "\n",
      "(source: [GLAM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html))\n",
      "\n",
      "You can find exhaustive details and comparison tables in the papers listed at the end of this section.\n",
      "\n",
      "The main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude \n",
      "larger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements.\n",
      "\n",
      "There is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or \n",
      "hundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the \n",
      "memory requirements moderately as well.\n",
      "\n",
      "Most related papers and implementations are built around Tensorflow/TPUs:\n",
      "\n",
      "- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)\n",
      "- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)\n",
      "- [GLaM: Generalist Language Model (GLaM)](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n",
      "  than the left.\n",
      "- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n",
      "  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n",
      "  number of (repeating) layers.\n",
      "- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters.\n",
      "- Layers are split in groups that share parameters (to save memory).\n",
      "Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n",
      "\n",
      "\n",
      "\n",
      "This model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\n",
      "[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\n",
      "\n",
      "\n",
      "## Resources\n",
      "\n",
      "\n",
      "The resources provided in the following sections consist of a list of official Hugging Face and community (indicated by 🌎) resources to help you get started with AlBERT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n",
      "\n",
      "\n",
      "<PipelineTag pipeline=\"text-classification\"/>\n",
      "\n",
      "```py\n",
      "import bitsandbytes as bnb\n",
      "from torch import nn\n",
      "from transformers.trainer_pt_utils import get_parameter_names\n",
      "\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
      "\n",
      "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
      "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
      "optimizer_grouped_parameters = [\n",
      "    {\n",
      "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
      "        \"weight_decay\": training_args.weight_decay,\n",
      "    },\n",
      "    {\n",
      "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
      "        \"weight_decay\": 0.0,\n",
      "    },\n",
      "]\n",
      "\n",
      "optimizer_kwargs = {\n",
      "    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
      "    \"eps\": training_args.adam_epsilon,\n",
      "}\n",
      "optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
      "adam_bnb_optim = bnb.optim.Adam8bit(\n",
      "    optimizer_grouped_parameters,\n",
      "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
      "    eps=training_args.adam_epsilon,\n",
      "    lr=training_args.learning_rate,\n",
      ")\n",
      "```\n",
      "\n",
      "Finally, pass the custom optimizer as an argument to the `Trainer`:\n",
      "\n",
      "```py\n",
      "trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))\n",
      "```\n",
      "\n",
      "Combined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training), \n",
      "you can expect to get about a 3x memory improvement and even slightly higher throughput as using Adafactor. \n",
      "\n",
      "### multi_tensor\n",
      "\n",
      "pytorch-nightly introduced `torch.optim._multi_tensor` which should significantly speed up the optimizers for situations \n",
      "with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner, take a look at this GitHub [issue](https://github.com/huggingface/transformers/issues/9965).\n",
      "\n",
      "## Data preloading\n",
      "-----------------LLm ANSWER------------\n",
      "The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\n",
      "\n",
      "1. Splitting the embedding matrix into two smaller matrices.\n",
      "2. Using repeating layers split among groups.\n",
      "-----------------------------\n",
      "17\n",
      "huggingface/course/blob/main/chapters/en/chapter5/1.mdx\n",
      "What are the three main steps for fine-tuning a model with the 🤗 Datasets library?\n",
      "\n",
      "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n",
      "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/chapters/en/chapter5/1.mdx', 'huggingface/peft/blob/main/docs/source/accelerate/fsdp.md', 'huggingface/peft/blob/main/docs/source/accelerate/fsdp.md', 'huggingface/blog/blob/main/intel.md', 'huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md']\n",
      "Introduction[[introduction]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={5}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "In [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n",
      "\n",
      "1. Load a dataset from the Hugging Face Hub.\n",
      "2. Preprocess the data with `Dataset.map()`.\n",
      "3. Load and compute metrics.\n",
      "\n",
      "But this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:\n",
      "\n",
      "* What do you do when your dataset is not on the Hub?\n",
      "* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\n",
      "* What do you do when your dataset is huge and will melt your laptop's RAM?\n",
      "* What the heck are \"memory mapping\" and Apache Arrow?\n",
      "* How can you create your own dataset and push it to the Hub?\n",
      "\n",
      "The techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let's get started!\n",
      "\n",
      "Next, the script wraps the base model and `peft_config` with the [`get_peft_model`] function to create a [`PeftModel`]. \n",
      "\n",
      "```diff\n",
      " def main():\n",
      "+    accelerator = Accelerator()\n",
      "     model_name_or_path = \"t5-base\"\n",
      "     base_path = \"temp/data/FinancialPhraseBank-v1.0\"\n",
      "+    peft_config = LoraConfig(\n",
      "         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
      "     )\n",
      "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
      "+   model = get_peft_model(model, peft_config)\n",
      "```\n",
      "\n",
      "Throughout the script, you'll see the [`~accelerate.Accelerator.main_process_first`] and [`~accelerate.Accelerator.wait_for_everyone`] functions which help control and synchronize when processes are executed.\n",
      "\n",
      "After your dataset is prepared, and all the necessary training components are loaded, the script checks if you're using the `fsdp_plugin`. PyTorch offers two ways for wrapping model layers in FSDP, automatically or manually. The simplest method is to allow FSDP to automatically recursively wrap model layers without changing any other code. You can choose to wrap the model layers based on the layer name or on the size (number of parameters). In the FSDP configuration file, it uses the `TRANSFORMER_BASED_WRAP` option to wrap the [`T5Block`] layer.\n",
      "\n",
      "```py\n",
      "if getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\n",
      "    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n",
      "```\n",
      "\n",
      "Next, use 🤗 Accelerate's [`~accelerate.Accelerator.prepare`] function to prepare the model, datasets, optimizer, and scheduler for training.\n",
      "\n",
      "```py\n",
      "model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n",
      "    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\n",
      ")\n",
      "```\n",
      "\n",
      "From here, the remainder of the script handles the training loop, evaluation, and sharing your model to the Hub.\n",
      "\n",
      "## Train\n",
      "\n",
      "For example, your FSDP configuration file may look like the following:\n",
      "\n",
      "```yaml\n",
      "command_file: null\n",
      "commands: null\n",
      "compute_environment: LOCAL_MACHINE\n",
      "deepspeed_config: {}\n",
      "distributed_type: FSDP\n",
      "downcast_bf16: 'no'\n",
      "dynamo_backend: 'NO'\n",
      "fsdp_config:\n",
      "  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n",
      "  fsdp_backward_prefetch_policy: BACKWARD_PRE\n",
      "  fsdp_offload_params: true\n",
      "  fsdp_sharding_strategy: 1\n",
      "  fsdp_state_dict_type: FULL_STATE_DICT\n",
      "  fsdp_transformer_layer_cls_to_wrap: T5Block\n",
      "gpu_ids: null\n",
      "machine_rank: 0\n",
      "main_process_ip: null\n",
      "main_process_port: null\n",
      "main_training_function: main\n",
      "megatron_lm_config: {}\n",
      "mixed_precision: 'no'\n",
      "num_machines: 1\n",
      "num_processes: 2\n",
      "rdzv_backend: static\n",
      "same_network: true\n",
      "tpu_name: null\n",
      "tpu_zone: null\n",
      "use_cpu: false\n",
      "```\n",
      "\n",
      "## The important parts\n",
      "\n",
      "Let's dig a bit deeper into the training script to understand how it works.\n",
      "\n",
      "The [`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14) function begins with initializing an [`~accelerate.Accelerator`] class which handles everything for distributed training, such as automatically detecting your training environment.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "💡 Feel free to change the model and dataset inside the `main` function. If your dataset format is different from the one in the script, you may also need to write your own preprocessing function. \n",
      "\n",
      "</Tip>\n",
      "\n",
      "The script also creates a configuration corresponding to the 🤗 PEFT method you're using. For LoRA, you'll use [`LoraConfig`] to specify the task type, and several other important parameters such as the dimension of the low-rank matrices, the matrices scaling factor, and the dropout probability of the LoRA layers. If you want to use a different 🤗 PEFT method, replace `LoraConfig` with the appropriate [class](../package_reference/tuners).\n",
      "\n",
      "Next, the script wraps the base model and `peft_config` with the [`get_peft_model`] function to create a [`PeftModel`].\n",
      "\n",
      "Let’s get started! All code is available in this [notebook](https://gitlab.com/juliensimon/huggingface-demos/-/blob/main/amazon-shoes/03_optimize_inc_quantize.ipynb). \n",
      "\n",
      "As usual, the first step is to install all required libraries. It’s worth mentioning that we have to work with a CPU-only version of PyTorch for the quantization process to work correctly.\n",
      "\n",
      "```\n",
      "pip -q uninstall torch -y \n",
      "pip -q install torch==1.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\n",
      "pip -q install transformers datasets optimum[neural-compressor] evaluate --upgrade\n",
      "```\n",
      "\n",
      "Then, we prepare an evaluation dataset to assess model performance during quantization. Starting from the dataset we used to fine-tune the original model, we only keep a few thousand reviews and their labels and save them to local storage.\n",
      "\n",
      "Next, we load the original model, its tokenizer, and the evaluation dataset from the Hugging Face hub.\n",
      "\n",
      "```\n",
      "from datasets import load_dataset\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "\n",
      "model_name = \"juliensimon/distilbert-amazon-shoe-reviews\"\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "eval_dataset = load_dataset(\"prashantgrao/amazon-shoe-reviews\", split=\"test\").select(range(300))\n",
      "```\n",
      "\n",
      "Next, we define an evaluation function that computes model metrics on the evaluation dataset. This allows the Optimum Intel library to compare these metrics before and after quantization. For this purpose, the Hugging Face [evaluate](https://github.com/huggingface/evaluate/) library is very convenient!\n",
      "\n",
      "```\n",
      "import evaluate\n",
      "\n",
      "Or:\n",
      "\n",
      "```python\n",
      "notebook_launcher(train_ddp_accelerate, args=(), num_processes=2)\n",
      "```\n",
      "\n",
      "## Using 🤗 Trainer\n",
      "\n",
      "Finally, we arrive at the highest level of API -- the Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).\n",
      "\n",
      "This wraps as much training as possible while still being able to train on distributed systems without the user needing to do anything at all.\n",
      "\n",
      "First we need to import the Trainer:\n",
      "\n",
      "```python\n",
      "from transformers import Trainer\n",
      "```\n",
      "\n",
      "Then we define some `TrainingArguments` to control all the usual hyper-parameters. The trainer also works through dictionaries, so a custom collate function needs to be made.\n",
      "\n",
      "Finally, we subclass the trainer and write our own `compute_loss`.\n",
      "\n",
      "Afterwards, this code will also work on a distributed setup without any training code needing to be written whatsoever!\n",
      "\n",
      "```python\n",
      "from transformers import Trainer, TrainingArguments\n",
      "\n",
      "model = BasicNet()\n",
      "\n",
      "training_args = TrainingArguments(\n",
      "    \"basic-trainer\",\n",
      "    per_device_train_batch_size=64,\n",
      "    per_device_eval_batch_size=64,\n",
      "    num_train_epochs=1,\n",
      "    evaluation_strategy=\"epoch\",\n",
      "    remove_unused_columns=False\n",
      ")\n",
      "\n",
      "def collate_fn(examples):\n",
      "    pixel_values = torch.stack([example[0] for example in examples])\n",
      "    labels = torch.tensor([example[1] for example in examples])\n",
      "    return {\"x\":pixel_values, \"labels\":labels}\n",
      "\n",
      "class MyTrainer(Trainer):\n",
      "    def compute_loss(self, model, inputs, return_outputs=False):\n",
      "        outputs = model(inputs[\"x\"])\n",
      "        target = inputs[\"labels\"]\n",
      "        loss = F.nll_loss(outputs, target)\n",
      "        return (loss, outputs) if return_outputs else loss\n",
      "\n",
      "trainer = MyTrainer(\n",
      "    model,\n",
      "    training_args,\n",
      "    train_dataset=train_dset,\n",
      "    eval_dataset=test_dset,\n",
      "    data_collator=collate_fn,\n",
      ")\n",
      "```\n",
      "\n",
      "```python\n",
      "trainer.train()\n",
      "```\n",
      "-----------------LLm ANSWER------------\n",
      "According to the context, the three main steps for fine-tuning a model with the 🤗 Datasets library are:\n",
      "\n",
      "1. Load a dataset from the Hugging Face Hub.\n",
      "2. Preprocess the data with `Dataset.map()`.\n",
      "3. Load and compute metrics.\n",
      "-----------------------------\n",
      "18\n",
      "huggingface/blog/blob/main/infinity-cpu-performance.md\n",
      "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n",
      "\n",
      "+800%\n",
      "+800%\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/infinity-cpu-performance.md', 'huggingface/blog/blob/main/infinity-cpu-performance.md', 'huggingface/blog/blob/main/infinity-cpu-performance.md', 'huggingface/blog/blob/main/infinity-cpu-performance.md', 'huggingface/blog/blob/main/infinity-cpu-performance.md']\n",
      "In this blog post, we will highlight a few results of the benchmark including the best latency and throughput configurations.\n",
      "\n",
      "In addition to this, we deployed the [DistilBERT](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) model we used for the benchmark as an API endpoint on 2 physical cores. You can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline.\n",
      "\n",
      "```bash\n",
      "curl --request POST `-i` \\\n",
      "  --url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\n",
      "  --header 'Content-Type: application/json' \\\n",
      "  --data '{\"inputs\":\"I like you. I love you\"}'\n",
      "```\n",
      "\n",
      "### Throughput\n",
      "\n",
      "Below you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers.\n",
      "\n",
      "<br>\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Throughput\" src=\"assets/46_infinity_cpu_performance/throughput.png\"></medium-zoom>\n",
      "  <figcaption>Figure 3. Throughput: Infinity vs Transformers</figcaption>\n",
      "</figure>\n",
      "<br>\n",
      "\n",
      "\n",
      "| Sequence Length | Infinity    | Transformers | improvement |\n",
      "|-----------------|-------------|--------------|-------------|\n",
      "| 8               | 248 req/sec | 49 req/sec   | +506%       |\n",
      "| 16              | 212 req/sec | 50 req/sec   | +424%       |\n",
      "| 32              | 150 req/sec | 40 req/sec   | +375%       |\n",
      "| 64              | 97 req/sec  | 28 req/sec   | +346%       |\n",
      "| 128             | 55 req/sec  | 18 req/sec   | +305%       |\n",
      "| 256             | 27 req/sec  | 9 req/sec    | +300%       |\n",
      "| 384             | 17 req/sec  | 5 req/sec    | +340%       |\n",
      "| 512             | 12 req/sec  | 4 req/sec    | +300%       |\n",
      "\n",
      "\n",
      "### Latency\n",
      "\n",
      "### Latency \n",
      "\n",
      "Below, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1. It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark.  \n",
      "\n",
      "<br>\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Latency\" src=\"assets/46_infinity_cpu_performance/latency.png\"></medium-zoom>\n",
      "  <figcaption>Figure 4. Latency (Batch=1, Physical Cores=2)</figcaption>\n",
      "</figure>\n",
      "<br>\n",
      "\n",
      "---\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "In this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n",
      "\n",
      "Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n",
      "\n",
      "The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n",
      "\n",
      "If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n",
      "\n",
      "\n",
      "## Resources\n",
      "\n",
      "### Methodologies\n",
      "\n",
      "When it comes to benchmarking BERT-like models, two metrics are most adopted:\n",
      "* **Latency**: Time it takes for a single prediction of the model (pre-process, prediction, post-process)\n",
      "* **Throughput**: Number of executions performed in a fixed amount of time for one benchmark configuration, respecting Physical CPU cores, Sequence Length, and Batch Size\n",
      "\n",
      "These two metrics will be used to benchmark Hugging Face Infinity across different setups to understand the benefits and tradeoffs in this blog post.\n",
      "\n",
      "---\n",
      "\n",
      "## Results\n",
      "\n",
      "To run the benchmark, we created an infinity container for the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) (Ice-lake) and optimized a [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model for sequence classification using Infinity Multiverse. \n",
      "\n",
      "This ice-lake optimized Infinity Container can achieve up to 34% better latency & throughput compared to existing cascade-lake-based instances, and up to 800% better latency & throughput compared to vanilla transformers running on ice-lake.\n",
      "\n",
      "The Benchmark we created consists of 192 different experiments and configurations. We ran experiments for: \n",
      "* Physical CPU cores: 1, 2, 4, 8\n",
      "* Sequence length: 8, 16, 32, 64, 128, 256, 384, 512\n",
      "* Batch_size: 1, 2, 4, 8, 16, 32\n",
      "\n",
      "In each experiment, we collect numbers for:\n",
      "* Throughput (requests per second)\n",
      "* Latency (min, max, avg, p90, p95, p99)\n",
      "\n",
      "You can find the full data of the benchmark in this google spreadsheet: [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing).\n",
      "\n",
      "In this blog post, we will highlight a few results of the benchmark including the best latency and throughput configurations.\n",
      "\n",
      "The main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\n",
      "\n",
      "With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n",
      "\n",
      "\n",
      "## What is Hugging Face Infinity\n",
      "\n",
      "Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.\n",
      "\n",
      "Hugging Face Infinity consists of 2 main services:\n",
      "* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.\n",
      "* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.\n",
      "\n",
      "The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.\n",
      "\n",
      "<br>\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Product overview\" src=\"assets/46_infinity_cpu_performance/overview.png\"></medium-zoom>\n",
      "  <figcaption>Figure 1. Infinity Overview</figcaption>\n",
      "</figure>\n",
      "<br>\n",
      "\n",
      "An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification.\n",
      "\n",
      "You can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).\n",
      "\n",
      "---\n",
      "\n",
      "## Benchmark \n",
      "\n",
      "Inference performance benchmarks often only measure the execution of the model. In this blog post, and when discussing the performance of Infinity, we always measure the end-to-end pipeline including pre-processing, prediction, post-processing. Please keep this in mind when comparing these results with other latency measurements. \n",
      "\n",
      "<br>\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pipeline\" src=\"assets/46_infinity_cpu_performance/pipeline.png\"></medium-zoom>\n",
      "  <figcaption>Figure 2. Infinity End-to-End Pipeline</figcaption>\n",
      "</figure>\n",
      "<br>\n",
      "\n",
      "### Environment\n",
      "\n",
      "As a benchmark environment, we are going to use the [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i), which are compute-optimized instances powered by the 3rd generation of Intel Xeon Scalable processors. These new Intel-based instances are using the ice-lake Process Technology and support Intel AVX-512, Intel Turbo Boost, and Intel Deep Learning Boost.\n",
      "\n",
      "In addition to superior performance for machine learning workloads, the Intel Ice Lake C6i instances offer great cost-performance and are our recommendation to deploy Infinity on Amazon Web Services.  To learn more, visit the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) page.\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided data, the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is 800%. This is seen in the \"Throughput\" table, where the throughput of Infinity is compared to vanilla transformers for sequence lengths of 8, 16, 32, 64, 128, 256, 384, and 512.\n",
      "-----------------------------\n",
      "19\n",
      "huggingface/blog/blob/main/spacy.md\n",
      "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n",
      "\n",
      "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/spacy.md', 'huggingface/blog/blob/main/spacy.md', 'huggingface/blog/blob/main/skops.md', 'huggingface/blog/blob/main/spacy.md', 'huggingface/hub-docs/blob/main/docs/hub/espnet.md']\n",
      "### Using existing models\n",
      "\n",
      "All models from the Hub can be directly installed using `pip install`. \n",
      "\n",
      "\n",
      "```bash\n",
      "pip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\n",
      "```\n",
      "\n",
      "```python\n",
      "# Using spacy.load().\n",
      "import spacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "# Importing as module.\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "```\n",
      "\n",
      "When you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!\n",
      "\n",
      "![snippet](assets/23_spacy/snippet.png)\n",
      "![snippet](assets/23_spacy/snippet2.png)\n",
      "\n",
      "You can even make HTTP requests to call the models from the Inference API, which is useful in production settings. Here is an example of a simple request:\n",
      "\n",
      "```bash\n",
      "curl -X POST  --data '{\"inputs\": \"Hello, this is Omar\"}' https://api-inference.huggingface.co/models/spacy/en_core_web_sm\n",
      ">>> [{\"entity_group\":\"PERSON\",\"word\":\"Omar\",\"start\":15,\"end\":19,\"score\":1.0}]\n",
      "```\n",
      "\n",
      "And for larger-scale use cases, you can click \"Deploy > Accelerated Inference\" and see how to do this with Python.\n",
      "\n",
      "\n",
      "### Sharing your models\n",
      "\n",
      "But probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \n",
      "\n",
      "```bash\n",
      "huggingface-cli login\n",
      "python -m spacy package ./en_ner_fashion ./output --build wheel\n",
      "cd ./output/en_ner_fashion-0.0.0/dist\n",
      "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "```\n",
      "\n",
      "In just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n",
      "\n",
      "Try it out and share your models with the community!\n",
      "\n",
      "## Would you like to integrate your library to the Hub?\n",
      "\n",
      "--\n",
      "title: \"Welcome spaCy to the Hugging Face Hub\"\n",
      "thumbnail: /blog/assets/23_spacy/thumbnail.png\n",
      "\n",
      "authors:\n",
      "- user: osanseviero\n",
      "- user: ines\n",
      "---\n",
      "\n",
      "# Welcome spaCy to the Hugging Face Hub\n",
      "\n",
      "\n",
      "[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n",
      "\n",
      "Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n",
      "\n",
      "### Finding models\n",
      "\n",
      "Over 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n",
      "\n",
      "\n",
      "### Widgets\n",
      "\n",
      "This integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS.\n",
      "\n",
      "We can also add any plot of our choice to the card using `add_plot` like below.\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "from pathlib import Path\n",
      "# we will create a confusion matrix\n",
      "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
      "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
      "disp.plot()\n",
      "\n",
      "# save the plot\n",
      "plt.savefig(Path(local_repo) / \"confusion_matrix.png\")\n",
      "\n",
      "# the plot will be written to the model card under the name confusion_matrix\n",
      "# we pass the path of the plot itself\n",
      "model_card.add_plot(confusion_matrix=\"confusion_matrix.png\")\n",
      "```\n",
      "\n",
      "Let's save the model card in the local repository. The file name here should be `README.md` since it is what Hugging Face Hub expects.\n",
      "```python\n",
      "model_card.save(Path(local_repo) / \"README.md\")\n",
      "```\n",
      "\n",
      "We can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either  `notebook_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI.\n",
      "\n",
      "```python\n",
      "# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\n",
      "repo_id = \"skops-user/my-awesome-model\"\n",
      "hub_utils.push(\n",
      "    repo_id=repo_id,\n",
      "    source=local_repo,\n",
      "    token=token,\n",
      "    commit_message=\"pushing files to the repo from the example!\",\n",
      "    create_remote=True,\n",
      ")\n",
      "```\n",
      "\n",
      "Once we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n",
      "\n",
      "```python\n",
      "download_repo = \"downloaded-model\"\n",
      "hub_utils.download(repo_id=repo_id, dst=download_repo)\n",
      "```\n",
      "\n",
      "The inference widget is enabled to make predictions in the repository.\n",
      "\n",
      "![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n",
      "\n",
      "Try it out and share your models with the community!\n",
      "\n",
      "## Would you like to integrate your library to the Hub?\n",
      "\n",
      "This integration is possible thanks to the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you!\n",
      "\n",
      "```py\n",
      "import soundfile\n",
      "from espnet2.bin.tts_inference import Text2Speech\n",
      "\n",
      "text2speech = Text2Speech.from_pretrained(\"model_name\")\n",
      "speech = text2speech(\"foobar\")[\"wav\"]\n",
      "soundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n",
      "```\n",
      "\n",
      "If you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\n",
      "<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\n",
      "</div>\n",
      "\n",
      "## Sharing your models\n",
      "\n",
      "`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\n",
      "\n",
      "The `run.sh` script allows to upload a given model to a Hugging Face repository.\n",
      "\n",
      "```bash\n",
      "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "```\n",
      "\n",
      "## Additional resources\n",
      "\n",
      "* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n",
      "* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n",
      "* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).\n",
      "-----------------LLm ANSWER------------\n",
      "The command to upload a spaCy pipeline to the Hugging Face Hub is:\n",
      "\n",
      "```\n",
      "huggingface-cli login\n",
      "python -m spacy package./en_ner_fashion./output --build wheel\n",
      "cd./output/en_ner_fashion-0.0.0/dist\n",
      "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "```\n",
      "-----------------------------\n",
      "20\n",
      "huggingface/blog/blob/main/nystromformer.md\n",
      "What is the time and memory complexity of the Nyströmformer's approximation of self-attention?\n",
      "\n",
      "O(n)\n",
      "O(n)\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/nystromformer.md', 'huggingface/blog/blob/main/nystromformer.md', 'huggingface/blog/blob/main/nystromformer.md', 'huggingface/blog/blob/main/nystromformer.md', 'huggingface/blog/blob/main/nystromformer.md']\n",
      "--\n",
      "title: \"Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\"\n",
      "thumbnail: /blog/assets/86_nystromformer/thumbnail.png\n",
      "authors:\n",
      "- user: asi\n",
      "  guest: true\n",
      "---\n",
      "\n",
      "# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\n",
      "\n",
      "\n",
      "<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Transformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\(O(n^2)\\\\) (where \\\\(n\\\\) is the length of the input sequence), making it expensive to train on long input sequences. \n",
      "\n",
      "The [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention.\n",
      "\n",
      "\n",
      "## Nyström method for matrix approximation\n",
      "\n",
      "At the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let's consider a matrix \\\\(P^{n \\times n}\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\(m\\\\) rows and columns from \\\\(P\\\\). We can then arrange the sampled rows and columns as follows:\n",
      "\n",
      "$$\\begin{aligned}\\hat{S} &= \\tilde{F} \\tilde{A} \\tilde{B} \\\\ &= softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+  softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}}) \\end{aligned}$$\n",
      "\n",
      "This is the Nyström approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( \\\\(V\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\(QK^T\\\\), avoiding the \\\\(O(n^2)\\\\) complexity. \n",
      "\n",
      "\n",
      "## How do we select landmarks?\n",
      "\n",
      "Instead of sampling \\\\(m\\\\) rows from \\\\(Q\\\\) and \\\\(K\\\\), the authors propose to construct \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\)\n",
      "using segment means. In this procedure, \\\\(n\\\\) tokens are grouped into \\\\(m\\\\) segments, and the mean of each segment is computed. Ideally, \\\\(m\\\\) is much smaller than \\\\(n\\\\). According to experiments from the paper, selecting just \\\\(32\\\\) or \\\\(64\\\\) landmarks produces competetive performance compared to standard self-attention and other efficient attention mechanisms, even for long sequences lengths ( \\\\(n=4096\\\\) or \\\\(8192\\\\)). \n",
      "\n",
      "The overall algorithm is summarised by the following figure from the paper:\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Efficient self-attention with the Nyström method\" src=\"assets/86_nystromformer/paper_figure.png\"></medium-zoom>\n",
      "  <figcaption>Efficient self-attention with the Nyström method</figcaption>\n",
      "</figure>\n",
      "\n",
      "The three orange matrices above correspond to the three matrices we constructed using the key and query landmarks. Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution.\n",
      "\n",
      "\n",
      "## How is Nyströmformer implemented?\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Representing P as a block matrix\" src=\"assets/86_nystromformer/p_block.png\"></medium-zoom>\n",
      "  <figcaption>Representing P as a block matrix</figcaption>\n",
      "</figure>\n",
      "\n",
      "We now have four submatrices: \\\\(A_P, B_P, F_P,\\\\) and \\\\(C_P\\\\), with sizes \\\\(m \\times m, m \\times (n - m), (n - m) \\times m\\\\) and \n",
      "\\\\((n - m) \\times (n - m)\\\\) respectively. The \\\\(m\\\\) sampled columns are contained in \\\\(A_P\\\\) and \\\\(F_P\\\\), whereas the \\\\(m\\\\) sampled rows are contained in \\\\(A_P\\\\) and \\\\(B_P\\\\). So, the entries of \\\\(A_P, B_P,\\\\) and \\\\(F_P\\\\) are known to us, and we will estimate \\\\(C_P\\\\). According to the Nyström method, \\\\(C_P\\\\) is given by:\n",
      "\n",
      "$$C_P = F_P A_P^+ B_P$$\n",
      "\n",
      "Here, \\\\(+\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). \n",
      "Thus, the Nyström approximation of \\\\(P, \\hat{P}\\\\) can be written as:\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nyström approximation of P\" src=\"assets/86_nystromformer/p_hat.png\"></medium-zoom>\n",
      "  <figcaption>Nyström approximation of P</figcaption>\n",
      "</figure>\n",
      "\n",
      "As shown in the second line, \\\\(\\hat{P}\\\\) can be expressed as a product of three matrices. The reason for doing so will become clear later.\n",
      "\n",
      "\n",
      "## Can we approximate self-attention with the Nyström method?\n",
      "\n",
      "Our goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\n",
      "\n",
      "Here, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nyström approximation of S\" src=\"assets/86_nystromformer/s_hat.png\"></medium-zoom>\n",
      "  <figcaption>Nyström approximation of S</figcaption>\n",
      "</figure>\n",
      "\n",
      "## How is Nyströmformer implemented?\n",
      "\n",
      "The original implementation of Nyströmformer can be found [here](https://github.com/mlpen/Nystromformer) and the HuggingFace implementation can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nystromformer/modeling_nystromformer.py). Let's take a look at a few lines of code (with some comments added) from the HuggingFace implementation. Note that some details such as normalization, attention masking, and depthwise convolution are avoided for simplicity.\n",
      "\n",
      "```python\n",
      "\n",
      "key_layer = self.transpose_for_scores(self.key(hidden_states)) # K\n",
      "value_layer = self.transpose_for_scores(self.value(hidden_states)) # V\n",
      "query_layer = self.transpose_for_scores(mixed_query_layer) # Q\n",
      "\n",
      "q_landmarks = query_layer.reshape(\n",
      "    -1,\n",
      "    self.num_attention_heads,\n",
      "    self.num_landmarks,\n",
      "    self.seq_len // self.num_landmarks,\n",
      "    self.attention_head_size,\n",
      ").mean(dim=-2) # \\tilde{Q}\n",
      "\n",
      "k_landmarks = key_layer.reshape(\n",
      "    -1,\n",
      "    self.num_attention_heads,\n",
      "    self.num_landmarks,\n",
      "    self.seq_len // self.num_landmarks,\n",
      "    self.attention_head_size,\n",
      ").mean(dim=-2) # \\tilde{K}\n",
      "\n",
      "kernel_1 = torch.nn.functional.softmax(torch.matmul(query_layer, k_landmarks.transpose(-1, -2)), dim=-1) # \\tilde{F}\n",
      "kernel_2 = torch.nn.functional.softmax(torch.matmul(q_landmarks, k_landmarks.transpose(-1, -2)), dim=-1) # \\tilde{A} before pseudo-inverse\n",
      "\n",
      "attention_scores = torch.matmul(q_landmarks, key_layer.transpose(-1, -2)) # \\tilde{B} before softmax\n",
      "\n",
      "kernel_3 = nn.functional.softmax(attention_scores, dim=-1) # \\tilde{B}\n",
      "attention_probs = torch.matmul(kernel_1, self.iterative_inv(kernel_2)) # \\tilde{F} * \\tilde{A}\n",
      "new_value_layer = torch.matmul(kernel_3, value_layer) # \\tilde{B} * V\n",
      "context_layer = torch.matmul(attention_probs, new_value_layer) # \\tilde{F} * \\tilde{A} * \\tilde{B} * V\n",
      "```\n",
      "\n",
      "\n",
      "## Using Nyströmformer with HuggingFace\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nyström approximation of S\" src=\"assets/86_nystromformer/s_hat.png\"></medium-zoom>\n",
      "  <figcaption>Nyström approximation of S</figcaption>\n",
      "</figure>\n",
      "\n",
      "But, what does it mean to sample a column from \\\\(S\\\\)? It means we select one element from each row. Recall how S is calculated: the final operation is a row-wise softmax. To find a single entry in a row, we must access all other entries (for the denominator in softmax). So, sampling one column requires us to know all other columns in the matrix. Therefore, we cannot directly apply the Nyström method to approximate the softmax matrix.\n",
      "\n",
      "\n",
      "## How can we adapt the Nyström method to approximate self-attention?\n",
      "\n",
      "Instead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nyström points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nyström approximation of \\\\(S\\\\). We define the following matrices:\n",
      "\n",
      "$$\\tilde{F} = softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) \\hspace{40pt} \\tilde{A} = softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+ \\hspace{40pt} \\tilde{B} = softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}})$$\n",
      "\n",
      "The sizes of \\\\(\\tilde{F}\\\\), \\\\(\\tilde{A}\\\\), and \\\\(\\tilde{B}) are \\\\(n \\times m, m \\times m,\\\\) and \\\\(m \\times n\\\\) respectively. \n",
      "We replace the three matrices in the Nyström approximation of \\\\(S\\\\) with the new matrices we have defined to obtain an alternative Nyström approximation:\n",
      "\n",
      "$$\\begin{aligned}\\hat{S} &= \\tilde{F} \\tilde{A} \\tilde{B} \\\\ &= softmax(\\frac{Q\\tilde{K}^T}{\\sqrt{d}}) softmax(\\frac{\\tilde{Q}\\tilde{K}^T}{\\sqrt{d}})^+  softmax(\\frac{\\tilde{Q}K^T}{\\sqrt{d}}) \\end{aligned}$$\n",
      "-----------------LLm ANSWER------------\n",
      "The time and memory complexity of the Nyströmformer's approximation of self-attention is O(n) complexity.\n",
      "-----------------------------\n",
      "21\n",
      "huggingface/course/blob/main/subtitles/en/raw/tasks.md\n",
      "What is the goal of the Named Entity Recognition task in token classification?\n",
      "\n",
      "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n",
      "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/subtitles/en/raw/tasks.md', 'huggingface/course/blob/main/chapters/en/chapter1/10.mdx', 'huggingface/blog/blob/main/spacy.md', 'huggingface/blog/blob/main/spacy.md', 'huggingface/blog/blob/main/spacy.md']\n",
      "ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI_JMQtyEr-sLJSy5_SnLCb4vtQf\n",
      "\n",
      "Token Classification video\n",
      "\n",
      "Welcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task.\n",
      "Token classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\n",
      "Let’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\n",
      "Another token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\n",
      "Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\n",
      "An example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\n",
      "You can extract important information from invoices using named entity recognition models, such as date, organization name or address.\n",
      "For more information about the Token classification task, check out the Hugging Face course.\n",
      "\n",
      "\n",
      "Question Answering video\n",
      "\n",
      "!-- DISABLE-FRONTMATTER-SECTIONS -->\n",
      "\n",
      "# End-of-chapter quiz[[end-of-chapter-quiz]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={1}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "This chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n",
      "\n",
      "First, though, let's test what you learned in this chapter!\n",
      "\n",
      "\n",
      "### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\n",
      "\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"Summarization\",\n",
      "\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Text classification\",\n",
      "\t\t\texplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called <em>natural language inference</em>.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Text generation\",\n",
      "\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 2. What will the following code return?\n",
      "\n",
      "```py\n",
      "from transformers import pipeline\n",
      "\n",
      "ner = pipeline(\"ner\", grouped_entities=True)\n",
      "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
      "```\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return classification scores for this sentence, with labels \\\"positive\\\" or \\\"negative\\\".\",\n",
      "\t\t\texplain: \"This is incorrect — this would be a <code>sentiment-analysis</code> pipeline.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return a generated text completing this sentence.\",\n",
      "\t\t\texplain: \"This is incorrect — it would be a <code>text-generation</code> pipeline.\",\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return the words representing persons, organizations or locations.\",\n",
      "\t\t\texplain: \"Furthermore, with <code>grouped_entities=True</code>, it will group together the words belonging to the same entity, like \\\"Hugging Face\\\".\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "<div class=\"SVELTE_HYDRATER \" data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingface.co&quot;,&quot;model&quot;:{&quot;author&quot;:&quot;spacy&quot;,&quot;autoArchitecture&quot;:&quot;AutoModel&quot;,&quot;branch&quot;:&quot;main&quot;,&quot;cardData&quot;:{&quot;tags&quot;:[&quot;spacy&quot;,&quot;token-classification&quot;],&quot;language&quot;:[&quot;en&quot;],&quot;license&quot;:&quot;MIT&quot;,&quot;model-index&quot;:[{&quot;name&quot;:&quot;en_core_web_sm&quot;,&quot;results&quot;:[{&quot;tasks&quot;:{&quot;name&quot;:&quot;NER&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Precision&quot;,&quot;type&quot;:&quot;precision&quot;,&quot;value&quot;:0.8424355924},{&quot;name&quot;:&quot;Recall&quot;,&quot;type&quot;:&quot;recall&quot;,&quot;value&quot;:0.8335336538},{&quot;name&quot;:&quot;F Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.8379609817}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;POS&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9720712187}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;SENTER&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Precision&quot;,&quot;type&quot;:&quot;precision&quot;,&quot;value&quot;:0.9074955788},{&quot;name&quot;:&quot;Recall&quot;,&quot;type&quot;:&quot;recall&quot;,&quot;value&quot;:0.8801372122},{&quot;name&quot;:&quot;F\n",
      "\n",
      "9.66867 12.6813C9.46478 12.4774 9.18834 12.3628 8.90001 12.3625V12.3625ZM2.37501 12.3625V9.09998H5.63751V12.3625H2.37501ZM6.72501 15.625V13.45H8.90001V15.625H6.72501Z\"></path><path d=\"M15.425 16.7125H13.25C12.9617 16.7122 12.6852 16.5976 12.4813 16.3937C12.2775 16.1898 12.1628 15.9134 12.1625 15.625V13.45C12.1628 13.1617 12.2775 12.8852 12.4813 12.6814C12.6852 12.4775 12.9617 12.3628 13.25 12.3625H15.425C15.7133 12.3628 15.9898 12.4775 16.1937 12.6814C16.3976 12.8852 16.5122 13.1617 16.5125 13.45V15.625C16.5122 15.9134 16.3976 16.1898 16.1937 16.3937C15.9898 16.5976 15.7133 16.7122 15.425 16.7125ZM13.25 13.45V15.625H15.425V13.45H13.25Z\"></path><path d=\"M15.425 1.48752H12.1625C11.8742 1.48781 11.5977 1.60247 11.3938 1.80636C11.19 2.01024 11.0753 2.28668 11.075 2.57502V5.83752H9.98751C9.69917 5.83781 9.42273 5.95247 9.21885 6.15636C9.01496 6.36024 8.9003 6.63668 8.90001 6.92502V8.01252C8.9003 8.30085 9.01496 8.5773 9.21885 8.78118C9.42273 8.98506 9.69917 9.09973 9.98751 9.10002H11.075C11.3633 9.09973 11.6398 8.98506 11.8437 8.78118C12.0476 8.5773 12.1622 8.30085 12.1625 8.01252V6.92502H15.425C15.7133 6.92473 15.9898 6.81006 16.1937 6.60618C16.3976 6.4023 16.5122 6.12585 16.5125 5.83752V2.57502C16.5122 2.28668 16.3976 2.01024 16.1937 1.80636C15.9898 1.60247 15.7133 1.48781 15.425 1.48752ZM9.98751 8.01252V6.92502H11.075V8.01252H9.98751ZM12.1625 5.83752V2.57502H15.425V5.83752H12.1625Z\"></path><path d=\"M4.55001 5.83752H2.37501C2.08667 5.83723 1.81023 5.72256 1.60635 5.51868C1.40246 5.3148 1.28779 5.03835 1.28751 4.75002V2.57502C1.28779 2.28668 1.40246 2.01024 1.60635 1.80636C1.81023 1.60247 2.08667 1.48781 2.37501 1.48752H4.55001C4.83834 1.48781 5.11478 1.60247 5.31867 1.80636C5.52255 2.01024 5.63722 2.28668 5.63751 2.57502V4.75002C5.63722 5.03835 5.52255 5.3148 5.31867 5.51868C5.11478 5.72256 4.83834 5.83723 4.55001 5.83752V5.83752ZM2.37501 2.57502V4.75002H4.55001V2.57502H2.37501Z\"></path></svg> <span>Token Classification</span></div> <div class=\"ml-auto\"></div></div>\n",
      "\n",
      "5.03835 5.52255 5.3148 5.31867 5.51868C5.11478 5.72256 4.83834 5.83723 4.55001 5.83752V5.83752ZM2.37501 2.57502V4.75002H4.55001V2.57502H2.37501Z\"></path></svg> <span>Token Classification</span></div> <div class=\"ml-auto\"></div></div> <form><div class=\"flex h-10\"><input class=\"form-input-alt flex-1 rounded-r-none \" placeholder=\"Your sentence here...\" required=\"\" type=\"text\"> <button class=\"btn-widget w-24 h-10 px-5 rounded-l-none border-l-0 \" type=\"submit\">Compute</button></div></form> <div class=\"mt-1.5\"><div class=\"text-gray-400 text-xs\">This model is currently loaded and running on the Inference API.</div> </div>   <div class=\"mt-auto pt-4 flex items-center text-xs text-gray-500\"><button class=\"flex items-center cursor-not-allowed text-gray-300\" disabled=\"\"><svg class=\"mr-1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\" style=\"transform: rotate(360deg);\"><path d=\"M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z\" fill=\"currentColor\"></path><path d=\"M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z\" fill=\"currentColor\"></path><path d=\"M12.419 25.484L17.639 6l1.932.518L14.35 26z\" fill=\"currentColor\"></path></svg>\n",
      "-----------------LLm ANSWER------------\n",
      "The goal of the Named Entity Recognition task in token classification is to find the entities in a piece of text, such as person, location, or organization.\n",
      "-----------------------------\n",
      "22\n",
      "huggingface/blog/blob/main/clipseg-zero-shot.md\n",
      "What is the resolution of images used by the CLIPSeg model?\n",
      "\n",
      "352 x 352 pixels\n",
      "352 x 352 pixels\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/clipseg-zero-shot.md', 'huggingface/blog/blob/main/clipseg-zero-shot.md', 'huggingface/blog/blob/main/clipseg-zero-shot.md', 'huggingface/blog/blob/main/clipseg-zero-shot.md', 'huggingface/blog/blob/main/clipseg-zero-shot.md']\n",
      "One limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? That’s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves.\n",
      "\n",
      "Currently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works.\n",
      "\n",
      "## CLIP: the magic model behind CLIPSeg\n",
      "\n",
      "```python\n",
      "_, ax = plt.subplots(1, 2, figsize=(6, 4))\n",
      "[a.axis('off') for a in ax.flatten()]\n",
      "ax[0].imshow(image)\n",
      "ax[1].imshow(torch.sigmoid(preds[0]))\n",
      "```\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The mask of the coffee cup in the breakfast image.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/7f75badfc245fc3a75e0e05058b8c4b6a3a991fa.png\"></medium-zoom>\n",
      "</figure>\n",
      "\n",
      "In this case, the result is pretty much the same. This is probably\n",
      "because the coffee cup was already separated well from the background in\n",
      "the original image.\n",
      "\n",
      "## Using CLIPSeg to pre-label images on Segments.ai\n",
      "\n",
      "As you can see, the results from CLIPSeg are a little fuzzy and very\n",
      "low-res. If we want to obtain better results, you can fine-tune a\n",
      "state-of-the-art segmentation model, as explained in [our previous\n",
      "blogpost](https://huggingface.co/blog/fine-tune-segformer). To finetune\n",
      "the model, we\\'ll need labeled data. In this section, we\\'ll show you\n",
      "how you can use CLIPSeg to create some rough segmentation masks and then\n",
      "refine them on\n",
      "[Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg),\n",
      "a labeling platform with smart labeling tools for image segmentation.\n",
      "\n",
      "First, create an account at\n",
      "[https://segments.ai/join](https://segments.ai/join?utm_source=hf&utm_medium=blog&utm_campaign=clipseg)\n",
      "and install the Segments Python SDK. Then you can initialize the\n",
      "Segments.ai Python client using an API key. This key can be found on\n",
      "[the account page](https://segments.ai/account?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).\n",
      "\n",
      "```python\n",
      "!pip install -q segments-ai\n",
      "```\n",
      "\n",
      "```python\n",
      "from segments import SegmentsClient\n",
      "from getpass import getpass\n",
      "\n",
      "api_key = getpass('Enter your API key: ')\n",
      "segments_client = SegmentsClient(api_key)\n",
      "```\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-full\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png\"></medium-zoom>\n",
      "  <figcaption><a href=\"https://arxiv.org/abs/2112.10003\">Source</a></figcaption>\n",
      "</figure>\n",
      "\n",
      "The decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks. The authors also experimented with various augmentations to expand the size of the dataset. The goal here is not only to be able to segment the categories that are present in the dataset, but also to segment unseen categories. Experiments indeed show that the decoder can generalize to unseen categories. \n",
      "\n",
      "One interesting feature of CLIPSeg is that both the query (the image we want to segment) and the prompt (the thing we want to segment in the image) are input as CLIP embeddings. The CLIP embedding for the prompt can either come from a piece of text (the category name), **or from another image**. This means you can segment oranges in a photo by giving CLIPSeg an example image of an orange.\n",
      "\n",
      "This technique, which is called \"visual prompting\", is really helpful when the thing you want to segment is hard to describe. For example, if you want to segment a logo in a picture of a t-shirt, it's not easy to describe the shape of the logo, but CLIPSeg allows you to simply use the image of the logo as the prompt.\n",
      "\n",
      "The CLIPSeg paper contains some tips on improving the effectiveness of visual prompting. They find that cropping the query image (so that it only contains the object you want to segment) helps a lot. Blurring and darkening the background of the query image also helps a little bit. In the next section, we'll show how you can try out visual prompting yourself using [`🤗 transformers`](https://huggingface.co/transformers).\n",
      "\n",
      "What’s more, CLIP is not only useful for classification, but it can also be used for [image search](https://huggingface.co/spaces/DrishtiSharma/Text-to-Image-search-using-CLIP) (can you see how this is similar to classification?), [text-to-image models](https://huggingface.co/spaces/kamiyamai/stable-diffusion-webui) ([DALL-E 2](https://openai.com/dall-e-2/) is powered by CLIP), [object detection](https://segments.ai/zeroshot?utm_source=hf&utm_medium=blog&utm_campaign=clipseg) ([OWL-ViT](https://arxiv.org/abs/2205.06230)), and most importantly for us: image segmentation. Now you see why CLIP was truly a breakthrough in machine learning.\n",
      "\n",
      "The reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them.\n",
      "\n",
      "## CLIPSeg: image segmentation with CLIP\n",
      "\n",
      "[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo Lüddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn’t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP.\n",
      "\n",
      "```python\n",
      "!pip install -q segments-ai\n",
      "```\n",
      "\n",
      "```python\n",
      "from segments import SegmentsClient\n",
      "from getpass import getpass\n",
      "\n",
      "api_key = getpass('Enter your API key: ')\n",
      "segments_client = SegmentsClient(api_key)\n",
      "```\n",
      "\n",
      "Next, let\\'s load an image from a dataset using the Segments client.\n",
      "We\\'ll use the [a2d2 self-driving\n",
      "dataset](https://www.a2d2.audi/a2d2/en.html). You can also create your\n",
      "own dataset by following [these\n",
      "instructions](https://docs.segments.ai/tutorials/getting-started?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).\n",
      "\n",
      "```python\n",
      "samples = segments_client.get_samples(\"admin-tobias/clipseg\")\n",
      "\n",
      "# Use the last image as an example\n",
      "sample = samples[1]\n",
      "image = Image.open(requests.get(sample.attributes.image.url, stream=True).raw)\n",
      "image\n",
      "```\n",
      "\n",
      "<figure class=\"image table text-center m-0 w-9/12\">\n",
      "  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A picture of a street with cars from the a2d2 dataset.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/a0ca3accab5a40547f16b2abc05edd4558818bdf.png\"></medium-zoom>\n",
      "</figure>\n",
      "\n",
      "We also need to get the category names from the dataset attributes.\n",
      "\n",
      "```python\n",
      "dataset = segments_client.get_dataset(\"admin-tobias/clipseg\")\n",
      "category_names = [category.name for category in dataset.task_attributes.categories]\n",
      "```\n",
      "\n",
      "Now we can use CLIPSeg on the image as before. This time, we\\'ll also\n",
      "scale up the outputs so that they match the input image\\'s size.\n",
      "\n",
      "```python\n",
      "from torch import nn\n",
      "\n",
      "inputs = processor(text=category_names, images=[image] * len(category_names), padding=\"max_length\", return_tensors=\"pt\")\n",
      "\n",
      "# predict\n",
      "with torch.no_grad():\n",
      "  outputs = model(**inputs)\n",
      "\n",
      "# resize the outputs\n",
      "preds = nn.functional.interpolate(\n",
      "    outputs.logits.unsqueeze(1),\n",
      "    size=(image.size[1], image.size[0]),\n",
      "    mode=\"bilinear\"\n",
      ")\n",
      "```\n",
      "\n",
      "And we can visualize the results again.\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, the CLIPSeg model uses images of 352 x 352 pixels.\n",
      "-----------------------------\n",
      "23\n",
      "huggingface/course/blob/main/chapters/en/chapter9/9.mdx\n",
      "What can you use Gradio for?\n",
      "\n",
      "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n",
      "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/chapters/en/chapter9/9.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/9.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/7.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/7.mdx', 'gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md']\n",
      "!-- DISABLE-FRONTMATTER-SECTIONS -->\n",
      "\n",
      "# End-of-chapter quiz[[end-of-chapter-quiz]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={9}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "Let's test what you learned in this chapter!\n",
      "\n",
      "### 1. What can you use Gradio to do?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Create a demo for your machine learning model\",\n",
      "\t\t\texplain: \"With a few lines of python code you can generate a demo for your ML model using our library of pre-built components.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Share your machine learning model with others\",\n",
      "\t\t\texplain: \"Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone.\",\n",
      "            correct: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Debug your model\",\n",
      "\t\t\texplain: \"One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Train your model\",\n",
      "\t\t\texplain: \"Gradio is designed to be used for model inference, AFTER your model is trained.\",\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 2. Gradio ONLY works with PyTorch models\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"True\",\n",
      "\t\t\texplain: \"Gradio works with PyTorch models, but also works for any type of machine learning model!\"\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"False\",\n",
      "\t\t\texplain: \"Gradio is model agnostic, meaning you can create a demo for any type of machine learning model.\",\n",
      "\t\t\tcorrect: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 3. Where can you launch a Gradio demo from?\n",
      "\n",
      "### 3. Where can you launch a Gradio demo from?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Standard python IDEs\",\n",
      "\t\t\texplain: \"Gradio works great with your favorite IDE.\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Google Colab notebooks\",\n",
      "\t\t\texplain: \"You can create and launch a demo within your Google colab notebook.\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Jupyter notebooks\",\n",
      "\t\t\texplain: \"Good choice - You can create and launch a demo within your Jupyter notebook.\",\n",
      "\t\t\tcorrect: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 4. Gradio is designed primarily for NLP models\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"True\",\n",
      "\t\t\texplain: \"Gradio works with pretty much any data type, not just NLP.\"\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"False\",\n",
      "\t\t\texplain: \"Gradio supplies developers with a library of pre-built components for pretty much all data types.\",\n",
      "            correct: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 5. Which of the following features are supported by Gradio?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Multiple inputs and outputs\",\n",
      "\t\t\texplain: \"Multiple inputs and outputs is possible with gradio. All you need to do is pass in a list of inputs and outputs to their corresponding parameters\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"State for data persistance\",\n",
      "\t\t\texplain: \"Gradio is capable of adding state to your interface.\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Username and passwords authentication\",\n",
      "\t\t\texplain: \"Pass in a list of username/password tuples to the launch method to add authentication.\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Automatic analytics for who uses your gradio demo\",\n",
      "\t\t\texplain: \"Try again - Gradio does not supply developers analytics on who uses their demos.\"\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Loading a model from Hugging Face's model hub or Hugging Face Spaces\",\n",
      "\t\t\texplain: \"Absolutely - load any Hugging Face model using the <code>gr.Interface.load()</code> method\",\n",
      "\t\t\tcorrect: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "We will explore all of these concepts below.\n",
      "\n",
      "### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\n",
      "\n",
      "After you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook.\n",
      "\n",
      "```py\n",
      "import gradio as gr\n",
      "\n",
      "\n",
      "def flip_text(x):\n",
      "    return x[::-1]\n",
      "\n",
      "\n",
      "demo = gr.Blocks()\n",
      "\n",
      "with demo:\n",
      "    gr.Markdown(\n",
      "        \"\"\"\n",
      "    # Flip Text!\n",
      "    Start typing below to see the output.\n",
      "    \"\"\"\n",
      "    )\n",
      "    input = gr.Textbox(placeholder=\"Flip this text\")\n",
      "    output = gr.Textbox()\n",
      "\n",
      "    input.change(fn=flip_text, inputs=input, outputs=output)\n",
      "\n",
      "demo.launch()\n",
      "```\n",
      "\n",
      "<iframe src=\"https://course-demos-flip-text.hf.space\" frameBorder=\"0\" height=\"400\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n",
      "\n",
      "This simple example above introduces 4 concepts that underlie Blocks:\n",
      "\n",
      "1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\n",
      "<Tip>\n",
      "🙋If you're not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that 🤗\n",
      "</Tip>\n",
      "The order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)\n",
      "\n",
      "Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\n",
      "\n",
      "<CourseFloatingBanner chapter={9}\n",
      "  classNames=\"absolute z-10 right-0 top-0\"\n",
      "  notebooks={[\n",
      "    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n",
      "    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\n",
      "]} />\n",
      "\n",
      "In the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\n",
      "\n",
      "Now, what's the difference between `Interface` and `Blocks`?\n",
      "\n",
      "- ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\n",
      "\n",
      "- 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\").\n",
      "\n",
      "\n",
      "### Why Blocks 🧱?[[why-blocks-]]\n",
      "\n",
      "As we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\n",
      "\n",
      "- Group together related demos as multiple tabs in one web application\n",
      "- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\n",
      "- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n",
      "- Change a component's properties (for example, the choices in a dropdown) or its visibility based on user input\n",
      "\n",
      "We will explore all of these concepts below.\n",
      "\n",
      "### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\n",
      "\n",
      "```py\n",
      "%%blocks\n",
      "\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    gr.Markdown(f\"# Greetings {args.name}!\")\n",
      "    inp = gr.Textbox()\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n",
      "```\n",
      "\n",
      "Notice that:\n",
      "\n",
      "- You do not need to launch your demo — Gradio does that for you automatically!\n",
      "\n",
      "- Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server. This means you'll see your changes _much, much faster_ than if you were rerunning the cell normally.\n",
      "\n",
      "Here's what it looks like in a jupyter notebook:\n",
      "\n",
      "![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter_reload.gif)\n",
      "\n",
      "🪄 This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code!\n",
      "\n",
      "The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio.\n",
      "\n",
      "---\n",
      "\n",
      "## Next Steps\n",
      "\n",
      "Now that you know how to develop quickly using Gradio, start building your own!\n",
      "\n",
      "If you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) 🤗\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided context, you can use Gradio to:\n",
      "\n",
      "1. Create a demo for your machine learning model.\n",
      "2. Share your machine learning model with others.\n",
      "3. Debug your model.\n",
      "\n",
      "You can launch a Gradio demo from:\n",
      "\n",
      "1. Standard python IDEs.\n",
      "2. Google Colab notebooks.\n",
      "3. Jupyter notebooks.\n",
      "\n",
      "Gradio is not designed primarily for NLP models. Instead, it works with pretty much any data type.\n",
      "\n",
      "Gradio supports the following features:\n",
      "\n",
      "1. Multiple inputs and outputs.\n",
      "2. State for data persistence.\n",
      "3. Username and password authentication.\n",
      "4. Loading a model from Hugging Face's model hub or Hugging Face Spaces.\n",
      "-----------------------------\n",
      "24\n",
      "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx\n",
      "What TensorFlow API function is used to load a saved tensor file?\n",
      "\n",
      "safetensors.tensorflow.load_file\n",
      "safetensors.tensorflow.load_file\n",
      "-----------------Context------------\n",
      "['huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx', 'huggingface/course/blob/main/chapters/en/chapter2/4.mdx', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md']\n",
      "Tensorflow API\n",
      "\n",
      "[[autodoc]] safetensors.tensorflow.load_file\n",
      "[[autodoc]] safetensors.tensorflow.load\n",
      "[[autodoc]] safetensors.tensorflow.save_file\n",
      "[[autodoc]] safetensors.tensorflow.save\n",
      "\n",
      "You should now have sufficient knowledge of how tokenizers work to get started with the API.\n",
      "\n",
      "## Loading and saving[[loading-and-saving]]\n",
      "\n",
      "Loading and saving tokenizers is as simple as it is with models. Actually, it's based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the *architecture* of the model) as well as its vocabulary (a bit like the *weights* of the model).\n",
      "\n",
      "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the `BertTokenizer` class:\n",
      "\n",
      "```py\n",
      "from transformers import BertTokenizer\n",
      "\n",
      "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
      "```\n",
      "\n",
      "{#if fw === 'pt'}\n",
      "Similar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:\n",
      "\n",
      "{:else}\n",
      "Similar to `TFAutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:\n",
      "\n",
      "{/if}\n",
      "\n",
      "```py\n",
      "from transformers import AutoTokenizer\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      "```\n",
      "\n",
      "We can now use the tokenizer as shown in the previous section:\n",
      "\n",
      "```python\n",
      "tokenizer(\"Using a Transformer network is simple\")\n",
      "```\n",
      "\n",
      "```python out\n",
      "{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "```\n",
      "\n",
      "Saving a tokenizer is identical to saving a model:\n",
      "\n",
      "```py\n",
      "tokenizer.save_pretrained(\"directory_on_my_computer\")\n",
      "```\n",
      "\n",
      "We'll talk more about `token_type_ids` in [Chapter 3](/course/chapter3), and we'll explain the `attention_mask` key a little later. First, let's see how the `input_ids` are generated. To do this, we'll need to look at the intermediate methods of the tokenizer.\n",
      "\n",
      "## Encoding[[encoding]]\n",
      "\n",
      "<Youtube id=\"Yffk5aydLzg\"/>\n",
      "\n",
      "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n",
      "\n",
      "```py\n",
      ">>> from transformers.modeling_utils import load_sharded_checkpoint\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     load_sharded_checkpoint(model, tmp_dir)\n",
      "```\n",
      "\n",
      "## Low memory loading\n",
      "\n",
      "Sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n",
      "\n",
      "Please read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
      "\n",
      "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n",
      "\n",
      "```py\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     new_model = AutoModel.from_pretrained(tmp_dir)\n",
      "```\n",
      "\n",
      "The main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n",
      "\n",
      "Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n",
      "\n",
      "```py\n",
      ">>> import json\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n",
      "...         index = json.load(f)\n",
      "\n",
      ">>> print(index.keys())\n",
      "dict_keys(['metadata', 'weight_map'])\n",
      "```\n",
      "\n",
      "The metadata just consists of the total size of the model for now. We plan to add other information in the future:\n",
      "\n",
      "```py\n",
      ">>> index[\"metadata\"]\n",
      "{'total_size': 433245184}\n",
      "```\n",
      "\n",
      "The weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n",
      "\n",
      "```py\n",
      ">>> index[\"weight_map\"]\n",
      "{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n",
      " 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n",
      " ...\n",
      "```\n",
      "\n",
      "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n",
      "\n",
      "</Tip>\n",
      "\n",
      "In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n",
      "\n",
      "## Sharded checkpoints\n",
      "\n",
      "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n",
      "\n",
      "You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n",
      "\n",
      "```py\n",
      "from transformers import AutoModel\n",
      "\n",
      "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
      "```\n",
      "\n",
      "If you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n",
      "\n",
      "```py\n",
      ">>> import os\n",
      ">>> import tempfile\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir)\n",
      "...     print(sorted(os.listdir(tmp_dir)))\n",
      "['config.json', 'pytorch_model.bin']\n",
      "```\n",
      "\n",
      "Now let's use a maximum shard size of 200MB:\n",
      "\n",
      "```py\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     print(sorted(os.listdir(tmp_dir)))\n",
      "['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n",
      "```\n",
      "\n",
      "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n",
      "-----------------LLm ANSWER------------\n",
      "Based on the provided context, the TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.\n",
      "-----------------------------\n",
      "25\n",
      "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx\n",
      "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n",
      "\n",
      "In the \"Logs\" tab of your Endpoint through the UI.\n",
      "In the \"Logs\" tab of your Endpoint through the UI.\n",
      "-----------------Context------------\n",
      "['huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx', 'huggingface/blog/blob/main/pricing-update.md', 'huggingface/blog/blob/main/unity-api.md', 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx', 'huggingface/datasets-server/blob/main/services/api/README.md']\n",
      "Access and read Logs\n",
      "\n",
      "Hugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your Endpoint. \n",
      "\n",
      "You will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n",
      "\n",
      "The Container Logs are only available when your Endpoint is in the “Running” state. \n",
      "\n",
      "_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n",
      "\n",
      "**Build Logs:**\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\n",
      "\n",
      "**Container Logs:**\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />\n",
      "\n",
      "--\n",
      "title: Introducing our new pricing\n",
      "thumbnail: /blog/assets/114_pricing-update/thumbnail.png\n",
      "authors:\n",
      "- user: sbrandeis\n",
      "- user: pierric\n",
      "---\n",
      "\n",
      "# Introducing our new pricing\n",
      "\n",
      "\n",
      "As you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n",
      "\n",
      "First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\n",
      "\n",
      "Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).\n",
      "\n",
      "Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records.\n",
      "\n",
      "**TL;DR**: **At HF we monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system.\n",
      "\n",
      "If you have any questions, feel free to reach out. We welcome your feedback 🔥\n",
      "\n",
      "Use the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n",
      "\n",
      "To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n",
      "\n",
      "## Usage Tips\n",
      "\n",
      "1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n",
      "2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!\n",
      "\n",
      "Add custom Dependencies\n",
      "\n",
      "Inference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n",
      "\n",
      "* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n",
      "* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n",
      "\n",
      "To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n",
      "\n",
      "```bash\n",
      "optimum[onnxruntime]==1.2.3\n",
      "mkl-include\n",
      "mkl\n",
      "```\n",
      "\n",
      "Check out the `requirements.txt` files in the following model repositories for examples:\n",
      "\n",
      "* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n",
      "* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\n",
      "\n",
      "For more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference.\n",
      "\n",
      "Datasets server API\n",
      "\n",
      "> API on 🤗 datasets\n",
      "\n",
      "## Configuration\n",
      "\n",
      "The service can be configured using environment variables. They are grouped by scope.\n",
      "\n",
      "### API service\n",
      "\n",
      "See [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n",
      "\n",
      "### Common\n",
      "\n",
      "See [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n",
      "\n",
      "## Endpoints\n",
      "\n",
      "See https://huggingface.co/docs/datasets-server\n",
      "\n",
      "- /healthcheck: Ensure the app is running\n",
      "- /metrics: Return a list of metrics in the Prometheus format\n",
      "- /webhook: Add, update or remove a dataset\n",
      "- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n",
      "- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n",
      "- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n",
      "- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided context, you can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the \"Logs\" tab of your Endpoint.\n",
      "-----------------------------\n",
      "26\n",
      "huggingface/blog/blob/main/autotrain-image-classification.md\n",
      "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n",
      "\n",
      "Image Classification\n",
      "Image Classification\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/autotrain-image-classification.md', 'huggingface/blog/blob/main/autotrain-image-classification.md', 'huggingface/blog/blob/main/autotrain-image-classification.md', 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md']\n",
      "--\n",
      "title: Image Classification with AutoTrain \n",
      "thumbnail: /blog/assets/105_autotrain-image-classification/thumbnail.png\n",
      "authors:\n",
      "- user: nimaboscarino\n",
      "---\n",
      "\n",
      "# Image Classification with AutoTrain\n",
      "\n",
      "\n",
      "<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n",
      "\n",
      "So you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\n",
      "\n",
      "[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we’ve added, with more on the way. But what does this mean for you?\n",
      "\n",
      "[Image Classification](https://huggingface.co/tasks/image-classification) models learn to *categorize* images, meaning that you can train one of these models to label any image. Do you want a model that can recognize signatures? Distinguish bird species? Identify plant diseases? As long as you can find an appropriate dataset, an image classification model has you covered.\n",
      "\n",
      "## How can you train your own image classifier?\n",
      "\n",
      "If you haven’t [created a Hugging Face account](https://huggingface.co/join) yet, now’s the time! Following that, make your way over to the [AutoTrain homepage](https://huggingface.co/autotrain) and click on “Create new project” to get started. You’ll be asked to fill in some basic info about your project. In the screenshot below you’ll see that I created a project named `butterflies-classification`, and I chose the “Image Classification” task. I’ve also chosen the “Automatic” model option, since I want to let AutoTrain do the work of finding the best model architectures for my project.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "  <figure class=\"image table text-center m-0 w-1/2\">\n",
      "    <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The 'New Project' form for AutoTrain, filled out for a new Image Classification project named 'butterflies-classification'.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/new-project.png\"></medium-zoom>\n",
      "  </figure>\n",
      "</div>\n",
      "\n",
      "Once AutoTrain creates your project, you just need to connect your data. If you have the data locally, you can drag and drop the folder into the window. Since we can also use [any of the image classification datasets on the Hugging Face Hub](https://huggingface.co/datasets?task_categories=task_categories:image-classification), in this example I’ve decided to use the [NimaBoscarino/butterflies](https://huggingface.co/datasets/NimaBoscarino/butterflies) dataset. You can select separate training and validation datasets if available, or you can ask AutoTrain to split the data for you.\n",
      "\n",
      "<div class=\"grid grid-cols-2 gap-4\">\n",
      "  <figure class=\"image table text-center m-0 w-full\">\n",
      "  </figure>\n",
      "\n",
      "  <figure class=\"image table text-center m-0 w-full\">\n",
      "    <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A form showing configurations to select for the imported dataset, including split types and data columns.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/add-dataset.png\"></medium-zoom>\n",
      "  </figure>\n",
      "</div>\n",
      "\n",
      "Once the data has been added, simply choose the number of model candidates that you’d like AutoModel to try out, review the expected training cost (training with 5 candidate models and less than 500 images is free 🤩), and start training!\n",
      "\n",
      "Image Classification with Vision Transformers\n",
      "\n",
      "Related spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\n",
      "Tags: VISION, TRANSFORMERS, HUB\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n",
      "\n",
      "State-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n",
      "\n",
      "Let's get started!\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "Make sure you have the `gradio` Python package already [installed](/getting_started).\n",
      "\n",
      "## Step 1 — Choosing a Vision Image Classification Model\n",
      "\n",
      "First, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n",
      "\n",
      "Expand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n",
      "\n",
      "At the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n",
      "\n",
      "## Step 2 — Loading the Vision Transformer Model with Gradio\n",
      "\n",
      "Ready to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio_classification) to learn how to finetune Wav2Vec2 and use it for inference!\n",
      "\n",
      "### Automatic speech recognition\n",
      "\n",
      "To use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription.\n",
      "\n",
      "Ready to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](tasks/asr) to learn how to finetune Wav2Vec2 and use it for inference!\n",
      "\n",
      "## Computer vision\n",
      "\n",
      "There are two ways to approach computer vision tasks:\n",
      "\n",
      "1. Split an image into a sequence of patches and process them in parallel with a Transformer.\n",
      "2. Use a modern CNN, like [ConvNeXT](model_doc/convnext), which relies on convolutional layers but adopts modern network designs.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "A third approach mixes Transformers with convolutions (for example, [Convolutional Vision Transformer](model_doc/cvt) or [LeViT](model_doc/levit)). We won't discuss those because they just combine the two approaches we examine here.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we'll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks.\n",
      "\n",
      "### Image classification\n",
      "\n",
      "ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.\n",
      "\n",
      "#### Transformer\n",
      "-----------------LLm ANSWER------------\n",
      "According to the context, the latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.\n",
      "-----------------------------\n",
      "27\n",
      "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md\n",
      "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n",
      "\n",
      "model\n",
      "model\n",
      "-----------------Context------------\n",
      "['huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md']\n",
      "The first step is to know how to create and delete repositories. You can only manage repositories that you own (under\n",
      "your username namespace) or from organizations in which you have write permissions.\n",
      "\n",
      "### Create a repository\n",
      "\n",
      "Create an empty repository with [`create_repo`] and give it a name with the `repo_id` parameter. The `repo_id` is your namespace followed by the repository name: `username_or_org/repo_name`.\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import create_repo\n",
      ">>> create_repo(\"lysandre/test-model\")\n",
      "'https://huggingface.co/lysandre/test-model'\n",
      "```\n",
      "\n",
      "By default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import create_repo\n",
      ">>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\n",
      "'https://huggingface.co/datasets/lysandre/test-dataset'\n",
      "```\n",
      "\n",
      "When you create a repository, you can set your repository visibility with the `private` parameter.\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import create_repo\n",
      ">>> create_repo(\"lysandre/test-private\", private=True)\n",
      "```\n",
      "\n",
      "If you want to change the repository visibility at a later time, you can use the [`update_repo_visibility`] function.\n",
      "\n",
      "### Delete a repository\n",
      "\n",
      "Delete a repository with [`delete_repo`]. Make sure you want to delete a repository because this is an irreversible process!\n",
      "\n",
      "Specify the `repo_id` of the repository you want to delete:\n",
      "\n",
      "```py\n",
      ">>> delete_repo(repo_id=\"lysandre/my-corrupted-dataset\", repo_type=\"dataset\")\n",
      "```\n",
      "\n",
      "### Duplicate a repository (only for Spaces)\n",
      "\n",
      "In some cases, you want to copy someone else's repo to adapt it to your use case.\n",
      "This is possible for Spaces using the [`duplicate_space`] method. It will duplicate the whole repository.\n",
      "You will still need to configure your own settings (hardware, sleep-time, storage, variables and secrets). Check out our [Manage your Space](./manage-spaces) guide for more details.\n",
      "\n",
      "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Create and manage a repository\n",
      "\n",
      "The Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\n",
      "development to easily version projects when working collaboratively. This guide will show you how to interact with the\n",
      "repositories on the Hub, especially:\n",
      "\n",
      "- Create and delete a repository.\n",
      "- Manage branches and tags. \n",
      "- Rename your repository.\n",
      "- Update your repository visibility.\n",
      "- Manage a local copy of your repository.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\n",
      "might be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n",
      "(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\n",
      "not share the same requirements and workflows. Model repositories might maintain large model weight files for different\n",
      "frameworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\n",
      "a result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\n",
      "explanation page for more details.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n",
      "[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\n",
      "\n",
      "## Repo creation and deletion\n",
      "\n",
      "The first step is to know how to create and delete repositories. You can only manage repositories that you own (under\n",
      "your username namespace) or from organizations in which you have write permissions.\n",
      "\n",
      "### Create a repository\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import duplicate_space\n",
      ">>> duplicate_space(\"multimodalart/dreambooth-training\", private=False)\n",
      "RepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n",
      "```\n",
      "\n",
      "## Upload and download files\n",
      "\n",
      "Now that you have created your repository, you are interested in pushing changes to it and downloading files from it.\n",
      "\n",
      "These 2 topics deserve their own guides. Please refer to the [upload](./upload) and the [download](./download) guides\n",
      "to learn how to use your repository.\n",
      "\n",
      "\n",
      "## Branches and tags\n",
      "\n",
      "Git repositories often make use of branches to store different versions of a same repository.\n",
      "Tags can also be used to flag a specific state of your repository, for example, when releasing a version.\n",
      "More generally, branches and tags are referred as [git references](https://git-scm.com/book/en/v2/Git-Internals-Git-References).\n",
      "\n",
      "### Create branches and tags\n",
      "\n",
      "You can create new branch and tags using [`create_branch`] and [`create_tag`]:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import create_branch, create_tag\n",
      "\n",
      "# Create a branch on a Space repo from `main` branch\n",
      ">>> create_branch(\"Matthijs/speecht5-tts-demo\", repo_type=\"space\", branch=\"handle-dog-speaker\")\n",
      "\n",
      "# Create a tag on a Dataset repo from `v0.1-release` branch\n",
      ">>> create_branch(\"bigcode/the-stack\", repo_type=\"dataset\", revision=\"v0.1-release\", tag=\"v0.1.1\", tag_message=\"Bump release version.\")\n",
      "```\n",
      "\n",
      "You can use the [`delete_branch`] and [`delete_tag`] functions in the same way to delete a branch or a tag.\n",
      "\n",
      "### List all branches and tags\n",
      "\n",
      "You can also list the existing git refs from a repository using [`list_repo_refs`]:\n",
      "\n",
      "### Clone\n",
      "\n",
      "The `clone_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local_dir` argument:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import Repository\n",
      ">>> repo = Repository(local_dir=\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n",
      "```\n",
      "\n",
      "`clone_from` can also clone a repository using a URL:\n",
      "\n",
      "```py\n",
      ">>> repo = Repository(local_dir=\"huggingface-hub\", clone_from=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60\")\n",
      "```\n",
      "\n",
      "You can combine the `clone_from` parameter with [`create_repo`] to create and clone a repository:\n",
      "\n",
      "```py\n",
      ">>> repo_url = create_repo(repo_id=\"repo_name\")\n",
      ">>> repo = Repository(local_dir=\"repo_local_path\", clone_from=repo_url)\n",
      "```\n",
      "\n",
      "You can also configure a Git username and email to a cloned repository by specifying the `git_user` and `git_email` parameters when you clone a repository. When users commit to that repository, Git will be aware of the commit author.\n",
      "\n",
      "```py\n",
      ">>> repo = Repository(\n",
      "...   \"my-dataset\",\n",
      "...   clone_from=\"<user>/<dataset_id>\",\n",
      "...   token=True,\n",
      "...   repo_type=\"dataset\",\n",
      "...   git_user=\"MyName\",\n",
      "...   git_email=\"me@cool.mail\"\n",
      "... )\n",
      "```\n",
      "\n",
      "### Branch\n",
      "\n",
      "Branches are important for collaboration and experimentation without impacting your current files and code. Switch between branches with [`~Repository.git_checkout`]. For example, if you want to switch from `branch1` to `branch2`:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import Repository\n",
      ">>> repo = Repository(local_dir=\"huggingface-hub\", clone_from=\"<user>/<dataset_id>\", revision='branch1')\n",
      ">>> repo.git_checkout(\"branch2\")\n",
      "```\n",
      "\n",
      "### Pull\n",
      "\n",
      "[`~Repository.git_pull`] allows you to update a current local branch with changes from a remote repository:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import Repository\n",
      ">>> repo.git_pull()\n",
      "```\n",
      "\n",
      "Set `rebase=True` if you want your local commits to occur after your branch is updated with the new commits from the remote:\n",
      "\n",
      "```py\n",
      ">>> repo.git_pull(rebase=True)\n",
      "```\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import update_repo_visibility\n",
      ">>> update_repo_visibility(repo_id=repo_id, private=True)\n",
      "```\n",
      "\n",
      "### Rename your repository\n",
      "\n",
      "You can rename your repository on the Hub using [`move_repo`]. Using this method, you can also move the repo from a user to\n",
      "an organization. When doing so, there are a [few limitations](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)\n",
      "that you should be aware of. For example, you can't transfer your repo to another user.\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import move_repo\n",
      ">>> move_repo(from_id=\"Wauplin/cool-model\", to_id=\"huggingface/cool-model\")\n",
      "```\n",
      "\n",
      "## Manage a local copy of your repository\n",
      "\n",
      "All the actions described above can be done using HTTP requests. However, in some cases you might be interested in having\n",
      "a local copy of your repository and interact with it using the Git commands you are familiar with.\n",
      "\n",
      "The [`Repository`] class allows you to interact with files and repositories on the Hub with functions similar to Git commands. It is a wrapper over Git and Git-LFS methods to use the Git commands you already know and love. Before starting, please make sure you have Git-LFS installed (see [here](https://git-lfs.github.com/) for installation instructions).\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "[`Repository`] is deprecated in favor of the http-based alternatives implemented in [`HfApi`]. Given its large adoption in legacy code, the complete removal of [`Repository`] will only happen in release `v1.0`. For more details, please read [this explanation page](./concepts/git_vs_http).\n",
      "\n",
      "</Tip>\n",
      "\n",
      "### Use a local repository\n",
      "\n",
      "Instantiate a [`Repository`] object with a path to a local repository:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import Repository\n",
      ">>> repo = Repository(local_dir=\"<path>/<to>/<folder>\")\n",
      "```\n",
      "\n",
      "### Clone\n",
      "\n",
      "The `clone_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local_dir` argument:\n",
      "-----------------LLm ANSWER------------\n",
      "The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository.\n",
      "-----------------------------\n",
      "28\n",
      "huggingface/datasets-server/blob/main/docs/source/splits.mdx\n",
      "How many splits does the \"duorc\" dataset have?\n",
      "\n",
      "Six\n",
      "Six\n",
      "-----------------Context------------\n",
      "['huggingface/datasets-server/blob/main/docs/source/splits.mdx', 'huggingface/datasets-server/blob/main/docs/source/splits.mdx', 'huggingface/blog/blob/main/spacy.md', 'huggingface/blog/blob/main/spacy.md', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx']\n",
      "The `/splits` endpoint accepts the dataset name as its query parameter:\n",
      "\n",
      "<inferencesnippet>\n",
      "<python>\n",
      "```python\n",
      "import requests\n",
      "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
      "API_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\n",
      "def query():\n",
      "    response = requests.get(API_URL, headers=headers)\n",
      "    return response.json()\n",
      "data = query()\n",
      "```\n",
      "</python>\n",
      "<js>\n",
      "```js\n",
      "import fetch from \"node-fetch\";\n",
      "async function query(data) {\n",
      "    const response = await fetch(\n",
      "        \"https://datasets-server.huggingface.co/splits?dataset=duorc\",\n",
      "        {\n",
      "            headers: { Authorization: `Bearer ${API_TOKEN}` },\n",
      "            method: \"GET\"\n",
      "        }\n",
      "    );\n",
      "    const result = await response.json();\n",
      "    return result;\n",
      "}\n",
      "query().then((response) => {\n",
      "    console.log(JSON.stringify(response));\n",
      "});\n",
      "```\n",
      "</js>\n",
      "<curl>\n",
      "```curl\n",
      "curl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n",
      "        -X GET \\\n",
      "        -H \"Authorization: Bearer ${API_TOKEN}\"\n",
      "```\n",
      "</curl>\n",
      "</inferencesnippet>\n",
      "\n",
      "The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"splits\": [\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\n",
      "  ],\n",
      "  \"pending\": [],\n",
      "  \"failed\": []\n",
      "}\n",
      "```\n",
      "\n",
      "List splits and configurations\n",
      "\n",
      "Datasets typically have splits and may also have configurations. A _split_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A _configuration_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load_hub)!\n",
      "\n",
      "This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\n",
      "\n",
      "The `/splits` endpoint accepts the dataset name as its query parameter:\n",
      "\n",
      "<div class=\"SVELTE_HYDRATER \" data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingface.co&quot;,&quot;model&quot;:{&quot;author&quot;:&quot;spacy&quot;,&quot;autoArchitecture&quot;:&quot;AutoModel&quot;,&quot;branch&quot;:&quot;main&quot;,&quot;cardData&quot;:{&quot;tags&quot;:[&quot;spacy&quot;,&quot;token-classification&quot;],&quot;language&quot;:[&quot;en&quot;],&quot;license&quot;:&quot;MIT&quot;,&quot;model-index&quot;:[{&quot;name&quot;:&quot;en_core_web_sm&quot;,&quot;results&quot;:[{&quot;tasks&quot;:{&quot;name&quot;:&quot;NER&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Precision&quot;,&quot;type&quot;:&quot;precision&quot;,&quot;value&quot;:0.8424355924},{&quot;name&quot;:&quot;Recall&quot;,&quot;type&quot;:&quot;recall&quot;,&quot;value&quot;:0.8335336538},{&quot;name&quot;:&quot;F Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.8379609817}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;POS&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9720712187}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;SENTER&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Precision&quot;,&quot;type&quot;:&quot;precision&quot;,&quot;value&quot;:0.9074955788},{&quot;name&quot;:&quot;Recall&quot;,&quot;type&quot;:&quot;recall&quot;,&quot;value&quot;:0.8801372122},{&quot;name&quot;:&quot;F\n",
      "\n",
      "Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.893607046}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;UNLABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9185392711}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;LABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9185392711}]}}]}]},&quot;cardSource&quot;:true,&quot;id&quot;:&quot;spacy/en_core_web_sm&quot;,&quot;pipeline_tag&quot;:&quot;token-classification&quot;,&quot;library_name&quot;:&quot;spacy&quot;,&quot;modelId&quot;:&quot;spacy/en_core_web_sm&quot;,&quot;private&quot;:false,&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattributes&quot;},{&quot;rfilename&quot;:&quot;LICENSE&quot;},{&quot;rfilename&quot;:&quot;LICENSES_SOURCES&quot;},{&quot;rfilename&quot;:&quot;README.md&quot;},{&quot;rfilename&quot;:&quot;accuracy.json&quot;},{&quot;rfilename&quot;:&quot;config.cfg&quot;},{&quot;rfilename&quot;:&quot;en_core_web_sm-any-py3-none-any.whl&quot;},{&quot;rfilename&quot;:&quot;meta.json&quot;},{&quot;rfilename&quot;:&quot;tokenizer&quot;},{&quot;rfilename&quot;:&quot;attribute_ruler/patterns&quot;},{&quot;rfilename&quot;:&quot;lemmatizer/lookups/lookups.bin&quot;},{&quot;rfilename&quot;:&quot;ner/cfg&quot;},{&quot;rfilename&quot;:&quot;ner/model&quot;},{&quot;rfilename&quot;:&quot;ner/moves&quot;},{&quot;rfilename&quot;:&quot;vocab/lookups.bin&quot;},{&quot;rfilename&quot;:&quot;vocab/strings.json&quot;},{&quot;rfilename&quot;:&quot;vocab/vectors&quot;}],&quot;tags&quot;:[&quot;en&quot;,&quot;spacy&quot;,&quot;token-classification&quot;,&quot;license:mit&quot;,&quot;model-index&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;token-classification&quot;,&quot;label&quot;:&quot;Token\n",
      "\n",
      "<tokenizerslangcontent>\n",
      "<python>\n",
      "| Name | Description | Example |\n",
      "| :--- | :--- | :--- |\n",
      "| ByteLevel | Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties: <ul> <li>Since it maps on bytes, a tokenizer using this only requires **256** characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.</li> <li>A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! 🎉🎉)</li> <li>For non ascii characters, it gets completely unreadable, but it works nonetheless!</li> </ul> | Input: `\"Hello my friend, how are you?\"` <br> Ouput: `\"Hello\", \"Ġmy\", Ġfriend\", \",\", \"Ġhow\", \"Ġare\", \"Ġyou\", \"?\"`  |\n",
      "| Whitespace | Splits on word boundaries (using the following regular expression: `\\w+&#124;[^\\w\\s]+` | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there\", \"!\"`  |\n",
      "| WhitespaceSplit | Splits on any whitespace character | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there!\"`  |\n",
      "| Punctuation | Will isolate all punctuation characters | Input: `\"Hello?\"` <br> Ouput: `\"Hello\", \"?\"`  |\n",
      "| Metaspace | Splits on whitespaces and replaces them with a special char “▁” (U+2581) | Input: `\"Hello there\"` <br> Ouput: `\"Hello\", \"▁there\"`  |\n",
      "| CharDelimiterSplit | Splits on a given character | Example with `x`: <br> Input: `\"Helloxthere\"` <br> Ouput: `\"Hello\", \"there\"`  |\n",
      "| Digits | Splits the numbers from any other characters. | Input: `\"Hello123there\"` <br>  Output: ``\"Hello\", \"123\", \"there\"``  |\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided JSON response, the \"duorc\" dataset has 6 splits.\n",
      "\n",
      "Here is the JSON response:\n",
      "```json\n",
      "{\n",
      "  \"splits\": [\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\n",
      "    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\n",
      "  ],\n",
      "  \"pending\": [],\n",
      "  \"failed\": []\n",
      "}\n",
      "```\n",
      "As you can see, there are 6 elements in the \"splits\" array, each representing a split of the \"duorc\" dataset.\n",
      "-----------------------------\n",
      "29\n",
      "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md\n",
      "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n",
      "\n",
      "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n",
      "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n",
      "-----------------Context------------\n",
      "['huggingface/peft/blob/main/docs/source/accelerate/fsdp.md', 'huggingface/peft/blob/main/docs/source/accelerate/fsdp.md', 'huggingface/peft/blob/main/docs/source/accelerate/fsdp.md', 'huggingface/blog/blob/main/accelerate-library.md', 'huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md']\n",
      "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Fully Sharded Data Parallel\n",
      "\n",
      "[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "FSDP is supported in 🤗 Accelerate, and you can use it with 🤗 PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py). You'll configure the script to train a large model for conditional generation.\n",
      "\n",
      "## Configuration\n",
      "\n",
      "Begin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp) with 🤗 Accelerate. Use the `--config_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default_config.yaml` file in the 🤗 Accelerate cache.\n",
      "\n",
      "The configuration file is used to set the default options when you launch the training script.\n",
      "\n",
      "```bash\n",
      "accelerate config --config_file fsdp_config.yaml\n",
      "```\n",
      "\n",
      "The configuration file is used to set the default options when you launch the training script.\n",
      "\n",
      "```bash\n",
      "accelerate config --config_file fsdp_config.yaml\n",
      "```\n",
      "\n",
      "You'll be asked a few questions about your setup, and configure the following arguments. For this example, make sure you fully shard the model parameters, gradients, optimizer states, leverage the CPU for offloading, and wrap model layers based on the Transformer layer class name.\n",
      "\n",
      "```bash\n",
      "`Sharding Strategy`: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD\n",
      "`Offload Params`: Decides Whether to offload parameters and gradients to CPU\n",
      "`Auto Wrap Policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3] NO_WRAP \n",
      "`Transformer Layer Class to Wrap`: When using `TRANSFORMER_BASED_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g, \n",
      "`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`...\n",
      "`Min Num Params`: minimum number of parameters when using `SIZE_BASED_WRAP`\n",
      "`Backward Prefetch`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH\n",
      "`State Dict Type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT  \n",
      "```\n",
      "\n",
      "For example, your FSDP configuration file may look like the following:\n",
      "\n",
      "For example, your FSDP configuration file may look like the following:\n",
      "\n",
      "```yaml\n",
      "command_file: null\n",
      "commands: null\n",
      "compute_environment: LOCAL_MACHINE\n",
      "deepspeed_config: {}\n",
      "distributed_type: FSDP\n",
      "downcast_bf16: 'no'\n",
      "dynamo_backend: 'NO'\n",
      "fsdp_config:\n",
      "  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n",
      "  fsdp_backward_prefetch_policy: BACKWARD_PRE\n",
      "  fsdp_offload_params: true\n",
      "  fsdp_sharding_strategy: 1\n",
      "  fsdp_state_dict_type: FULL_STATE_DICT\n",
      "  fsdp_transformer_layer_cls_to_wrap: T5Block\n",
      "gpu_ids: null\n",
      "machine_rank: 0\n",
      "main_process_ip: null\n",
      "main_process_port: null\n",
      "main_training_function: main\n",
      "megatron_lm_config: {}\n",
      "mixed_precision: 'no'\n",
      "num_machines: 1\n",
      "num_processes: 2\n",
      "rdzv_backend: static\n",
      "same_network: true\n",
      "tpu_name: null\n",
      "tpu_zone: null\n",
      "use_cpu: false\n",
      "```\n",
      "\n",
      "## The important parts\n",
      "\n",
      "Let's dig a bit deeper into the training script to understand how it works.\n",
      "\n",
      "The [`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14) function begins with initializing an [`~accelerate.Accelerator`] class which handles everything for distributed training, such as automatically detecting your training environment.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "💡 Feel free to change the model and dataset inside the `main` function. If your dataset format is different from the one in the script, you may also need to write your own preprocessing function. \n",
      "\n",
      "</Tip>\n",
      "\n",
      "The script also creates a configuration corresponding to the 🤗 PEFT method you're using. For LoRA, you'll use [`LoraConfig`] to specify the task type, and several other important parameters such as the dimension of the low-rank matrices, the matrices scaling factor, and the dropout probability of the LoRA layers. If you want to use a different 🤗 PEFT method, replace `LoraConfig` with the appropriate [class](../package_reference/tuners).\n",
      "\n",
      "Next, the script wraps the base model and `peft_config` with the [`get_peft_model`] function to create a [`PeftModel`].\n",
      "\n",
      "model = torch.nn.Transformer().to(device)\n",
      "+ model = DistributedDataParallel(model)  \n",
      "  optim = torch.optim.Adam(model.parameters())\n",
      "\n",
      "  dataset = load_dataset('my_dataset')\n",
      "+ sampler = DistributedSampler(dataset)\n",
      "- data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
      "+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)\n",
      "\n",
      "  model.train()\n",
      "  for epoch in range(10):\n",
      "+     sampler.set_epoch(epoch)  \n",
      "      for source, targets in data:\n",
      "          source = source.to(device)\n",
      "          targets = targets.to(device)\n",
      "\n",
      "          optimizer.zero_grad()\n",
      "\n",
      "          output = model(source)\n",
      "          loss = F.cross_entropy(output, targets)\n",
      "\n",
      "          loss.backward()\n",
      "\n",
      "          optimizer.step()\n",
      "```\n",
      "\n",
      "These changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere). Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\n",
      "- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible.\n",
      "- to have the same functions work for any distributed setup, so only have to learn one API.\n",
      "\n",
      "### How does it work?\n",
      "\n",
      "To see how the library works in practice, let's have a look at each line of code we need to add to a training loop.\n",
      "\n",
      "```python\n",
      "accelerator = Accelerator()\n",
      "```\n",
      "\n",
      "On top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script.\n",
      "\n",
      "```python\n",
      "model, optim, data = accelerator.prepare(model, optim, data)\n",
      "```\n",
      "\n",
      "def cleanup():\n",
      "    \"Cleans up the distributed environment\"\n",
      "    dist.destroy_process_group()\n",
      "```\n",
      "\n",
      "The last piece of the puzzle is *how do I send my data and model to another GPU?*\n",
      "\n",
      "This is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step.\n",
      "\n",
      "Below is an example of our training setup, refactored as a function, with this capability:\n",
      "\n",
      "> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n",
      "\n",
      "```python\n",
      "from torch.nn.parallel import DistributedDataParallel as DDP\n",
      "\n",
      "def train(model, rank, world_size):\n",
      "    setup(rank, world_size)\n",
      "    model = model.to(rank)\n",
      "    ddp_model = DDP(model, device_ids=[rank])\n",
      "    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\n",
      "    # Train for one epoch\n",
      "    model.train()\n",
      "    for batch_idx, (data, target) in enumerate(train_loader):\n",
      "        data, target = data.to(device), target.to(device)\n",
      "        output = model(data)\n",
      "        loss = F.nll_loss(output, target)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        optimizer.zero_grad()\n",
      "    cleanup()\n",
      "```\n",
      "\n",
      "The optimizer needs to be declared based on the model *on the specific device* (so `ddp_model` and not `model`) for all of the gradients to properly be calculated.\n",
      "\n",
      "Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n",
      "\n",
      "```bash\n",
      "torchrun --nproc_per_node=2 --nnodes=1 example_script.py\n",
      "```\n",
      "\n",
      "The above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n",
      "-----------------LLm ANSWER------------\n",
      "The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to allow for the training of large models with a large number of parameters by sharding the model parameters, gradients, and optimizer states across multiple data parallel processes. This allows for more efficient use of memory and enables the training of larger models that would not fit in memory on a single machine.\n",
      "-----------------------------\n",
      "30\n",
      "huggingface/safetensors/blob/main/docs/source/convert-weights.md\n",
      "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n",
      "\n",
      "`.safetensors`\n",
      "`.safetensors`\n",
      "-----------------Context------------\n",
      "['huggingface/safetensors/blob/main/docs/source/convert-weights.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx', 'huggingface/transformers/blob/main/docs/source/en/big_models.md']\n",
      "Convert weights to safetensors\n",
      "\n",
      "PyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n",
      "\n",
      "The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n",
      "\n",
      "Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n",
      "\n",
      "```py\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     new_model = AutoModel.from_pretrained(tmp_dir)\n",
      "```\n",
      "\n",
      "The main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n",
      "\n",
      "Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n",
      "\n",
      "```py\n",
      ">>> import json\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n",
      "...         index = json.load(f)\n",
      "\n",
      ">>> print(index.keys())\n",
      "dict_keys(['metadata', 'weight_map'])\n",
      "```\n",
      "\n",
      "The metadata just consists of the total size of the model for now. We plan to add other information in the future:\n",
      "\n",
      "```py\n",
      ">>> index[\"metadata\"]\n",
      "{'total_size': 433245184}\n",
      "```\n",
      "\n",
      "The weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n",
      "\n",
      "```py\n",
      ">>> index[\"weight_map\"]\n",
      "{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n",
      " 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n",
      " ...\n",
      "```\n",
      "\n",
      "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n",
      "\n",
      "</Tip>\n",
      "\n",
      "In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n",
      "\n",
      "## Sharded checkpoints\n",
      "\n",
      "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n",
      "\n",
      "You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n",
      "\n",
      "```py\n",
      "from transformers import AutoModel\n",
      "\n",
      "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
      "```\n",
      "\n",
      "If you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n",
      "\n",
      "```py\n",
      ">>> import os\n",
      ">>> import tempfile\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir)\n",
      "...     print(sorted(os.listdir(tmp_dir)))\n",
      "['config.json', 'pytorch_model.bin']\n",
      "```\n",
      "\n",
      "Now let's use a maximum shard size of 200MB:\n",
      "\n",
      "```py\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     print(sorted(os.listdir(tmp_dir)))\n",
      "['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n",
      "```\n",
      "\n",
      "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n",
      "\n",
      "Tensorflow API\n",
      "\n",
      "[[autodoc]] safetensors.tensorflow.load_file\n",
      "[[autodoc]] safetensors.tensorflow.load\n",
      "[[autodoc]] safetensors.tensorflow.save_file\n",
      "[[autodoc]] safetensors.tensorflow.save\n",
      "\n",
      "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# Instantiating a big model\n",
      "\n",
      "When you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\n",
      "from PyTorch is:\n",
      "\n",
      "1. Create your model with random weights.\n",
      "2. Load your pretrained weights.\n",
      "3. Put those pretrained weights in your random model.\n",
      "\n",
      "Step 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Note that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n",
      "\n",
      "</Tip>\n",
      "-----------------LLm ANSWER------------\n",
      "According to the context, the file format used to save and store PyTorch model weights more securely than `.bin` files is `.safetensors`.\n",
      "-----------------------------\n",
      "31\n",
      "huggingface/hub-docs/blob/main/docs/hub/security.md\n",
      "What type of security certification does Hugging Face have?\n",
      "\n",
      "SOC2 Type 2 certified\n",
      "SOC2 Type 2 certified\n",
      "-----------------Context------------\n",
      "['huggingface/hub-docs/blob/main/docs/hub/security.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md', 'huggingface/blog/blob/main/intel.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md', 'huggingface/blog/blob/main/pricing-update.md']\n",
      "Security\n",
      "\n",
      "The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n",
      "\n",
      "Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \n",
      "\n",
      "Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n",
      "\n",
      "<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n",
      "\n",
      "For any other security questions, please feel free to send us an email at security@huggingface.co.\n",
      "\n",
      "## Contents\n",
      "\n",
      "- [User Access Tokens](./security-tokens)\n",
      "- [Git over SSH](./security-git-ssh)\n",
      "- [Signing commits with GPG](./security-gpg)\n",
      "- [Single Sign-On (SSO)](./security-sso)\n",
      "- [Malware Scanning](./security-malware)\n",
      "- [Pickle Scanning](./security-pickle)\n",
      "- [Secrets Scanning](./security-secrets)\n",
      "\n",
      "- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n",
      "\n",
      "- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n",
      "\n",
      "- **Encouraging safety in deployment**\n",
      "\n",
      "  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n",
      "\n",
      "  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n",
      "\n",
      "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository’s authors to have more control over its use.\n",
      "\n",
      "- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
      "\n",
      "--\n",
      "title: \"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\"\n",
      "thumbnail: /blog/assets/80_intel/01.png\n",
      "authors:\n",
      "- user: juliensimon\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "![image](assets/80_intel/01.png)\n",
      "\n",
      "The mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption.\n",
      "\n",
      "Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware).  Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers.\n",
      "\n",
      "Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.\n",
      "\n",
      "Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# 🧨 Diffusers’ Ethical Guidelines\n",
      "\n",
      "## Preamble\n",
      "\n",
      "[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n",
      "\n",
      "Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library.\n",
      "\n",
      "The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\n",
      "We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n",
      "\n",
      "\n",
      "## Scope\n",
      "\n",
      "The Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n",
      "\n",
      "\n",
      "## Ethical guidelines\n",
      "\n",
      "--\n",
      "title: Introducing our new pricing\n",
      "thumbnail: /blog/assets/114_pricing-update/thumbnail.png\n",
      "authors:\n",
      "- user: sbrandeis\n",
      "- user: pierric\n",
      "---\n",
      "\n",
      "# Introducing our new pricing\n",
      "\n",
      "\n",
      "As you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n",
      "\n",
      "First of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\n",
      "\n",
      "Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).\n",
      "\n",
      "Your billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records.\n",
      "\n",
      "**TL;DR**: **At HF we monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system.\n",
      "\n",
      "If you have any questions, feel free to reach out. We welcome your feedback 🔥\n",
      "-----------------LLm ANSWER------------\n",
      "Hugging Face is SOC2 Type 2 certified, which means they provide security certification to their customers and actively monitor and patch any security weaknesses.\n",
      "-----------------------------\n",
      "32\n",
      "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md\n",
      "What do RAG models combine to generate outputs?\n",
      "\n",
      "Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n",
      "Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md', 'huggingface/course/blob/main/chapters/en/chapter1/10.mdx', 'huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md']\n",
      "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# RAG\n",
      "\n",
      "<div class=\"flex flex-wrap space-x-1\">\n",
      "<a href=\"https://huggingface.co/models?filter=rag\">\n",
      "<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\">\n",
      "</a>\n",
      "</div>\n",
      "\n",
      "## Overview\n",
      "\n",
      "Retrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\n",
      "sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\n",
      "outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\n",
      "both retrieval and generation to adapt to downstream tasks.\n",
      "\n",
      "It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\n",
      "Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "This model was contributed by [ola13](https://huggingface.co/ola13).\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "Retrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \n",
      "RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \n",
      "modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \n",
      "to downstream tasks.\n",
      "\n",
      "## RagConfig\n",
      "\n",
      "[[autodoc]] RagConfig\n",
      "\n",
      "## RagTokenizer\n",
      "\n",
      "[[autodoc]] RagTokenizer\n",
      "\n",
      "## Rag specific outputs\n",
      "\n",
      "[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n",
      "\n",
      "[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n",
      "\n",
      "## RagRetriever\n",
      "\n",
      "[[autodoc]] RagRetriever\n",
      "\n",
      "<frameworkcontent>\n",
      "<pt>\n",
      "\n",
      "## RagModel\n",
      "\n",
      "[[autodoc]] RagModel\n",
      "    - forward\n",
      "\n",
      "## RagSequenceForGeneration\n",
      "\n",
      "[[autodoc]] RagSequenceForGeneration\n",
      "    - forward\n",
      "    - generate\n",
      "\n",
      "## RagTokenForGeneration\n",
      "\n",
      "[[autodoc]] RagTokenForGeneration\n",
      "    - forward\n",
      "    - generate\n",
      "\n",
      "</pt>\n",
      "<tf>\n",
      "\n",
      "## TFRagModel\n",
      "\n",
      "[[autodoc]] TFRagModel\n",
      "    - call\n",
      "\n",
      "## TFRagSequenceForGeneration\n",
      "\n",
      "[[autodoc]] TFRagSequenceForGeneration\n",
      "    - call\n",
      "    - generate\n",
      "\n",
      "## TFRagTokenForGeneration\n",
      "\n",
      "[[autodoc]] TFRagTokenForGeneration\n",
      "    - call\n",
      "    - generate\n",
      "\n",
      "</tf>\n",
      "</frameworkcontent>\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\n",
      "state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\n",
      "manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind\n",
      "task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\n",
      "remain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\n",
      "memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\n",
      "general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained\n",
      "parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\n",
      "pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\n",
      "pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\n",
      "across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\n",
      "models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\n",
      "outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\n",
      "tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\n",
      "parametric-only seq2seq baseline.*\n",
      "\n",
      "This model was contributed by [ola13](https://huggingface.co/ola13).\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "### 9. Which of those types of models would you use for summarizing texts?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"An encoder model\",\n",
      "\t\t\texplain: \"An encoder model generates a representation of the whole sentence that is better suited for tasks like classification.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"A decoder model\",\n",
      "\t\t\texplain: \"Decoder models are good for generating output text (like summaries), but they don't have the ability to exploit a context like the whole text to summarize.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"A sequence-to-sequence model\",\n",
      "\t\t\texplain: \"Sequence-to-sequence models are perfectly suited for a summarization task.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 10. Which of these types of models would you use for classifying text inputs according to certain labels?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"An encoder model\",\n",
      "\t\t\texplain: \"An encoder model generates a representation of the whole sentence which is perfectly suited for a task like classification.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"A decoder model\",\n",
      "\t\t\texplain: \"Decoder models are good for generating output texts, not extracting a label out of a sentence.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"A sequence-to-sequence model\",\n",
      "\t\t\texplain: \"Sequence-to-sequence models are better suited for tasks where you want to generate text based on an input sentence, not a label.\",\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 11. What possible source can the bias observed in a model have?\n",
      "\n",
      "He was wearing a long black coat and a top hat.\n",
      "\n",
      "The little girl ran\n",
      "```\n",
      "\n",
      "Looks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky Halloween story about a ghost.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "For longer outputs like this, you will greatly benefit from tweaking the text generation strategy. This can help \n",
      "you significantly improve the quality of the generated output. Check out [Text generation strategies](../generation_strategies) \n",
      "to learn more. \n",
      "</Tip>\n",
      "\n",
      "## Running inference in batch mode\n",
      "\n",
      "All of the earlier sections illustrated IDEFICS for a single example. In a very similar fashion, you can run inference \n",
      "for a batch of examples by passing a list of prompts:\n",
      "\n",
      "```py\n",
      ">>> prompts = [\n",
      "...     [   \"https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80\",\n",
      "...         \"This is an image of \",\n",
      "...     ],\n",
      "...     [   \"https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n",
      "...         \"This is an image of \",\n",
      "...     ],\n",
      "...     [   \"https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\n",
      "...         \"This is an image of \",\n",
      "...     ],\n",
      "... ]\n",
      "\n",
      ">>> inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\n",
      ">>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n",
      "\n",
      ">>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n",
      ">>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
      ">>> for i,t in enumerate(generated_text):\n",
      "...     print(f\"{i}:\\n{t}\\n\") \n",
      "0:\n",
      "This is an image of the Eiffel Tower in Paris, France.\n",
      "\n",
      "1:\n",
      "This is an image of a couple on a picnic blanket.\n",
      "\n",
      "2:\n",
      "This is an image of a vegetable stand.\n",
      "```\n",
      "-----------------LLm ANSWER------------\n",
      "RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence (Seq2Seq) models.\n",
      "-----------------------------\n",
      "33\n",
      "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md\n",
      "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n",
      "\n",
      "Beautiful Soup\n",
      "Beautiful Soup\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md']\n",
      "<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n",
      "\n",
      "## Usage: MarkupLMProcessor\n",
      "\n",
      "The easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n",
      "([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\n",
      "used to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\n",
      "token-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\n",
      "if you only want to handle one of the two tasks.\n",
      "\n",
      "```python\n",
      "from transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\n",
      "\n",
      "feature_extractor = MarkupLMFeatureExtractor()\n",
      "tokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\n",
      "processor = MarkupLMProcessor(feature_extractor, tokenizer)\n",
      "```\n",
      "\n",
      "In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\n",
      "and it will create the inputs expected by the model. Internally, the processor first uses\n",
      "[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\n",
      "xpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\n",
      "to token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\n",
      "Optionally, one can provide node labels to the processor, which are turned into token-level `labels`.\n",
      "\n",
      "[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\n",
      "pulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\n",
      "choice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\n",
      "\n",
      ">>> html_string = \"\"\"\n",
      "...  <!DOCTYPE html>\n",
      "...  <html>\n",
      "...  <head>\n",
      "...  <title>Hello world</title>\n",
      "...  </head>\n",
      "...  <body>\n",
      "...  <h1>Welcome</h1>\n",
      "...  <p>My name is Niels.</p>\n",
      "...  </body>\n",
      "...  </html>\"\"\"\n",
      "\n",
      ">>> question = \"What's his name?\"\n",
      ">>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\n",
      ">>> print(encoding.keys())\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n",
      "```\n",
      "\n",
      "**Use case 5: web page question answering (inference), parse_html=False**\n",
      "\n",
      "For question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\n",
      "all nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\n",
      "\n",
      "```python\n",
      ">>> from transformers import MarkupLMProcessor\n",
      "\n",
      ">>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n",
      ">>> processor.parse_html = False\n",
      "\n",
      ">>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n",
      ">>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n",
      ">>> question = \"What's his name?\"\n",
      ">>> encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\n",
      ">>> print(encoding.keys())\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n",
      "```\n",
      "\n",
      "## Resources\n",
      "\n",
      "- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\n",
      "- [Text classification task guide](../tasks/sequence_classification)\n",
      "- [Token classification task guide](../tasks/token_classification)\n",
      "- [Question answering task guide](../tasks/question_answering)\n",
      "\n",
      "## MarkupLMConfig\n",
      "\n",
      "[[autodoc]] MarkupLMConfig\n",
      "    - all\n",
      "\n",
      "## MarkupLMFeatureExtractor\n",
      "\n",
      "[[autodoc]] MarkupLMFeatureExtractor\n",
      "    - __call__\n",
      "\n",
      "## MarkupLMTokenizer\n",
      "\n",
      "[[autodoc]] MarkupLMTokenizer\n",
      "    - build_inputs_with_special_tokens\n",
      "    - get_special_tokens_mask\n",
      "    - create_token_type_ids_from_sequences\n",
      "    - save_vocabulary\n",
      "\n",
      "## MarkupLMTokenizerFast\n",
      "\n",
      "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# MarkupLM\n",
      "\n",
      "## Overview\n",
      "\n",
      "The MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\n",
      "Understanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\n",
      "applied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\n",
      "performance, similar to [LayoutLM](layoutlm).\n",
      "\n",
      "The model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\n",
      "state-of-the-art results on 2 important benchmarks:\n",
      "- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\n",
      "- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\n",
      "for information extraction from web pages (basically named-entity recogntion on web pages)\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "In total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\n",
      "use cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n",
      "\n",
      "**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\n",
      "\n",
      "This is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n",
      "\n",
      "```python\n",
      ">>> from transformers import MarkupLMProcessor\n",
      "\n",
      ">>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n",
      "\n",
      ">>> html_string = \"\"\"\n",
      "...  <!DOCTYPE html>\n",
      "...  <html>\n",
      "...  <head>\n",
      "...  <title>Hello world</title>\n",
      "...  </head>\n",
      "...  <body>\n",
      "...  <h1>Welcome</h1>\n",
      "...  <p>Here is my website.</p>\n",
      "...  </body>\n",
      "...  </html>\"\"\"\n",
      "\n",
      ">>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\n",
      ">>> encoding = processor(html_string, return_tensors=\"pt\")\n",
      ">>> print(encoding.keys())\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n",
      "```\n",
      "\n",
      "**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\n",
      "\n",
      "In case one already has obtained all nodes and xpaths, one doesn't need the feature extractor. In that case, one should\n",
      "provide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\n",
      "\n",
      "```python\n",
      ">>> from transformers import MarkupLMProcessor\n",
      "\n",
      ">>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n",
      ">>> processor.parse_html = False\n",
      "\n",
      ">>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\n",
      ">>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\n",
      ">>> encoding = processor(nodes=nodes, xpaths=xpaths, return_tensors=\"pt\")\n",
      ">>> print(encoding.keys())\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'xpath_tags_seq', 'xpath_subs_seq'])\n",
      "```\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\n",
      "Understanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\n",
      "large number of digital documents where the layout information is not fixed and needs to be interactively and\n",
      "dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\n",
      "paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\n",
      "HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\n",
      "pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\n",
      "tasks. The pre-trained model and code will be publicly available.*\n",
      "\n",
      "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\n",
      "These are the XPATH tags and subscripts respectively for each token in the input sequence.\n",
      "- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\n",
      "\n",
      "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\n",
      "alt=\"drawing\" width=\"600\"/> \n",
      "\n",
      "<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n",
      "\n",
      "## Usage: MarkupLMProcessor\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, under the hood.\n",
      "-----------------------------\n",
      "34\n",
      "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md\n",
      "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n",
      "\n",
      "10MB\n",
      "10MB\n",
      "-----------------Context------------\n",
      "['huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md', 'huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md']\n",
      "Managing Spaces with Github Actions\n",
      "\n",
      "You can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n",
      "\n",
      "First, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n",
      "\n",
      "```bash\n",
      "git remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n",
      "```\n",
      "\n",
      "Then force push to sync everything for the first time:\n",
      "\n",
      "```bash\n",
      "git push --force space main\n",
      "```\n",
      "\n",
      "Next, set up a GitHub Action to push your main branch to Spaces. In the example below:\n",
      "\n",
      "* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n",
      "* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.\n",
      "\n",
      "```yaml\n",
      "name: Sync to Hugging Face hub\n",
      "on:\n",
      "  push:\n",
      "    branches: [main]\n",
      "\n",
      "  # to run this workflow manually from the Actions tab\n",
      "  workflow_dispatch:\n",
      "\n",
      "jobs:\n",
      "  sync-to-hub:\n",
      "    runs-on: ubuntu-latest\n",
      "    steps:\n",
      "      - uses: actions/checkout@v3\n",
      "        with:\n",
      "          fetch-depth: 0\n",
      "          lfs: true\n",
      "      - name: Push to hub\n",
      "        env:\n",
      "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
      "        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n",
      "```\n",
      "\n",
      "Finally, create an Action that automatically checks the file size of any new pull request:\n",
      "\n",
      "\n",
      "```yaml\n",
      "name: Check file size\n",
      "on:               # or directly `on: [push]` to run the action on every push on any branch\n",
      "  pull_request:\n",
      "    branches: [main]\n",
      "\n",
      "```yaml\n",
      "name: Check file size\n",
      "on:               # or directly `on: [push]` to run the action on every push on any branch\n",
      "  pull_request:\n",
      "    branches: [main]\n",
      "\n",
      "  # to run this workflow manually from the Actions tab\n",
      "  workflow_dispatch:\n",
      "\n",
      "jobs:\n",
      "  sync-to-hub:\n",
      "    runs-on: ubuntu-latest\n",
      "    steps:\n",
      "      - name: Check large files\n",
      "        uses: ActionsDesk/lfs-warning@v2.0\n",
      "        with:\n",
      "          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n",
      "```\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import duplicate_space\n",
      ">>> duplicate_space(\"multimodalart/dreambooth-training\", private=False)\n",
      "RepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n",
      "```\n",
      "\n",
      "## Upload and download files\n",
      "\n",
      "Now that you have created your repository, you are interested in pushing changes to it and downloading files from it.\n",
      "\n",
      "These 2 topics deserve their own guides. Please refer to the [upload](./upload) and the [download](./download) guides\n",
      "to learn how to use your repository.\n",
      "\n",
      "\n",
      "## Branches and tags\n",
      "\n",
      "Git repositories often make use of branches to store different versions of a same repository.\n",
      "Tags can also be used to flag a specific state of your repository, for example, when releasing a version.\n",
      "More generally, branches and tags are referred as [git references](https://git-scm.com/book/en/v2/Git-Internals-Git-References).\n",
      "\n",
      "### Create branches and tags\n",
      "\n",
      "You can create new branch and tags using [`create_branch`] and [`create_tag`]:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import create_branch, create_tag\n",
      "\n",
      "# Create a branch on a Space repo from `main` branch\n",
      ">>> create_branch(\"Matthijs/speecht5-tts-demo\", repo_type=\"space\", branch=\"handle-dog-speaker\")\n",
      "\n",
      "# Create a tag on a Dataset repo from `v0.1-release` branch\n",
      ">>> create_branch(\"bigcode/the-stack\", repo_type=\"dataset\", revision=\"v0.1-release\", tag=\"v0.1.1\", tag_message=\"Bump release version.\")\n",
      "```\n",
      "\n",
      "You can use the [`delete_branch`] and [`delete_tag`] functions in the same way to delete a branch or a tag.\n",
      "\n",
      "### List all branches and tags\n",
      "\n",
      "You can also list the existing git refs from a repository using [`list_repo_refs`]:\n",
      "\n",
      "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Create and manage a repository\n",
      "\n",
      "The Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\n",
      "development to easily version projects when working collaboratively. This guide will show you how to interact with the\n",
      "repositories on the Hub, especially:\n",
      "\n",
      "- Create and delete a repository.\n",
      "- Manage branches and tags. \n",
      "- Rename your repository.\n",
      "- Update your repository visibility.\n",
      "- Manage a local copy of your repository.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\n",
      "might be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n",
      "(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\n",
      "not share the same requirements and workflows. Model repositories might maintain large model weight files for different\n",
      "frameworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\n",
      "a result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\n",
      "explanation page for more details.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n",
      "[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\n",
      "\n",
      "## Repo creation and deletion\n",
      "\n",
      "The first step is to know how to create and delete repositories. You can only manage repositories that you own (under\n",
      "your username namespace) or from organizations in which you have write permissions.\n",
      "\n",
      "### Create a repository\n",
      "\n",
      "You can use the [`delete_branch`] and [`delete_tag`] functions in the same way to delete a branch or a tag.\n",
      "\n",
      "### List all branches and tags\n",
      "\n",
      "You can also list the existing git refs from a repository using [`list_repo_refs`]:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import list_repo_refs\n",
      ">>> list_repo_refs(\"bigcode/the-stack\", repo_type=\"dataset\")\n",
      "GitRefs(\n",
      "   branches=[\n",
      "         GitRefInfo(name='main', ref='refs/heads/main', target_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),\n",
      "         GitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')\n",
      "   ],\n",
      "   converts=[],\n",
      "   tags=[\n",
      "         GitRefInfo(name='v1.0', ref='refs/tags/v1.0', target_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')\n",
      "   ]\n",
      ")\n",
      "``` \n",
      "\n",
      "## Change repository settings\n",
      "\n",
      "Repositories come with some settings that you can configure. Most of the time, you will want to do that manually in the\n",
      "repo settings page in your browser. You must have write access to a repo to configure it (either own it or being part of\n",
      "an organization). In this section, we will see the settings that you can also configure programmatically using `huggingface_hub`.\n",
      "\n",
      "Some settings are specific to Spaces (hardware, environment variables,...). To configure those, please refer to our [Manage your Spaces](../guides/manage-spaces) guide.\n",
      "\n",
      "### Update visibility\n",
      "\n",
      "A repository can be public or private. A private repository is only visible to you or members of the organization in which the repository is located. Change a repository to private as shown in the following:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import update_repo_visibility\n",
      ">>> update_repo_visibility(repo_id=repo_id, private=True)\n",
      "```\n",
      "\n",
      "### Rename your repository\n",
      "-----------------LLm ANSWER------------\n",
      "The file size limit for syncing to HF Spaces without using Git-LFS is 10MB.\n",
      "-----------------------------\n",
      "35\n",
      "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md\n",
      "What is the title of the paper introducing the ByT5 model?\n",
      "\n",
      "ByT5: Towards a token-free future with pre-trained byte-to-byte models\n",
      "ByT5: Towards a token-free future with pre-trained byte-to-byte models\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md']\n",
      "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# ByT5\n",
      "\n",
      "## Overview\n",
      "\n",
      "The ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\n",
      "Kale, Adam Roberts, Colin Raffel.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "\n",
      "*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.\n",
      "Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\n",
      "the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\n",
      "can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\n",
      "removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\n",
      "sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\n",
      "operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\n",
      "minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\n",
      "training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\n",
      "counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\n",
      "tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\n",
      "pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\n",
      "experiments.*\n",
      "\n",
      "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\n",
      "found [here](https://github.com/google-research/byt5).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "ByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\n",
      "only differ in how inputs should be prepared for the model, see the code examples below.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "<Tip>\n",
      "\n",
      "ByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\n",
      "only differ in how inputs should be prepared for the model, see the code examples below.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Since ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\n",
      "fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n",
      "\n",
      "\n",
      "## Usage example\n",
      "\n",
      "ByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n",
      "\n",
      "```python\n",
      ">>> from transformers import T5ForConditionalGeneration\n",
      ">>> import torch\n",
      "\n",
      ">>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n",
      "\n",
      ">>> num_special_tokens = 3\n",
      ">>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.\n",
      ">>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.\n",
      "\n",
      ">>> input_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num_special_tokens\n",
      "\n",
      ">>> labels = torch.tensor([list(\"La vie est comme une boîte de chocolat.\".encode(\"utf-8\"))]) + num_special_tokens\n",
      "\n",
      ">>> loss = model(input_ids, labels=labels).loss\n",
      ">>> loss.item()\n",
      "2.66\n",
      "```\n",
      "\n",
      "For batched inference and training it is however recommended to make use of the tokenizer:\n",
      "\n",
      "```python\n",
      ">>> from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
      "\n",
      ">>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
      "\n",
      ">>> model_inputs = tokenizer(\n",
      "...     [\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\"\n",
      "... )\n",
      ">>> labels_dict = tokenizer(\n",
      "...     [\"La vie est comme une boîte de chocolat.\", \"Aujourd'hui c'est lundi.\"], padding=\"longest\", return_tensors=\"pt\"\n",
      "... )\n",
      ">>> labels = labels_dict.input_ids\n",
      "\n",
      ">>> loss = model(**model_inputs, labels=labels).loss\n",
      ">>> loss.item()\n",
      "17.9\n",
      "```\n",
      "\n",
      ">>> loss = model(**model_inputs, labels=labels).loss\n",
      ">>> loss.item()\n",
      "17.9\n",
      "```\n",
      "\n",
      "Similar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \n",
      "since the model works directly on characters, the pretraining task is a bit \n",
      "different. Let's corrupt some characters of the \n",
      "input sentence `\"The dog chases a ball in the park.\"` and ask ByT5 to predict them \n",
      "for us.\n",
      "\n",
      "```python\n",
      ">>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      ">>> import torch\n",
      "\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\n",
      ">>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-base\")\n",
      "\n",
      ">>> input_ids_prompt = \"The dog chases a ball in the park.\"\n",
      ">>> input_ids = tokenizer(input_ids_prompt).input_ids\n",
      "\n",
      ">>> # Note that we cannot add \"{extra_id_...}\" to the string directly\n",
      ">>> # as the Byte tokenizer would incorrectly merge the tokens\n",
      ">>> # For ByT5, we need to work directly on the character level\n",
      ">>> # Contrary to T5, ByT5 does not use sentinel tokens for masking, but instead\n",
      ">>> # uses final utf character ids.\n",
      ">>> # UTF-8 is represented by 8 bits and ByT5 has 3 special tokens.\n",
      ">>> # => There are 2**8+2 = 259 input ids and mask tokens count down from index 258.\n",
      ">>> # => mask to \"The dog [258]a ball [257]park.\"\n",
      "\n",
      ">>> input_ids = torch.tensor([input_ids[:8] + [258] + input_ids[14:21] + [257] + input_ids[28:]])\n",
      ">>> input_ids\n",
      "tensor([[ 87, 107, 104,  35, 103, 114, 106,  35, 258,  35, 100,  35, 101, 100, 111, 111, 257,  35, 115, 100, 117, 110,  49,   1]])\n",
      "\n",
      ">>> input_ids = torch.tensor([input_ids[:8] + [258] + input_ids[14:21] + [257] + input_ids[28:]])\n",
      ">>> input_ids\n",
      "tensor([[ 87, 107, 104,  35, 103, 114, 106,  35, 258,  35, 100,  35, 101, 100, 111, 111, 257,  35, 115, 100, 117, 110,  49,   1]])\n",
      "\n",
      ">>> # ByT5 produces only one char at a time so we need to produce many more output characters here -> set `max_length=100`.\n",
      ">>> output_ids = model.generate(input_ids, max_length=100)[0].tolist()\n",
      ">>> output_ids\n",
      "[0, 258, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118, 257,  35, 108, 113,  35, 119, 107, 104,  35, 103, 108, 118, 102, 114, 256, 108, 113,  35, 119, 107, 104, 35, 115, 100, 117, 110,  49,  35,  87, 107, 104,  35, 103, 114, 106, 35, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118,  35, 100,  35, 101, 100, 111, 111,  35, 108, 113, 255,  35, 108, 113,  35, 119, 107, 104,  35, 115, 100, 117, 110,  49]\n",
      "\n",
      ">>> # ^- Note how 258 descends to 257, 256, 255\n",
      "\n",
      ">>> # Now we need to split on the sentinel tokens, let's write a short loop for this\n",
      ">>> output_ids_list = []\n",
      ">>> start_token = 0\n",
      ">>> sentinel_token = 258\n",
      ">>> while sentinel_token in output_ids:\n",
      "...     split_idx = output_ids.index(sentinel_token)\n",
      "...     output_ids_list.append(output_ids[start_token:split_idx])\n",
      "...     start_token = split_idx\n",
      "...     sentinel_token -= 1\n",
      "\n",
      ">>> output_ids_list.append(output_ids[start_token:])\n",
      ">>> output_string = tokenizer.batch_decode(output_ids_list)\n",
      ">>> output_string\n",
      "['<pad>', 'is the one who does', ' in the disco', 'in the park. The dog is the one who does a ball in', ' in the park.']\n",
      "```\n",
      "\n",
      "\n",
      "## ByT5Tokenizer\n",
      "\n",
      "[[autodoc]] ByT5Tokenizer\n",
      "\n",
      "See [`ByT5Tokenizer`] for all details.\n",
      "-----------------LLm ANSWER------------\n",
      "The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\".\n",
      "-----------------------------\n",
      "36\n",
      "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md\n",
      "What is the dimension of the feature vector for the base BERT model?\n",
      "\n",
      "768\n",
      "768\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md']\n",
      "Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the “context”.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence. As we've seen before, this means that the resulting representation of a word has been affected by other words in the sequence. We won't dive into the specifics here, but we'll offer some further readings if you want to get a better understanding at what happens under the hood. So when should one use an encoder? Encoders can be used as standalone models in a wide variety of tasks. For example BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of release, beat the state of the art in many sequence classification tasks, question answering tasks, and masked language modeling, to only cite a few. The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequence. This vector can then be handled down the road by additional layers of neurons to make sense of them. Let's take a look at some examples\n",
      "\n",
      "### Text classification\n",
      "\n",
      "[BERT](model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.\n",
      "\n",
      "1. BERT uses [WordPiece](tokenizer_summary#wordpiece) tokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special `[SEP]` token is added to differentiate them. A special `[CLS]` token is added to the beginning of every sequence of text. The final output with the `[CLS]` token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.\n",
      "\n",
      "2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and \"predict\" the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.\n",
      "\n",
      "    The second pretraining object is next-sentence prediction. The model must predict whether sentence B follows sentence A. Half of the time sentence B is the next sentence, and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feedforward network with a softmax over the two classes (`IsNext` and `NotNext`).\n",
      "\n",
      "3. The input embeddings are passed through multiple encoder layers to output some final hidden states.\n",
      "\n",
      "n this video, we'll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let's first start by understanding how it works. We'll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words “Welcome to NYC” in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \"Feature vector\", or \"Feature tensor\".\n",
      "\n",
      "3. The input embeddings are passed through multiple encoder layers to output some final hidden states.\n",
      "\n",
      "To use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label.\n",
      "\n",
      "Ready to try your hand at text classification? Check out our complete [text classification guide](tasks/sequence_classification) to learn how to finetune DistilBERT and use it for inference!\n",
      "\n",
      "### Token classification\n",
      "\n",
      "To use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.\n",
      "\n",
      "Ready to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!\n",
      "\n",
      "### Question answering\n",
      "\n",
      "To use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer.\n",
      "\n",
      "Ready to try your hand at question answering? Check out our complete [question answering guide](tasks/question_answering) to learn how to finetune DistilBERT and use it for inference!\n",
      "\n",
      "<Tip>\n",
      "\n",
      "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# ALBERT\n",
      "\n",
      "<div class=\"flex flex-wrap space-x-1\">\n",
      "<a href=\"https://huggingface.co/models?filter=albert\">\n",
      "<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\n",
      "</a>\n",
      "<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\n",
      "<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n",
      "</a>\n",
      "</div>\n",
      "\n",
      "## Overview\n",
      "\n",
      "The ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\n",
      "Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\n",
      "speed of BERT:\n",
      "\n",
      "- Splitting the embedding matrix into two smaller matrices.\n",
      "- Using repeating layers split among groups.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "-----------------LLm ANSWER------------\n",
      "According to the context, the dimension of the feature vector for the base BERT model is 768.\n",
      "-----------------------------\n",
      "37\n",
      "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx\n",
      "What special identifier does the WordPiece Model use for continuing subwords?\n",
      "\n",
      "##\n",
      "##\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/chapters/en/chapter2/4.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx', 'huggingface/course/blob/main/chapters/en/chapter2/4.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx']\n",
      "Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.\n",
      "\n",
      "If we want to completely cover a language with a word-based tokenizer, we'll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we'd need to keep track of that many IDs. Furthermore, words like \"dog\" are represented differently from words like \"dogs\", and the model will initially have no way of knowing that \"dog\" and \"dogs\" are similar: it will identify the two words as unrelated. The same applies to other similar words, like \"run\" and \"running\", which the model will not see as being similar initially.\n",
      "\n",
      "Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the \"unknown\" token, often represented as \"[UNK]\" or \"&lt;unk&gt;\". It's generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn't able to retrieve a sensible representation of a word and you're losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.\n",
      "\n",
      "One way to reduce the amount of unknown tokens is to go one level deeper, using a _character-based_ tokenizer.\n",
      "\n",
      "## Character-based[[character-based]]\n",
      "\n",
      "<Youtube id=\"ssLq_EK2jLE\"/>\n",
      "\n",
      "Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
      "\n",
      "- The vocabulary is much smaller.\n",
      "- There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
      "\n",
      "But here too some questions arise concerning spaces and punctuation:\n",
      "\n",
      "## Models\n",
      "\n",
      "Models are the core algorithms used to actually tokenize, and therefore,\n",
      "they are the only mandatory component of a Tokenizer.\n",
      "\n",
      "| Name | Description |\n",
      "| :--- | :--- |\n",
      "| WordLevel | This is the “classic” tokenization algorithm. It let’s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs.  |\n",
      "| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having “unk” (unknown) tokens.  |\n",
      "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\n",
      "| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\n",
      "\n",
      "## Post-Processors\n",
      "\n",
      "## Post-Processors\n",
      "\n",
      "After the whole pipeline, we sometimes want to insert some special\n",
      "tokens before feed a tokenized string into a model like \"[CLS] My\n",
      "horse is amazing [SEP]\". The `PostProcessor` is the component doing\n",
      "just that.\n",
      "\n",
      "| Name | Description | Example |\n",
      "| :--- | :--- | :--- |\n",
      "| TemplateProcessing | Let’s you easily template the post processing, adding special tokens, and specifying the `type_id` for each sequence/special token. The template is given two strings representing the single sequence and the pair of sequences, as well as a set of special tokens to use. | Example, when specifying a template with these values:<br> <ul> <li> single: `\"[CLS] $A [SEP]\"` </li> <li> pair: `\"[CLS] $A [SEP] $B [SEP]\"` </li> <li> special tokens: <ul> <li>`\"[CLS]\"`</li> <li>`\"[SEP]\"`</li> </ul> </li> </ul> <br> Input: `(\"I like this\", \"but not this\")` <br> Output: `\"[CLS] I like this [SEP] but not this [SEP]\"` |\n",
      "\n",
      "## Decoders\n",
      "\n",
      "The Decoder knows how to go from the IDs used by the Tokenizer, back to\n",
      "a readable piece of text. Some `Normalizer` and `PreTokenizer` use\n",
      "special characters or identifiers that need to be reverted for example.\n",
      "\n",
      "| Name | Description |\n",
      "| :--- | :--- |\n",
      "| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n",
      "| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `▁` to identify whitespaces, and so this Decoder helps with decoding these. |\n",
      "| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |\n",
      "\n",
      "<Youtube id=\"zHvTiHr506c\"/>\n",
      "\n",
      "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
      "\n",
      "For instance, \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\".\n",
      "\n",
      "Here is an example showing how a subword tokenization algorithm would tokenize the sequence \"Let's do tokenization!\":\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "  <img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg\" alt=\"A subword tokenization algorithm.\"/>\n",
      "  <img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg\" alt=\"A subword tokenization algorithm.\"/>\n",
      "</div>\n",
      "\n",
      "These subwords end up providing a lot of semantic meaning: for instance, in the example above \"tokenization\" was split into \"token\" and \"ization\", two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.\n",
      "\n",
      "This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n",
      "\n",
      "### And more![[and-more]]\n",
      "\n",
      "Unsurprisingly, there are many more techniques out there. To name a few:\n",
      "\n",
      "- Byte-level BPE, as used in GPT-2\n",
      "- WordPiece, as used in BERT\n",
      "- SentencePiece or Unigram, as used in several multilingual models\n",
      "\n",
      "You should now have sufficient knowledge of how tokenizers work to get started with the API.\n",
      "\n",
      "## Loading and saving[[loading-and-saving]]\n",
      "\n",
      "| Digits | Splits the numbers from any other characters. | Input: `\"Hello123there\"` <br>  Output: ``\"Hello\", \"123\", \"there\"``  |\n",
      "| Split | Versatile pre-tokenizer that splits on provided pattern and according to provided behavior. The pattern can be inverted if necessary. <ul> <li>pattern should be either a custom string or regexp.</li> <li>behavior should be one of: <ul><li>removed</li><li>isolated</li><li>merged_with_previous</li><li>merged_with_next</li><li>contiguous</li></ul></li> <li>invert should be a boolean flag.</li> </ul> | Example with pattern = ` `, behavior = `\"isolated\"`, invert = `False`: <br> Input: `\"Hello, how are you?\"` <br> Output: `\"Hello,\", \" \", \"how\", \" \", \"are\", \" \", \"you?\"` |\n",
      "| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the given order | `Sequence([Punctuation(), WhitespaceSplit()])` |\n",
      "</python>\n",
      "<rust>\n",
      "| Name | Description | Example |\n",
      "| :--- | :--- | :--- |\n",
      "| ByteLevel | Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties: <ul> <li>Since it maps on bytes, a tokenizer using this only requires **256** characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.</li> <li>A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! 🎉🎉)</li> <li>For non ascii characters, it gets completely unreadable, but it works nonetheless!</li> </ul> | Input: `\"Hello my friend, how are you?\"` <br> Ouput: `\"Hello\", \"Ġmy\", Ġfriend\", \",\", \"Ġhow\", \"Ġare\", \"Ġyou\", \"?\"`  |\n",
      "| Whitespace | Splits on word boundaries (using the following regular expression: `\\w+&#124;[^\\w\\s]+` | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there\", \"!\"`  |\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, the WordPiece Model uses the special identifier `##` for continuing subwords.\n",
      "-----------------------------\n",
      "38\n",
      "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md\n",
      "What is the purpose of the 🧨 Diffusers tutorials?\n",
      "\n",
      "To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n",
      "To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Overview\n",
      "\n",
      "Welcome to 🧨 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is meant to be used.\n",
      "\n",
      "You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want.\n",
      "\n",
      "After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\n",
      "\n",
      "Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\n",
      "\n",
      "Let's start diffusing! 🧨\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Philosophy\n",
      "\n",
      "🧨 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\n",
      "Its purpose is to serve as a **modular toolbox** for both inference and training.\n",
      "\n",
      "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
      "\n",
      "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n",
      "\n",
      "## Usability over Performance\n",
      "\n",
      "At Hugging Face, we call this design the **single-file policy** which means that almost all of the code of a certain class should be written in a single, self-contained file. To read more about the philosophy, you can have a look\n",
      "at [this blog post](https://huggingface.co/blog/transformers-design-philosophy).\n",
      "\n",
      "In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffusion models. The reason we don't follow this design fully for diffusion models is because almost all diffusion pipelines, such\n",
      "as [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm), [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview#stable-diffusion-pipelines), [unCLIP (DALL·E 2)](https://huggingface.co/docs/diffusers/api/pipelines/unclip) and [Imagen](https://imagen.research.google/) all rely on the same diffusion model, the [UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond).\n",
      "\n",
      "Great, now you should have generally understood why 🧨 Diffusers is designed the way it is 🤗.\n",
      "We try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would ❤️  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\n",
      "\n",
      "## Design Philosophy in Details\n",
      "\n",
      "Now, let's look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\n",
      "Let's walk through more in-detail design decisions for each class.\n",
      "\n",
      "### Pipelines\n",
      "\n",
      "## Simple over easy\n",
      "\n",
      "As PyTorch states, **explicit is better than implicit** and **simple is better than complex**. This design philosophy is reflected in multiple parts of the library:\n",
      "- We follow PyTorch's API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management.\n",
      "- Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible.\n",
      "- Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers.\n",
      "- Separately trained components of the diffusion pipeline, *e.g.* the text encoder, the unet, and the variational autoencoder, each have their own model class. This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\n",
      "is very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline.\n",
      "\n",
      "## Tweakable, contributor-friendly over abstraction\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# 🧨 Diffusers’ Ethical Guidelines\n",
      "\n",
      "## Preamble\n",
      "\n",
      "[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n",
      "\n",
      "Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library.\n",
      "\n",
      "The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\n",
      "We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n",
      "\n",
      "\n",
      "## Scope\n",
      "\n",
      "The Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n",
      "\n",
      "\n",
      "## Ethical guidelines\n",
      "-----------------LLm ANSWER------------\n",
      "The purpose of the 🧨 Diffusers tutorials is to provide a gentle introduction to diffusion models and generative AI, helping users understand the library fundamentals and learn how to use it as a modular toolbox for building their own diffusion systems.\n",
      "-----------------------------\n",
      "39\n",
      "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md\n",
      "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n",
      "\n",
      "\"manual\"\n",
      "\"manual\"\n",
      "-----------------Context------------\n",
      "['gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md', 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md', 'huggingface/course/blob/main/chapters/en/chapter9/9.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/9.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/9.mdx']\n",
      "iface = gr.Interface(\n",
      "    calculator,\n",
      "    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n",
      "    \"number\",\n",
      "    allow_flagging=\"manual\"\n",
      ")\n",
      "\n",
      "iface.launch()\n",
      "```\n",
      "\n",
      "<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n",
      "\n",
      "当您点击上面的标记按钮时，启动界面的目录将包括一个新的标记子文件夹，其中包含一个 CSV 文件。该 CSV 文件包括所有被标记的数据。\n",
      "\n",
      "```directory\n",
      "+-- flagged/\n",
      "|   +-- logs.csv\n",
      "```\n",
      "\n",
      "_flagged/logs.csv_\n",
      "\n",
      "```csv\n",
      "num1,operation,num2,Output,timestamp\n",
      "5,add,7,12,2022-01-31 11:40:51.093412\n",
      "6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\n",
      "```\n",
      "\n",
      "如果界面涉及文件数据，例如图像和音频组件，还将创建文件夹来存储这些标记的数据。例如，将 `image` 输入到 `image` 输出界面将创建以下结构。\n",
      "\n",
      "```directory\n",
      "+-- flagged/\n",
      "|   +-- logs.csv\n",
      "|   +-- image/\n",
      "|   |   +-- 0.png\n",
      "|   |   +-- 1.png\n",
      "|   +-- Output/\n",
      "|   |   +-- 0.png\n",
      "|   |   +-- 1.png\n",
      "```\n",
      "\n",
      "_flagged/logs.csv_\n",
      "\n",
      "```csv\n",
      "im,Output timestamp\n",
      "im/0.png,Output/0.png,2022-02-04 19:49:58.026963\n",
      "im/1.png,Output/1.png,2022-02-02 10:40:51.093412\n",
      "```\n",
      "\n",
      "如果您希望用户为标记提供一个原因，您可以将字符串列表传递给 Interface 的 `flagging_options` 参数。用户在标记时必须选择其中一项，选项将作为附加列保存在 CSV 文件中。\n",
      "\n",
      "如果我们回到计算器示例，下面的代码将创建嵌入其中的界面。\n",
      "\n",
      "```python\n",
      "iface = gr.Interface(\n",
      "    calculator,\n",
      "    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n",
      "    \"number\",\n",
      "    allow_flagging=\"manual\",\n",
      "    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\n",
      ")\n",
      "\n",
      "iface.launch()\n",
      "```\n",
      "\n",
      "<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\n",
      "\n",
      "当用户点击标记按钮时，CSV 文件现在将包括指示所选选项的列。\n",
      "\n",
      "_flagged/logs.csv_\n",
      "\n",
      "```csv\n",
      "num1,operation,num2,Output,flag,timestamp\n",
      "5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\n",
      "6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\n",
      "```\n",
      "\n",
      "## HuggingFaceDatasetSaver 回调\n",
      "\n",
      "有时，将数据保存到本地 CSV 文件是不合理的。例如，在 Hugging Face Spaces 上\n",
      "，开发者通常无法访问托管 Gradio 演示的底层临时机器。这就是为什么，默认情况下，在 Hugging Face Space 中关闭标记的原因。然而，\n",
      "您可能希望对标记的数据做其他处理。\n",
      "you may want to do something else with the flagged data.\n",
      "\n",
      "通过 `flagging_callback` 参数，我们使这变得非常简单。\n",
      "\n",
      "例如，下面我们将会将标记的数据从我们的计算器示例导入到 Hugging Face 数据集中，以便我们可以构建一个“众包”数据集：\n",
      "\n",
      "```python\n",
      "import os\n",
      "\n",
      "使用标记\n",
      "\n",
      "相关空间：https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\n",
      "标签：标记，数据\n",
      "\n",
      "## 简介\n",
      "\n",
      "当您演示一个机器学习模型时，您可能希望收集试用模型的用户的数据，特别是模型行为不如预期的数据点。捕获这些“困难”数据点是有价值的，因为它允许您改进机器学习模型并使其更可靠和稳健。\n",
      "\n",
      "Gradio 通过在每个“界面”中包含一个**标记**按钮来简化这些数据的收集。这使得用户或测试人员可以轻松地将数据发送回运行演示的机器。样本会保存在一个 CSV 日志文件中（默认情况下）。如果演示涉及图像、音频、视频或其他类型的文件，则这些文件会单独保存在一个并行目录中，并且这些文件的路径会保存在 CSV 文件中。\n",
      "\n",
      "## 在 `gradio.Interface` 中使用**标记**按钮\n",
      "\n",
      "使用 Gradio 的 `Interface` 进行标记特别简单。默认情况下，在输出组件下方有一个标记为**标记**的按钮。当用户测试您的模型时，如果看到有趣的输出，他们可以点击标记按钮将输入和输出数据发送回运行演示的机器。样本会保存在一个 CSV 日志文件中（默认情况下）。如果演示涉及图像、音频、视频或其他类型的文件，则这些文件会单独保存在一个并行目录中，并且这些文件的路径会保存在 CSV 文件中。\n",
      "\n",
      "在 `gradio.Interface` 中有[四个参数](https://gradio.app/docs/#interface-header)控制标记的工作方式。我们将详细介绍它们。\n",
      "\n",
      "- `allow_flagging`：此参数可以设置为 `\"manual\"`（默认值），`\"auto\"` 或 `\"never\"`。\n",
      "  - `manual`：用户将看到一个标记按钮，只有在点击按钮时样本才会被标记。\n",
      "  - `auto`：用户将不会看到一个标记按钮，但每个样本都会自动被标记。\n",
      "  - `never`：用户将不会看到一个标记按钮，并且不会标记任何样本。\n",
      "- `flagging_options`：此参数可以是 `None`（默认值）或字符串列表。\n",
      "  - 如果是 `None`，则用户只需点击**标记**按钮，不会显示其他选项。\n",
      "  - 如果提供了一个字符串列表，则用户会看到多个按钮，对应于提供的每个字符串。例如，如果此参数的值为`[\" 错误 \", \" 模糊 \"]`，则会显示标记为**标记为错误**和**标记为模糊**的按钮。这仅适用于 `allow_flagging` 为 `\"manual\"` 的情况。\n",
      "  - 所选选项将与输入和输出一起记录。\n",
      "- `flagging_dir`：此参数接受一个字符串。\n",
      "  - 它表示标记数据存储的目录名称。\n",
      "- `flagging_callback`：此参数接受 `FlaggingCallback` 类的子类的实例\n",
      "  - 使用此参数允许您编写在点击标记按钮时运行的自定义代码\n",
      "  - 默认情况下，它设置为 `gr.CSVLogger` 的一个实例\n",
      "  - 一个示例是将其设置为 `gr.HuggingFaceDatasetSaver` 的一个实例，这样您可以将任何标记的数据导入到 HuggingFace 数据集中（参见下文）。\n",
      "\n",
      "## 标记的数据会发生什么？\n",
      "\n",
      "在 `flagging_dir` 参数提供的目录中，将记录标记的数据的 CSV 文件。\n",
      "\n",
      "以下是一个示例：下面的代码创建了嵌入其中的计算器界面：\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "\n",
      "def calculator(num1, operation, num2):\n",
      "    if operation == \"add\":\n",
      "        return num1 + num2\n",
      "    elif operation == \"subtract\":\n",
      "        return num1 - num2\n",
      "    elif operation == \"multiply\":\n",
      "        return num1 * num2\n",
      "    elif operation == \"divide\":\n",
      "        return num1 / num2\n",
      "\n",
      "### 3. Where can you launch a Gradio demo from?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Standard python IDEs\",\n",
      "\t\t\texplain: \"Gradio works great with your favorite IDE.\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Google Colab notebooks\",\n",
      "\t\t\texplain: \"You can create and launch a demo within your Google colab notebook.\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Jupyter notebooks\",\n",
      "\t\t\texplain: \"Good choice - You can create and launch a demo within your Jupyter notebook.\",\n",
      "\t\t\tcorrect: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 4. Gradio is designed primarily for NLP models\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"True\",\n",
      "\t\t\texplain: \"Gradio works with pretty much any data type, not just NLP.\"\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"False\",\n",
      "\t\t\texplain: \"Gradio supplies developers with a library of pre-built components for pretty much all data types.\",\n",
      "            correct: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 5. Which of the following features are supported by Gradio?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Multiple inputs and outputs\",\n",
      "\t\t\texplain: \"Multiple inputs and outputs is possible with gradio. All you need to do is pass in a list of inputs and outputs to their corresponding parameters\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"State for data persistance\",\n",
      "\t\t\texplain: \"Gradio is capable of adding state to your interface.\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Username and passwords authentication\",\n",
      "\t\t\texplain: \"Pass in a list of username/password tuples to the launch method to add authentication.\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Automatic analytics for who uses your gradio demo\",\n",
      "\t\t\texplain: \"Try again - Gradio does not supply developers analytics on who uses their demos.\"\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Loading a model from Hugging Face's model hub or Hugging Face Spaces\",\n",
      "\t\t\texplain: \"Absolutely - load any Hugging Face model using the <code>gr.Interface.load()</code> method\",\n",
      "\t\t\tcorrect: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "!-- DISABLE-FRONTMATTER-SECTIONS -->\n",
      "\n",
      "# End-of-chapter quiz[[end-of-chapter-quiz]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={9}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "Let's test what you learned in this chapter!\n",
      "\n",
      "### 1. What can you use Gradio to do?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Create a demo for your machine learning model\",\n",
      "\t\t\texplain: \"With a few lines of python code you can generate a demo for your ML model using our library of pre-built components.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Share your machine learning model with others\",\n",
      "\t\t\texplain: \"Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone.\",\n",
      "            correct: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Debug your model\",\n",
      "\t\t\texplain: \"One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Train your model\",\n",
      "\t\t\texplain: \"Gradio is designed to be used for model inference, AFTER your model is trained.\",\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 2. Gradio ONLY works with PyTorch models\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"True\",\n",
      "\t\t\texplain: \"Gradio works with PyTorch models, but also works for any type of machine learning model!\"\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"False\",\n",
      "\t\t\texplain: \"Gradio is model agnostic, meaning you can create a demo for any type of machine learning model.\",\n",
      "\t\t\tcorrect: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 3. Where can you launch a Gradio demo from?\n",
      "\n",
      "### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"gr.Interface.load('huggingface/{user}/{model_name}')\",\n",
      "\t\t\texplain: \"This is a valid method of loading a Hugging Face model from the Hub\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"gr.Interface.load('model/{user}/{model_name}')\",\n",
      "\t\t\texplain: \"This is a valid method of loading a Hugging Face model from the Hub\",\n",
      "\t\t\tcorrect: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"gr.Interface.load('demos/{user}/{model_name}')\",\n",
      "\t\t\texplain: \"Try again -- you cannot load a model by using the 'demos' prefix.\"\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"gr.Interface.load('spaces/{user}/{model_name}')\",\n",
      "\t\t\texplain: \"This is a valid method of loading a Hugging Face model from Spaces\",\n",
      "\t\t\tcorrect: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 7. Select all the steps necessary for adding state to your Gradio interface\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Pass in an extra parameter into your prediction function, which represents the state of the interface.\",\n",
      "\t\t\texplain: \"An extra parameter storing history or state of your interface is necessary.\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"At the end of the prediction function, return the updated value of the state as an extra return value.\",\n",
      "\t\t\texplain: \"This history or state value needs to be returned by your function.\",\n",
      "            correct: true\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"Add the state input and state output components when creating your Interface\",\n",
      "\t\t\texplain: \"Gradio provides a state input and output component to persist data.\",\n",
      "            correct: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 8. Which of the following are components included in the Gradio library?\n",
      "-----------------LLm ANSWER------------\n",
      "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`.\n",
      "-----------------------------\n",
      "40\n",
      "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md\n",
      "Where can the full code for the Stable Diffusion demo be found?\n",
      "\n",
      "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "-----------------Context------------\n",
      "['gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md', 'huggingface/optimum/blob/main/docs/source/notebooks.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md']\n",
      "ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "\n",
      "| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb) | Show how to load and compare outputs from two Stable Diffusion models with different precision| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)|\n",
      "\n",
      "- [Stable Diffusion](./stable_diffusion/overview)\n",
      "- [ControlNet](./controlnet)\n",
      "- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n",
      "- [DeepFloyd IF](./deepfloyd_if)\n",
      "- [Kandinsky 2.1](./kandinsky)\n",
      "- [Kandinsky 2.2](./kandinsky_v22)\n",
      "\n",
      "\n",
      "## AutoPipelineForText2Image\n",
      "\n",
      "[[autodoc]] AutoPipelineForText2Image\n",
      "\t- all\n",
      "\t- from_pretrained\n",
      "\t- from_pipe\n",
      "\n",
      "## AutoPipelineForImage2Image\n",
      "\n",
      "[[autodoc]] AutoPipelineForImage2Image\n",
      "\t- all\n",
      "\t- from_pretrained\n",
      "\t- from_pipe\n",
      "\n",
      "## AutoPipelineForInpainting\n",
      "\n",
      "[[autodoc]] AutoPipelineForInpainting\n",
      "\t- all\n",
      "\t- from_pretrained\n",
      "\t- from_pipe\n",
      "\n",
      "The abstract from the paper is:\n",
      "\n",
      "*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*\n",
      "\n",
      "The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\n",
      "\n",
      "`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "You can find lucidrains' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "## UnCLIPPipeline\n",
      "[[autodoc]] UnCLIPPipeline\n",
      "\t- all\n",
      "\t- __call__\n",
      "\n",
      "## UnCLIPImageVariationPipeline\n",
      "[[autodoc]] UnCLIPImageVariationPipeline\n",
      "\t- all\n",
      "\t- __call__\n",
      "\n",
      "## ImagePipelineOutput\n",
      "[[autodoc]] pipelines.ImagePipelineOutput\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided context, the full code for the Stable Diffusion demo can be found at https://hf.co/spaces/stabilityai/stable-diffusion/tree/main.\n",
      "-----------------------------\n",
      "41\n",
      "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md\n",
      "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n",
      "\n",
      "Fourier transform\n",
      "Fourier transform\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md']\n",
      "!--Copyright 2021 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# FNet\n",
      "\n",
      "## Overview\n",
      "\n",
      "The FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\n",
      "James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\n",
      "model with a fourier transform which returns only the real parts of the transform. The model is significantly faster\n",
      "than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\n",
      "accuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\n",
      "paper is the following:\n",
      "\n",
      "*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\n",
      "self-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\n",
      "standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\n",
      "classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\n",
      "with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\n",
      "benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\n",
      "our FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\n",
      "benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\n",
      "sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\n",
      "and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\n",
      "outperform Transformer counterparts.*\n",
      "\n",
      "This model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\n",
      "\n",
      "## Usage tips\n",
      "\n",
      "The model was trained without an attention mask as it is based on Fourier Transform. The model was trained with \n",
      "maximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum \n",
      "sequence length for fine-tuning and inference.\n",
      "\n",
      "## Resources\n",
      "\n",
      "## Resources\n",
      "\n",
      "- [Text classification task guide](../tasks/sequence_classification)\n",
      "- [Token classification task guide](../tasks/token_classification)\n",
      "- [Question answering task guide](../tasks/question_answering)\n",
      "- [Masked language modeling task guide](../tasks/masked_language_modeling)\n",
      "- [Multiple choice task guide](../tasks/multiple_choice)\n",
      "\n",
      "## FNetConfig\n",
      "\n",
      "[[autodoc]] FNetConfig\n",
      "\n",
      "## FNetTokenizer\n",
      "\n",
      "[[autodoc]] FNetTokenizer\n",
      "    - build_inputs_with_special_tokens\n",
      "    - get_special_tokens_mask\n",
      "    - create_token_type_ids_from_sequences\n",
      "    - save_vocabulary\n",
      "\n",
      "## FNetTokenizerFast\n",
      "\n",
      "[[autodoc]] FNetTokenizerFast\n",
      "\n",
      "## FNetModel\n",
      "\n",
      "[[autodoc]] FNetModel\n",
      "    - forward\n",
      "\n",
      "## FNetForPreTraining\n",
      "\n",
      "[[autodoc]] FNetForPreTraining\n",
      "    - forward\n",
      "\n",
      "## FNetForMaskedLM\n",
      "\n",
      "[[autodoc]] FNetForMaskedLM\n",
      "    - forward\n",
      "\n",
      "## FNetForNextSentencePrediction\n",
      "\n",
      "[[autodoc]] FNetForNextSentencePrediction\n",
      "    - forward\n",
      "\n",
      "## FNetForSequenceClassification\n",
      "\n",
      "[[autodoc]] FNetForSequenceClassification\n",
      "    - forward\n",
      "\n",
      "## FNetForMultipleChoice\n",
      "\n",
      "[[autodoc]] FNetForMultipleChoice\n",
      "    - forward\n",
      "\n",
      "## FNetForTokenClassification\n",
      "\n",
      "[[autodoc]] FNetForTokenClassification\n",
      "    - forward\n",
      "\n",
      "## FNetForQuestionAnswering\n",
      "\n",
      "[[autodoc]] FNetForQuestionAnswering\n",
      "    - forward\n",
      "\n",
      "5. ConvNeXT also makes several layer design changes that imitate Transformer models. There are fewer activation and normalization layers,  the activation function is switched to GELU instead of ReLU, and it uses LayerNorm instead of BatchNorm.\n",
      "\n",
      "The output from the convolution blocks is passed to a classification head which converts the outputs into logits and calculates the cross-entropy loss to find the most likely label.\n",
      "\n",
      "### Object detection\n",
      "\n",
      "[DETR](model_doc/detr), *DEtection TRansformer*, is an end-to-end object detection model that combines a CNN with a Transformer encoder-decoder.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png\"/>\n",
      "</div>\n",
      "\n",
      "1. A pretrained CNN *backbone* takes an image, represented by its pixel values, and creates a low-resolution feature map of it. A 1x1 convolution is applied to the feature map to reduce dimensionality and it creates a new feature map with a high-level image representation. Since the Transformer is a sequential model, the feature map is flattened into a sequence of feature vectors that are combined with positional embeddings.\n",
      "\n",
      "2. The feature vectors are passed to the encoder, which learns the image representations using its attention layers. Next, the encoder hidden states are combined with *object queries* in the decoder. Object queries are learned embeddings that focus on the different regions of an image, and they're updated as they progress through each attention layer. The decoder hidden states are passed to a feedforward network that predicts the bounding box coordinates and class label for each object query, or `no object` if there isn't one.\n",
      "\n",
      "!--Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "\n",
      "⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "\n",
      "-->\n",
      "\n",
      "# SqueezeBERT\n",
      "\n",
      "## Overview\n",
      "\n",
      "The SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\n",
      "bidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\n",
      "SqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\n",
      "instead of fully-connected layers for the Q, K, V and FFN layers.\n",
      "\n",
      "The abstract from the paper is the following:\n",
      "-----------------LLm ANSWER------------\n",
      "The FNet model uses a Fourier Transform to replace the self-attention layer in a BERT model.\n",
      "-----------------------------\n",
      "42\n",
      "gradio-app/gradio/blob/main/test-strategy.md\n",
      "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n",
      "\n",
      "Dynamic code test\n",
      "Dynamic code test\n",
      "-----------------Context------------\n",
      "['gradio-app/gradio/blob/main/test-strategy.md', 'gradio-app/gradio/blob/main/test-strategy.md', 'gradio-app/gradio/blob/main/test-strategy.md', 'gradio-app/gradio/blob/main/test-strategy.md', 'huggingface/course/blob/main/chapters/en/chapter9/9.mdx']\n",
      "For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n",
      "\n",
      "## Managing defects\n",
      "\n",
      "As we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n",
      "\n",
      "- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.\n",
      "\n",
      "Test Strategy\n",
      "\n",
      "Very brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.\n",
      "\n",
      "This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n",
      "\n",
      "## Objectives\n",
      "\n",
      "The purposes of all testing activities on Gradio fit one of the following objectives:\n",
      "\n",
      "1. Ensure that the Gradio library functions as we expect it to.\n",
      "2. Enable the maintenance team to quickly identify both the presence and source of defects.\n",
      "3. Prevent regressions, i.e. if we fix something it should stay fixed.\n",
      "4. Improve the quality of the codebase in order to ease maintenance efforts.\n",
      "5. Reduce the amount of manual testing required.\n",
      "\n",
      "## Scope\n",
      "\n",
      "Testing is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas.\n",
      "\n",
      "We should not focus on code coverage but on test coverage following the below criteria:\n",
      "\n",
      "Testing is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas.\n",
      "\n",
      "We should not focus on code coverage but on test coverage following the below criteria:\n",
      "\n",
      "- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n",
      "- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n",
      "- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n",
      "- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n",
      "- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n",
      "- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n",
      "\n",
      "## Types of testing\n",
      "\n",
      "Our tests will broadly fall into one of three categories:\n",
      "\n",
      "- Static Quality checks\n",
      "- Dynamic 'Code' tests\n",
      "- Dynamic Functional tests\n",
      "\n",
      "### Static Quality checks\n",
      "\n",
      "Static quality checks are generally very fast to run and do not require building the code base. They also provide the least value. These tests would be things like linting, typechecking, and formatting.\n",
      "\n",
      "While they offer little in terms of testing functionality they align very closely with objective (4, 5) as they generally help to keep the codebase in good shape and offer very fast feedback. Such check are almost free from an authoring point of view as fixes can be mostly automated (either via scripts or editor integrations).\n",
      "\n",
      "### Dynamic code tests\n",
      "\n",
      "### Functional/acceptance tests\n",
      "\n",
      "- playwright (full end to end testing)\n",
      "- chromatic (visual testing) [in progress]\n",
      "- Accessibility testing [to do]\n",
      "\n",
      "## Supported environments and versions\n",
      "\n",
      "All operating systems refer to the current runner variants supported by GitHub actions.\n",
      "\n",
      "All unspecified version segments (`x`) refer to latest.\n",
      "\n",
      "| Software | Version(s)            | Operating System(s)               |\n",
      "| -------- | --------------------- | --------------------------------- |\n",
      "| Python   | `3.8.x`               | `ubuntu-latest`, `windows-latest` |\n",
      "| Node     | `18.x.x`              | `ubuntu-latest`                   |\n",
      "| Browser  | `playwright-chrome-x` | `ubuntu-latest`                   |\n",
      "\n",
      "## Test execution\n",
      "\n",
      "Tests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\n",
      "\n",
      "- **Locally**: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally.\n",
      "- **CI** - It is _critical_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action.\n",
      "\n",
      "For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n",
      "\n",
      "## Managing defects\n",
      "\n",
      "!-- DISABLE-FRONTMATTER-SECTIONS -->\n",
      "\n",
      "# End-of-chapter quiz[[end-of-chapter-quiz]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={9}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "Let's test what you learned in this chapter!\n",
      "\n",
      "### 1. What can you use Gradio to do?\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"Create a demo for your machine learning model\",\n",
      "\t\t\texplain: \"With a few lines of python code you can generate a demo for your ML model using our library of pre-built components.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Share your machine learning model with others\",\n",
      "\t\t\texplain: \"Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone.\",\n",
      "            correct: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Debug your model\",\n",
      "\t\t\texplain: \"One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Train your model\",\n",
      "\t\t\texplain: \"Gradio is designed to be used for model inference, AFTER your model is trained.\",\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 2. Gradio ONLY works with PyTorch models\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "        {\n",
      "\t\t\ttext: \"True\",\n",
      "\t\t\texplain: \"Gradio works with PyTorch models, but also works for any type of machine learning model!\"\n",
      "        },\n",
      "        {\n",
      "\t\t\ttext: \"False\",\n",
      "\t\t\texplain: \"Gradio is model agnostic, meaning you can create a demo for any type of machine learning model.\",\n",
      "\t\t\tcorrect: true\n",
      "        }\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 3. Where can you launch a Gradio demo from?\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should typically be a dynamic code test, but it could be a linting rule or new type if that is appropriate.\n",
      "-----------------------------\n",
      "43\n",
      "huggingface/blog/blob/main/accelerate-library.md\n",
      "How can you force mixed precision training when initializing the Accelerator in 🤗 Accelerate?\n",
      "\n",
      "By passing `fp16=True` to the Accelerator init.\n",
      "By passing `fp16=True` to the Accelerator init.\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md', 'huggingface/blog/blob/main/accelerate-library.md', 'huggingface/blog/blob/main/accelerate-library.md', 'huggingface/blog/blob/main/accelerate-library.md']\n",
      "Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures \n",
      "(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check \n",
      "out the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about \n",
      "the differences between these data types.\n",
      "\n",
      "### fp16\n",
      "\n",
      "The main advantage of mixed precision training comes from saving the activations in half precision (fp16). \n",
      "Although the gradients are also computed in half precision they are converted back to full precision for the optimization \n",
      "step so no memory is saved here. \n",
      "While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes.\n",
      "This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\n",
      "\n",
      "To enable mixed precision training, set the `fp16` flag to `True`:\n",
      "\n",
      "```py\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n",
      "```\n",
      "\n",
      "If you prefer to use 🤗 Accelerate, find the 🤗 Accelerate example [further in this guide](#using-accelerate). \n",
      "\n",
      "### BF16\n",
      "\n",
      "If you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \n",
      "bf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \n",
      "is `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \n",
      "is about the same as fp32 - because both have 8-bits used for the numerical range.\n",
      "\n",
      "You can enable BF16 in the 🤗 Trainer with:\n",
      "\n",
      "```python\n",
      "training_args = TrainingArguments(bf16=True, **default_args)\n",
      "```\n",
      "\n",
      "### TF32\n",
      "\n",
      "if training_args.gradient_checkpointing:\n",
      "    model.gradient_checkpointing_enable()\n",
      "\n",
      "accelerator = Accelerator(fp16=training_args.fp16)\n",
      "model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n",
      "\n",
      "model.train()\n",
      "for step, batch in enumerate(dataloader, start=1):\n",
      "    loss = model(**batch).loss\n",
      "    loss = loss / training_args.gradient_accumulation_steps\n",
      "    accelerator.backward(loss)\n",
      "    if step % training_args.gradient_accumulation_steps == 0:\n",
      "        optimizer.step()\n",
      "        optimizer.zero_grad()\n",
      "```\n",
      "\n",
      "First we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \n",
      "Then we can enable gradient checkpointing by calling the model's [`~PreTrainedModel.gradient_checkpointing_enable`] method. \n",
      "When we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator) \n",
      "we can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. \n",
      "During the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare) \n",
      "call the dataloader will also be distributed across workers should we use multiple GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example.\n",
      "\n",
      "Finally, we can add the main training loop. Note that the `backward` call is handled by 🤗 Accelerate. We can also see\n",
      "how gradient accumulation works: we normalize the loss, so we get the average at the end of accumulation and once we have \n",
      "enough steps we run the optimization. \n",
      "\n",
      "Implementing these optimization techniques with 🤗 Accelerate only takes a handful of lines of code and comes with the \n",
      "benefit of more flexibility in the training loop. For a full documentation of all features have a look at the \n",
      "[Accelerate documentation](https://huggingface.co/docs/accelerate/index).\n",
      "\n",
      "\n",
      "## Efficient Software Prebuilds\n",
      "\n",
      "optimizer.zero_grad()\n",
      "\n",
      "          output = model(source)\n",
      "          loss = F.cross_entropy(output, targets)\n",
      "\n",
      "-         loss.backward()\n",
      "+         accelerator.backward(loss)\n",
      "\n",
      "          optimizer.step()\n",
      "```\n",
      "\n",
      "By just adding five lines of code to any standard PyTorch training script, you can now run said script on any kind of distributed setting, as well as with or without mixed precision. 🤗 Accelerate even handles the device placement for you, so you can simplify the training loop above even further:\n",
      "\n",
      "```diff\n",
      "  import torch\n",
      "  import torch.nn.functional as F\n",
      "  from datasets import load_dataset\n",
      "+ from accelerate import Accelerator\n",
      "\n",
      "+ accelerator = Accelerator()\n",
      "- device = 'cpu'\n",
      "\n",
      "- model = torch.nn.Transformer().to(device)\n",
      "+ model = torch.nn.Transformer()\n",
      "  optim = torch.optim.Adam(model.parameters())\n",
      "\n",
      "  dataset = load_dataset('my_dataset')\n",
      "  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
      "\n",
      "+ model, optim, data = accelerator.prepare(model, optim, data)\n",
      "\n",
      "  model.train()\n",
      "  for epoch in range(10):\n",
      "      for source, targets in data:\n",
      "-         source = source.to(device)\n",
      "-         targets = targets.to(device)\n",
      "\n",
      "          optimizer.zero_grad()\n",
      "\n",
      "          output = model(source)\n",
      "          loss = F.cross_entropy(output, targets)\n",
      "\n",
      "-         loss.backward()\n",
      "+         accelerator.backward(loss)\n",
      "\n",
      "          optimizer.step()\n",
      "```\n",
      "\n",
      "In contrast, here are the changes needed to have this code run with distributed training are the followings:\n",
      "\n",
      "```diff\n",
      "+ import os\n",
      "  import torch\n",
      "  import torch.nn.functional as F\n",
      "  from datasets import load_dataset\n",
      "+ from torch.utils.data import DistributedSampler\n",
      "+ from torch.nn.parallel import DistributedDataParallel\n",
      "\n",
      "+ local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
      "- device = 'cpu'\n",
      "+ device = device = torch.device(\"cuda\", local_rank)\n",
      "\n",
      "  model = torch.nn.Transformer().to(device)\n",
      "+ model = DistributedDataParallel(model)  \n",
      "  optim = torch.optim.Adam(model.parameters())\n",
      "\n",
      "--\n",
      "title: \"Introducing 🤗 Accelerate\"\n",
      "thumbnail: /blog/assets/20_accelerate_library/accelerate_diff.png\n",
      "authors:\n",
      "- user: sgugger\n",
      "---\n",
      "\n",
      "# Introducing 🤗 Accelerate\n",
      "\n",
      "\n",
      "## 🤗 Accelerate\n",
      "\n",
      "Run your **raw** PyTorch training scripts on any kind of device.\n",
      "\n",
      "Most high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. 🤗 Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism.\n",
      "\n",
      "It provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups.\n",
      "\n",
      "### Easy integration!\n",
      "\n",
      "Let's first have a look at an example:\n",
      "\n",
      "```diff\n",
      "  import torch\n",
      "  import torch.nn.functional as F\n",
      "  from datasets import load_dataset\n",
      "+ from accelerate import Accelerator\n",
      "\n",
      "+ accelerator = Accelerator()\n",
      "- device = 'cpu'\n",
      "+ device = accelerator.device\n",
      "\n",
      "  model = torch.nn.Transformer().to(device)\n",
      "  optim = torch.optim.Adam(model.parameters())\n",
      "\n",
      "  dataset = load_dataset('my_dataset')\n",
      "  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
      "\n",
      "+ model, optim, data = accelerator.prepare(model, optim, data)\n",
      "\n",
      "  model.train()\n",
      "  for epoch in range(10):\n",
      "      for source, targets in data:\n",
      "          source = source.to(device)\n",
      "          targets = targets.to(device)\n",
      "\n",
      "          optimizer.zero_grad()\n",
      "\n",
      "          output = model(source)\n",
      "          loss = F.cross_entropy(output, targets)\n",
      "\n",
      "-         loss.backward()\n",
      "+         accelerator.backward(loss)\n",
      "\n",
      "          optimizer.step()\n",
      "```\n",
      "\n",
      "model = torch.nn.Transformer().to(device)\n",
      "+ model = DistributedDataParallel(model)  \n",
      "  optim = torch.optim.Adam(model.parameters())\n",
      "\n",
      "  dataset = load_dataset('my_dataset')\n",
      "+ sampler = DistributedSampler(dataset)\n",
      "- data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
      "+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)\n",
      "\n",
      "  model.train()\n",
      "  for epoch in range(10):\n",
      "+     sampler.set_epoch(epoch)  \n",
      "      for source, targets in data:\n",
      "          source = source.to(device)\n",
      "          targets = targets.to(device)\n",
      "\n",
      "          optimizer.zero_grad()\n",
      "\n",
      "          output = model(source)\n",
      "          loss = F.cross_entropy(output, targets)\n",
      "\n",
      "          loss.backward()\n",
      "\n",
      "          optimizer.step()\n",
      "```\n",
      "\n",
      "These changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere). Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\n",
      "- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible.\n",
      "- to have the same functions work for any distributed setup, so only have to learn one API.\n",
      "\n",
      "### How does it work?\n",
      "\n",
      "To see how the library works in practice, let's have a look at each line of code we need to add to a training loop.\n",
      "\n",
      "```python\n",
      "accelerator = Accelerator()\n",
      "```\n",
      "\n",
      "On top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script.\n",
      "\n",
      "```python\n",
      "model, optim, data = accelerator.prepare(model, optim, data)\n",
      "```\n",
      "-----------------LLm ANSWER------------\n",
      "According to the context, you can enable mixed precision training by setting the `fp16` flag to `True` when initializing the `Accelerator`:\n",
      "\n",
      "```python\n",
      "accelerator = Accelerator(fp16=True)\n",
      "```\n",
      "\n",
      "Alternatively, you can also enable BF16 mixed precision training on Ampere or newer hardware by setting the `bf16` flag to `True`:\n",
      "\n",
      "```python\n",
      "accelerator = Accelerator(bf16=True)\n",
      "```\n",
      "-----------------------------\n",
      "44\n",
      "huggingface/course/blob/main/chapters/en/chapter2/4.mdx\n",
      "What is the purpose of tokenizers in the NLP pipeline?\n",
      "\n",
      "To translate text into data that can be processed by the model.\n",
      "To translate text into data that can be processed by the model.\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/chapters/en/chapter2/4.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx', 'huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx']\n",
      "FrameworkSwitchCourse {fw} />\n",
      "\n",
      "# Tokenizers[[tokenizers]]\n",
      "\n",
      "{#if fw === 'pt'}\n",
      "\n",
      "<CourseFloatingBanner chapter={2}\n",
      "  classNames=\"absolute z-10 right-0 top-0\"\n",
      "  notebooks={[\n",
      "    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb\"},\n",
      "    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb\"},\n",
      "]} />\n",
      "\n",
      "{:else}\n",
      "\n",
      "<CourseFloatingBanner chapter={2}\n",
      "  classNames=\"absolute z-10 right-0 top-0\"\n",
      "  notebooks={[\n",
      "    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\n",
      "    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\n",
      "]} />\n",
      "\n",
      "{/if}\n",
      "\n",
      "<Youtube id=\"VFp38yj8h3A\"/>\n",
      "\n",
      "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \n",
      "\n",
      "In NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n",
      "\n",
      "```\n",
      "Jim Henson was a puppeteer\n",
      "```\n",
      "\n",
      "However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That's what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation.\n",
      "\n",
      "Let's take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.\n",
      "\n",
      "## Word-based[[word-based]]\n",
      "\n",
      "<Youtube id=\"nhJxYji1aho\"/>\n",
      "\n",
      "## Pre-tokenizers\n",
      "\n",
      "The `PreTokenizer` takes care of splitting the input according to a set\n",
      "of rules. This pre-processing lets you ensure that the underlying\n",
      "`Model` does not build tokens across multiple \"splits\". For example if\n",
      "you don't want to have whitespaces inside a token, then you can have a\n",
      "`PreTokenizer` that splits on these whitespaces.\n",
      "\n",
      "You can easily combine multiple `PreTokenizer` together using a\n",
      "`Sequence` (see below). The `PreTokenizer` is also allowed to modify the\n",
      "string, just like a `Normalizer` does. This is necessary to allow some\n",
      "complicated algorithms that require to split before normalizing (e.g.\n",
      "the ByteLevel)\n",
      "\n",
      "Components\n",
      "\n",
      "When building a Tokenizer, you can attach various types of components to\n",
      "this Tokenizer in order to customize its behavior. This page lists most\n",
      "provided components.\n",
      "\n",
      "## Normalizers\n",
      "\n",
      "A `Normalizer` is in charge of pre-processing the input string in order\n",
      "to normalize it as relevant for a given use case. Some common examples\n",
      "of normalization are the Unicode normalization algorithms (NFD, NFKD,\n",
      "NFC & NFKC), lowercasing etc... The specificity of `tokenizers` is that\n",
      "we keep track of the alignment while normalizing. This is essential to\n",
      "allow mapping from the generated tokens back to the input text.\n",
      "\n",
      "The `Normalizer` is optional.\n",
      "\n",
      "| Digits | Splits the numbers from any other characters. | Input: `\"Hello123there\"` <br>  Output: ``\"Hello\", \"123\", \"there\"``  |\n",
      "| Split | Versatile pre-tokenizer that splits on provided pattern and according to provided behavior. The pattern can be inverted if necessary. <ul> <li>pattern should be either a custom string or regexp.</li> <li>behavior should be one of: <ul><li>removed</li><li>isolated</li><li>merged_with_previous</li><li>merged_with_next</li><li>contiguous</li></ul></li> <li>invert should be a boolean flag.</li> </ul> | Example with pattern = ` `, behavior = `\"isolated\"`, invert = `False`: <br> Input: `\"Hello, how are you?\"` <br> Output: `\"Hello,\", \" \", \"how\", \" \", \"are\", \" \", \"you?\"` |\n",
      "| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the given order | `Sequence([Punctuation(), WhitespaceSplit()])` |\n",
      "</python>\n",
      "<rust>\n",
      "| Name | Description | Example |\n",
      "| :--- | :--- | :--- |\n",
      "| ByteLevel | Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties: <ul> <li>Since it maps on bytes, a tokenizer using this only requires **256** characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.</li> <li>A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! 🎉🎉)</li> <li>For non ascii characters, it gets completely unreadable, but it works nonetheless!</li> </ul> | Input: `\"Hello my friend, how are you?\"` <br> Ouput: `\"Hello\", \"Ġmy\", Ġfriend\", \",\", \"Ġhow\", \"Ġare\", \"Ġyou\", \"?\"`  |\n",
      "| Whitespace | Splits on word boundaries (using the following regular expression: `\\w+&#124;[^\\w\\s]+` | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there\", \"!\"`  |\n",
      "\n",
      "| Digits | Splits the numbers from any other characters. | Input: `\"Hello123there\"` <br>  Output: ``\"Hello\", \"123\", \"there\"``  |\n",
      "| Split | Versatile pre-tokenizer that splits on provided pattern and according to provided behavior. The pattern can be inverted if necessary. <ul> <li>pattern should be either a custom string or regexp.</li> <li>behavior should be one of: <ul><li>removed</li><li>isolated</li><li>mergedWithPrevious</li><li>mergedWithNext</li><li>contiguous</li></ul></li> <li>invert should be a boolean flag.</li> </ul> | Example with pattern = ` `, behavior = `\"isolated\"`, invert = `False`: <br> Input: `\"Hello, how are you?\"` <br> Output: `\"Hello,\", \" \", \"how\", \" \", \"are\", \" \", \"you?\"` |\n",
      "| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the given order | |\n",
      "</node>\n",
      "</tokenizerslangcontent>\n",
      "-----------------LLm ANSWER------------\n",
      "According to the context, the purpose of tokenizers in the NLP pipeline is to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert the raw text to numbers.\n",
      "-----------------------------\n",
      "45\n",
      "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md\n",
      "What is the purpose of the Safety Checker in the Diffusers library?\n",
      "\n",
      "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n",
      "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md', 'huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Philosophy\n",
      "\n",
      "🧨 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\n",
      "Its purpose is to serve as a **modular toolbox** for both inference and training.\n",
      "\n",
      "We aim at building a library that stands the test of time and therefore take API design very seriously.\n",
      "\n",
      "In a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch's Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let's go over the most important ones:\n",
      "\n",
      "## Usability over Performance\n",
      "\n",
      "- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n",
      "\n",
      "- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\n",
      "\n",
      "- **Encouraging safety in deployment**\n",
      "\n",
      "  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\n",
      "\n",
      "  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n",
      "\n",
      "- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository’s authors to have more control over its use.\n",
      "\n",
      "- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# 🧨 Diffusers’ Ethical Guidelines\n",
      "\n",
      "## Preamble\n",
      "\n",
      "[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n",
      "\n",
      "Given its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library.\n",
      "\n",
      "The risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\n",
      "We will keep tracking risks and adapt the following guidelines based on the community's responsiveness and valuable feedback.\n",
      "\n",
      "\n",
      "## Scope\n",
      "\n",
      "The Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\n",
      "\n",
      "\n",
      "## Ethical guidelines\n",
      "\n",
      "## Ethical guidelines\n",
      "\n",
      "The following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\n",
      "\n",
      "- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\n",
      "\n",
      "- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\n",
      "\n",
      "- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project’s goals lean and coherent.\n",
      "\n",
      "- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\n",
      "\n",
      "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n",
      "\n",
      "- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n",
      "\n",
      "\n",
      "## Examples of implementations: Safety features and Mechanisms\n",
      "\n",
      "The team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n",
      "\n",
      "- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Overview\n",
      "\n",
      "Welcome to 🧨 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is meant to be used.\n",
      "\n",
      "You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want.\n",
      "\n",
      "After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\n",
      "\n",
      "Feel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\n",
      "\n",
      "Let's start diffusing! 🧨\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, the purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker. This is done to mitigate the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration.\n",
      "-----------------------------\n",
      "46\n",
      "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md\n",
      "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n",
      "\n",
      "HfApi\n",
      "HfApi\n",
      "-----------------Context------------\n",
      "['huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md']\n",
      "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Interact with Discussions and Pull Requests \n",
      "\n",
      "The `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\n",
      "Visit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\n",
      "for a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n",
      "\n",
      "## Retrieve Discussions and Pull Requests from the Hub\n",
      "\n",
      "The `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import get_repo_discussions\n",
      ">>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\n",
      "...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")\n",
      "\n",
      "# 11 - Add Flax weights, pr: True\n",
      "# 10 - Update README.md, pr: True\n",
      "# 9 - Training languages in the model card, pr: True\n",
      "# 8 - Update tokenizer_config.json, pr: True\n",
      "# 7 - Slurm training script, pr: False\n",
      "[...]\n",
      "```\n",
      "\n",
      "`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import get_repo_discussions\n",
      ">>> for discussion in get_repo_discussions(\n",
      "...    repo_id=\"bigscience/bloom\",\n",
      "...    author=\"ArthurZ\",\n",
      "...    discussion_type=\"pull_request\",\n",
      "...    discussion_status=\"open\",\n",
      "... ):\n",
      "...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")\n",
      "\n",
      "# 19 - Add Flax weights by ArthurZ, pr: True\n",
      "```\n",
      "\n",
      "`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\n",
      "[`Discussion`] objects. To get all the Discussions in a single list, run:\n",
      "\n",
      "In case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\n",
      "Pull Request are listed in [`DiscussionWithDetails.events`].\n",
      "\n",
      "\n",
      "## Create and edit a Discussion or Pull Request programmatically\n",
      "\n",
      "The [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests.\n",
      "You will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\n",
      "or Pull Requests.\n",
      "\n",
      "The simplest way to propose changes on a repo on the Hub is via the [`create_commit`] API: just \n",
      "set the `create_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create_commit`]:\n",
      "\n",
      "    * [`upload_file`]\n",
      "    * [`upload_folder`]\n",
      "    * [`delete_file`]\n",
      "    * [`delete_folder`]\n",
      "    * [`metadata_update`]\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import metadata_update\n",
      "\n",
      ">>> metadata_update(\n",
      "...     repo_id=\"username/repo_name\",\n",
      "...     metadata={\"tags\": [\"computer-vision\", \"awesome-model\"]},\n",
      "...     create_pr=True,\n",
      "... )\n",
      "```\n",
      "\n",
      "You can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to create a Discussion (respectively a Pull Request) on a repo.\n",
      "Opening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode.\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import create_discussion, create_pull_request\n",
      "\n",
      ">>> create_discussion(\n",
      "...     repo_id=\"username/repo-name\",\n",
      "...     title=\"Hi from the huggingface_hub library!\",\n",
      "...     token=\"<insert your access token here>\",\n",
      "... )\n",
      "DiscussionWithDetails(...)\n",
      "\n",
      ">>> create_pull_request(\n",
      "...     repo_id=\"username/repo-name\",\n",
      "...     title=\"Hi from the huggingface_hub library!\",\n",
      "...     token=\"<insert your access token here>\",\n",
      "... )\n",
      "DiscussionWithDetails(..., is_pull_request=True)\n",
      "```\n",
      "\n",
      "Managing Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\n",
      "\n",
      "# 19 - Add Flax weights by ArthurZ, pr: True\n",
      "```\n",
      "\n",
      "`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\n",
      "[`Discussion`] objects. To get all the Discussions in a single list, run:\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import get_repo_discussions\n",
      ">>> discussions_list = list(get_repo_discussions(repo_id=\"bert-base-uncased\"))\n",
      "```\n",
      "\n",
      "The [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\n",
      "Discussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\n",
      "\n",
      "```python\n",
      ">>> from huggingface_hub import get_discussion_details\n",
      "\n",
      ">>> get_discussion_details(\n",
      "...     repo_id=\"bigscience/bloom-1b3\",\n",
      "...     discussion_num=2\n",
      "... )\n",
      "DiscussionWithDetails(\n",
      "    num=2,\n",
      "    author='cakiki',\n",
      "    title='Update VRAM memory for the V100s',\n",
      "    status='open',\n",
      "    is_pull_request=True,\n",
      "    events=[\n",
      "        DiscussionComment(type='comment', author='cakiki', ...),\n",
      "        DiscussionCommit(type='commit', author='cakiki', summary='Update VRAM memory for the V100s', oid='1256f9d9a33fa8887e1c1bf0e09b4713da96773a', ...),\n",
      "    ],\n",
      "    conflicting_files=[],\n",
      "    target_branch='refs/heads/main',\n",
      "    merge_commit_oid=None,\n",
      "    diff='diff --git a/README.md b/README.md\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\n--- a/README.md\\n+++ b/README.md\\n@@ -132,7 +132,7 [...]',\n",
      ")\n",
      "```\n",
      "\n",
      "[`HfApi.get_discussion_details`] returns a [`DiscussionWithDetails`] object, which is a subclass of [`Discussion`]\n",
      "with more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes,\n",
      "and renames of the Discussion via [`DiscussionWithDetails.events`].\n",
      "\n",
      "In case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\n",
      "Pull Request are listed in [`DiscussionWithDetails.events`].\n",
      "\n",
      "Managing Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\n",
      "\n",
      "    * [`comment_discussion`] to add comments\n",
      "    * [`edit_discussion_comment`] to edit comments\n",
      "    * [`rename_discussion`] to rename a Discussion or Pull Request \n",
      "    * [`change_discussion_status`] to open or close a Discussion / Pull Request \n",
      "    * [`merge_pull_request`] to merge a Pull Request \n",
      "\n",
      "\n",
      "Visit the [`HfApi`] documentation page for an exhaustive reference of all available methods.\n",
      "\n",
      "## Push changes to a Pull Request\n",
      "\n",
      "*Coming soon !*\n",
      "\n",
      "## See also\n",
      "\n",
      "For a more detailed reference, visit the [Discussions and Pull Requests](../package_reference/community) and the [hf_api](../package_reference/hf_api) documentation page.\n",
      "\n",
      "### Clone\n",
      "\n",
      "The `clone_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local_dir` argument:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import Repository\n",
      ">>> repo = Repository(local_dir=\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\n",
      "```\n",
      "\n",
      "`clone_from` can also clone a repository using a URL:\n",
      "\n",
      "```py\n",
      ">>> repo = Repository(local_dir=\"huggingface-hub\", clone_from=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60\")\n",
      "```\n",
      "\n",
      "You can combine the `clone_from` parameter with [`create_repo`] to create and clone a repository:\n",
      "\n",
      "```py\n",
      ">>> repo_url = create_repo(repo_id=\"repo_name\")\n",
      ">>> repo = Repository(local_dir=\"repo_local_path\", clone_from=repo_url)\n",
      "```\n",
      "\n",
      "You can also configure a Git username and email to a cloned repository by specifying the `git_user` and `git_email` parameters when you clone a repository. When users commit to that repository, Git will be aware of the commit author.\n",
      "\n",
      "```py\n",
      ">>> repo = Repository(\n",
      "...   \"my-dataset\",\n",
      "...   clone_from=\"<user>/<dataset_id>\",\n",
      "...   token=True,\n",
      "...   repo_type=\"dataset\",\n",
      "...   git_user=\"MyName\",\n",
      "...   git_email=\"me@cool.mail\"\n",
      "... )\n",
      "```\n",
      "\n",
      "### Branch\n",
      "\n",
      "Branches are important for collaboration and experimentation without impacting your current files and code. Switch between branches with [`~Repository.git_checkout`]. For example, if you want to switch from `branch1` to `branch2`:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import Repository\n",
      ">>> repo = Repository(local_dir=\"huggingface-hub\", clone_from=\"<user>/<dataset_id>\", revision='branch1')\n",
      ">>> repo.git_checkout(\"branch2\")\n",
      "```\n",
      "\n",
      "### Pull\n",
      "\n",
      "[`~Repository.git_pull`] allows you to update a current local branch with changes from a remote repository:\n",
      "\n",
      "```py\n",
      ">>> from huggingface_hub import Repository\n",
      ">>> repo.git_pull()\n",
      "```\n",
      "\n",
      "Set `rebase=True` if you want your local commits to occur after your branch is updated with the new commits from the remote:\n",
      "\n",
      "```py\n",
      ">>> repo.git_pull(rebase=True)\n",
      "```\n",
      "-----------------LLm ANSWER------------\n",
      "The Python class that allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub is `HfApi`.\n",
      "-----------------------------\n",
      "47\n",
      "huggingface/blog/blob/main/skops.md\n",
      "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n",
      "\n",
      "Skops\n",
      "Skops\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/skops.md', 'huggingface/blog/blob/main/spacy.md', 'huggingface/blog/blob/main/skops.md', 'huggingface/blog/blob/main/skops.md', 'huggingface/blog/blob/main/spacy.md']\n",
      "--\n",
      "title: \"Introducing Skops\"\n",
      "thumbnail: /blog/assets/94_skops/introducing_skops.png\n",
      "authors:\n",
      "- user: merve\n",
      "- user: adrin\n",
      "- user: BenjaminB\n",
      "---\n",
      "\n",
      "# Introducing Skops\n",
      "\n",
      "\n",
      "## Introducing Skops\n",
      "\n",
      "At Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.\n",
      "\n",
      "Let's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production.\n",
      "\n",
      "```python\n",
      "# let's import the libraries first\n",
      "import sklearn\n",
      "from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load the data and split\n",
      "X, y = load_breast_cancer(as_frame=True, return_X_y=True)\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.3, random_state=42\n",
      ")\n",
      "\n",
      "# Train the model\n",
      "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "You can use any model filename and serialization method, like `pickle` or `joblib`. At the moment, our backend uses `joblib` to load the model. `hub_utils.init` creates a local folder containing the model in the given path, and the configuration file containing the specifications of the environment the model is trained in. The data and the task passed to the `init` will help Hugging Face Hub enable the inference widget on the model page as well as discoverability features to find the model.\n",
      "\n",
      "```python\n",
      "from skops import hub_utils\n",
      "import pickle\n",
      "\n",
      "# let's save the model\n",
      "model_path = \"example.pkl\"\n",
      "local_repo = \"my-awesome-model\"\n",
      "with open(model_path, mode=\"bw\") as f:\n",
      "    pickle.dump(model, file=f)\n",
      "\n",
      "Try it out and share your models with the community!\n",
      "\n",
      "## Would you like to integrate your library to the Hub?\n",
      "\n",
      "This integration is possible thanks to the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you!\n",
      "\n",
      "```python\n",
      "download_repo = \"downloaded-model\"\n",
      "hub_utils.download(repo_id=repo_id, dst=download_repo)\n",
      "```\n",
      "\n",
      "The inference widget is enabled to make predictions in the repository.\n",
      "\n",
      "![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n",
      "\n",
      "If the requirements of your project have changed, you can use `update_env` to update the environment.\n",
      "\n",
      "```python\n",
      "hub_utils.update_env(path=local_repo, requirements=[\"scikit-learn\"])\n",
      "```\n",
      "\n",
      "You can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example).\n",
      "We have prepared two examples to show how to save your models and use model card utilities. You can find them in the resources section below.\n",
      "\n",
      "\n",
      "## Resources\n",
      "- [Model card tutorial](https://skops.readthedocs.io/en/latest/auto_examples/plot_model_card.html)\n",
      "- [hub_utils tutorial](https://skops.readthedocs.io/en/latest/auto_examples/plot_hf_hub.html)\n",
      "- [skops documentation](https://skops.readthedocs.io/en/latest/modules/classes.html)\n",
      "\n",
      "We can also add any plot of our choice to the card using `add_plot` like below.\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "from pathlib import Path\n",
      "# we will create a confusion matrix\n",
      "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
      "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
      "disp.plot()\n",
      "\n",
      "# save the plot\n",
      "plt.savefig(Path(local_repo) / \"confusion_matrix.png\")\n",
      "\n",
      "# the plot will be written to the model card under the name confusion_matrix\n",
      "# we pass the path of the plot itself\n",
      "model_card.add_plot(confusion_matrix=\"confusion_matrix.png\")\n",
      "```\n",
      "\n",
      "Let's save the model card in the local repository. The file name here should be `README.md` since it is what Hugging Face Hub expects.\n",
      "```python\n",
      "model_card.save(Path(local_repo) / \"README.md\")\n",
      "```\n",
      "\n",
      "We can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either  `notebook_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI.\n",
      "\n",
      "```python\n",
      "# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\n",
      "repo_id = \"skops-user/my-awesome-model\"\n",
      "hub_utils.push(\n",
      "    repo_id=repo_id,\n",
      "    source=local_repo,\n",
      "    token=token,\n",
      "    commit_message=\"pushing files to the repo from the example!\",\n",
      "    create_remote=True,\n",
      ")\n",
      "```\n",
      "\n",
      "Once we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n",
      "\n",
      "```python\n",
      "download_repo = \"downloaded-model\"\n",
      "hub_utils.download(repo_id=repo_id, dst=download_repo)\n",
      "```\n",
      "\n",
      "The inference widget is enabled to make predictions in the repository.\n",
      "\n",
      "![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n",
      "\n",
      "--\n",
      "title: \"Welcome spaCy to the Hugging Face Hub\"\n",
      "thumbnail: /blog/assets/23_spacy/thumbnail.png\n",
      "\n",
      "authors:\n",
      "- user: osanseviero\n",
      "- user: ines\n",
      "---\n",
      "\n",
      "# Welcome spaCy to the Hugging Face Hub\n",
      "\n",
      "\n",
      "[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\n",
      "\n",
      "Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n",
      "\n",
      "### Finding models\n",
      "\n",
      "Over 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\n",
      "\n",
      "\n",
      "### Widgets\n",
      "\n",
      "This integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS.\n",
      "-----------------LLm ANSWER------------\n",
      "The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops.\n",
      "-----------------------------\n",
      "48\n",
      "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md\n",
      "What is the purpose of Textual Inversion?\n",
      "\n",
      "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n",
      "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md', 'huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md']\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Textual Inversion\n",
      "\n",
      "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\n",
      "\n",
      "[`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading_adapters#textual-inversion) loading guide.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "## TextualInversionLoaderMixin\n",
      "\n",
      "[[autodoc]] loaders.textual_inversion.TextualInversionLoaderMixin\n",
      "\n",
      "## Do distillation for quantization\n",
      "\n",
      "Distillation for quantization is a method that combines [intermediate layer knowledge distillation](https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md#intermediate-layer-knowledge-distillation) and [quantization aware training](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md#quantization-aware-training) in the same training process to improve the performance of the quantized model. Provided a FP32 model, the distillation for quantization approach will take this model itself as the teacher model and transfer the knowledges of the specified layers to the student model, i.e. quantized version of the FP32 model, during the quantization aware training process.\n",
      "\n",
      "Once you have the FP32 Textual Inversion model, the following command will take the FP32 Textual Inversion model as input to do distillation for quantization and generate the INT8 Textual Inversion model.\n",
      "\n",
      "```bash\n",
      "export FP32_MODEL_NAME=\"./dicoo_model\"\n",
      "export DATA_DIR=\"./dicoo\"\n",
      "\n",
      "accelerate launch textual_inversion.py \\\n",
      "  --pretrained_model_name_or_path=$FP32_MODEL_NAME \\\n",
      "  --train_data_dir=$DATA_DIR \\\n",
      "  --use_ema --learnable_property=\"object\" \\\n",
      "  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n",
      "  --resolution=512 \\\n",
      "  --train_batch_size=1 \\\n",
      "  --gradient_accumulation_steps=4 \\\n",
      "  --max_train_steps=300 \\\n",
      "  --learning_rate=5.0e-04 --max_grad_norm=3 \\\n",
      "  --lr_scheduler=\"constant\" \\\n",
      "  --lr_warmup_steps=0 \\\n",
      "  --output_dir=\"int8_model\" \\\n",
      "  --do_quantization --do_distillation --verify_loading\n",
      "```\n",
      "\n",
      "After the distillation for quantization process, the quantized UNet would be 4 times smaller (3279MB -> 827MB).\n",
      "\n",
      "## Inference\n",
      "\n",
      "Once you have trained a INT8 model with the above command, the inference can be done simply using the `text2images.py` script. Make sure to include the `placeholder_token` in your prompt.\n",
      "\n",
      "```bash\n",
      "export INT8_MODEL_NAME=\"./int8_model\"\n",
      "\n",
      "GPT-2's pretraining objective is based entirely on [causal language modeling](glossary#causal-language-modeling), predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text.\n",
      "\n",
      "Ready to try your hand at text generation? Check out our complete [causal language modeling guide](tasks/language_modeling#causal-language-modeling) to learn how to finetune DistilGPT-2 and use it for inference!\n",
      "\n",
      "<Tip>\n",
      "\n",
      "For more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n",
      "\n",
      "</Tip>\n",
      "\n",
      "### Summarization\n",
      "\n",
      "Encoder-decoder models like [BART](model_doc/bart) and [T5](model_doc/t5) are designed for the sequence-to-sequence pattern of a summarization task. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png\"/>\n",
      "</div>\n",
      "\n",
      "1. BART's encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. The *text infilling* corruption strategy works the best though. In text infilling, a number of text spans are replaced with a **single** [`mask`] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn't add a final feedforward network at the end to predict a word.\n",
      "\n",
      "## Simple over easy\n",
      "\n",
      "As PyTorch states, **explicit is better than implicit** and **simple is better than complex**. This design philosophy is reflected in multiple parts of the library:\n",
      "- We follow PyTorch's API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management.\n",
      "- Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible.\n",
      "- Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers.\n",
      "- Separately trained components of the diffusion pipeline, *e.g.* the text encoder, the unet, and the variational autoencoder, each have their own model class. This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\n",
      "is very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline.\n",
      "\n",
      "## Tweakable, contributor-friendly over abstraction\n",
      "\n",
      "Ready to try your hand at translation? Check out our complete [translation guide](tasks/summarization) to learn how to finetune T5 and use it for inference!\n",
      "\n",
      "<Tip>\n",
      "\n",
      "For more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n",
      "\n",
      "</Tip>\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided context, the purpose of Textual Inversion is to learn new text embeddings from a few example images, allowing for personalizing models and creating small (a few KBs) files that can be loaded into the text encoder.\n",
      "-----------------------------\n",
      "49\n",
      "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md\n",
      "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n",
      "\n",
      "64\n",
      "64\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md']\n",
      "[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) \n",
      "define the multiplier based on the dtype and the hardware. For instance, for fp16 data type a multiple of 8 is recommended, unless \n",
      "it's an A100 GPU, in which case use multiples of 64.\n",
      "\n",
      "For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). \n",
      "This is where tiling happens and the right multiplier can have a significant speedup.\n",
      "\n",
      "## Gradient Accumulation\n",
      "\n",
      "The **gradient accumulation** method aims to calculate gradients in smaller increments instead of computing them for the \n",
      "entire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward \n",
      "and backward passes through the model and accumulating the gradients during the process. Once a sufficient number of \n",
      "gradients have been accumulated, the model's optimization step is executed. By employing gradient accumulation, it \n",
      "becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU's memory capacity. \n",
      "However, it is important to note that the additional forward and backward passes introduced by gradient accumulation can \n",
      "slow down the training process.\n",
      "\n",
      "You can enable gradient accumulation by adding the `gradient_accumulation_steps` argument to  [`TrainingArguments`]: \n",
      "\n",
      "```py\n",
      "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n",
      "```\n",
      "\n",
      "In the above example, your effective batch size becomes 4. \n",
      "\n",
      "Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example \n",
      "[further down in this guide](#using-accelerate).\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Note: when using mixed precision with a small model and a large batch size, there will be some memory savings but with a \n",
      "large model and a small batch size, the memory use will be larger.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "You can combine the above methods to get a cumulative effect. These techniques are available to you whether you are \n",
      "training your model with [`Trainer`] or writing a pure PyTorch loop, in which case you can [configure these optimizations \n",
      "with 🤗 Accelerate](#using-accelerate).\n",
      "\n",
      "If these methods do not result in sufficient gains, you can explore the following options: \n",
      "* [Look into building your own custom Docker container with efficient softare prebuilds](#efficient-software-prebuilds)\n",
      "* [Consider a model that uses Mixture of Experts (MoE)](#mixture-of-experts)\n",
      "* [Convert your model to BetterTransformer to leverage PyTorch native attention](#using-pytorch-native-attention)\n",
      "\n",
      "Finally, if all of the above is still not enough, even after switching to a server-grade GPU like A100, consider moving \n",
      "to a multi-GPU setup. All these approaches are still valid in a multi-GPU setup, plus you can leverage additional parallelism \n",
      "techniques outlined in the [multi-GPU section](perf_train_gpu_many). \n",
      "\n",
      "## Batch size choice\n",
      "\n",
      "To achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and \n",
      "input/output neuron counts that are of size 2^N. Often it's a multiple of 8, but it can be \n",
      "higher depending on the hardware being used and the model's dtype.\n",
      "\n",
      "For reference, check out NVIDIA's recommendation for [input/output neuron counts](\n",
      "https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) and \n",
      "[batch size](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size) for \n",
      "fully connected layers (which are involved in GEMMs (General Matrix Multiplications)).\n",
      "\n",
      "Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures \n",
      "(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check \n",
      "out the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about \n",
      "the differences between these data types.\n",
      "\n",
      "### fp16\n",
      "\n",
      "The main advantage of mixed precision training comes from saving the activations in half precision (fp16). \n",
      "Although the gradients are also computed in half precision they are converted back to full precision for the optimization \n",
      "step so no memory is saved here. \n",
      "While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes.\n",
      "This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\n",
      "\n",
      "To enable mixed precision training, set the `fp16` flag to `True`:\n",
      "\n",
      "```py\n",
      "training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n",
      "```\n",
      "\n",
      "If you prefer to use 🤗 Accelerate, find the 🤗 Accelerate example [further in this guide](#using-accelerate). \n",
      "\n",
      "### BF16\n",
      "\n",
      "If you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \n",
      "bf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \n",
      "is `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \n",
      "is about the same as fp32 - because both have 8-bits used for the numerical range.\n",
      "\n",
      "You can enable BF16 in the 🤗 Trainer with:\n",
      "\n",
      "```python\n",
      "training_args = TrainingArguments(bf16=True, **default_args)\n",
      "```\n",
      "\n",
      "### TF32\n",
      "\n",
      "You can enable BF16 in the 🤗 Trainer with:\n",
      "\n",
      "```python\n",
      "training_args = TrainingArguments(bf16=True, **default_args)\n",
      "```\n",
      "\n",
      "### TF32\n",
      "\n",
      "The Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead \n",
      "of 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total. It's \"magical\" in the sense that \n",
      "you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput \n",
      "improvement. All you need to do is to add the following to your code:\n",
      "\n",
      "```\n",
      "import torch\n",
      "torch.backends.cuda.matmul.allow_tf32 = True\n",
      "torch.backends.cudnn.allow_tf32 = True\n",
      "```\n",
      "\n",
      "CUDA will automatically switch to using tf32 instead of fp32 where possible, assuming that the used GPU is from the Ampere series.\n",
      "\n",
      "According to [NVIDIA research](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/), the \n",
      "majority of machine learning training workloads show the same perplexity and convergence with tf32 training as with fp32. \n",
      "If you're already using fp16 or bf16 mixed precision it may help with the throughput as well.\n",
      "\n",
      "You can enable this mode in the 🤗 Trainer:\n",
      "\n",
      "```python\n",
      "TrainingArguments(tf32=True, **default_args)\n",
      "```\n",
      "\n",
      "<Tip>\n",
      "\n",
      "tf32 can't be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an internal CUDA data type. You need `torch>=1.7` to use tf32 data types.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "For additional information on tf32 vs other precisions, please refer to the following benchmarks: \n",
      "[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803) and\n",
      "[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189).\n",
      "\n",
      "## Flash Attention 2\n",
      "\n",
      "You can speedup the training throughput by using Flash Attention 2 integration in transformers. Check out the appropriate section in the [single GPU section](./perf_infer_gpu_one#Flash-Attention-2) to learn more about how to load a model with Flash Attention 2 modules.\n",
      "\n",
      "In the above example, your effective batch size becomes 4. \n",
      "\n",
      "Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example \n",
      "[further down in this guide](#using-accelerate).\n",
      "\n",
      "While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can \n",
      "result in a more pronounced training slowdown. Consider the following example. Let's say, the `per_device_train_batch_size=4` \n",
      "without gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the \n",
      "`per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` \n",
      "and set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of \n",
      "the available GPU resources.\n",
      "\n",
      "For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\n",
      "and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\n",
      "\n",
      "## Gradient Checkpointing\n",
      "\n",
      "Some large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. \n",
      "This is because there are other components that also require memory storage.\n",
      "\n",
      "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in \n",
      "significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed \n",
      "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
      "-----------------LLm ANSWER------------\n",
      "According to the context, the recommended multiple of batch size for fp16 data type on an A100 GPU is 64.\n",
      "-----------------------------\n",
      "50\n",
      "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md\n",
      "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n",
      "\n",
      "Run `gradio run.py` in the terminal.\n",
      "Run `gradio run.py` in the terminal.\n",
      "-----------------Context------------\n",
      "['gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md', 'gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md', 'gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md', 'gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md', 'huggingface/course/blob/main/chapters/en/chapter9/7.mdx']\n",
      "Developing Faster with Auto-Reloading\n",
      "\n",
      "**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n",
      "\n",
      "This guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n",
      "\n",
      "## Why Auto-Reloading?\n",
      "\n",
      "When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n",
      "\n",
      "To make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n",
      "\n",
      "This short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n",
      "\n",
      "## Python IDE Reload 🔥\n",
      "\n",
      "If you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    gr.Markdown(\"# Greetings from Gradio!\")\n",
      "    inp = gr.Textbox(placeholder=\"What is your name?\")\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n",
      "               inputs=inp,\n",
      "               outputs=out)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    demo.launch()\n",
      "```\n",
      "\n",
      "The problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n",
      "\n",
      "Instead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n",
      "\n",
      "In the terminal, run `gradio run.py`. That's it!\n",
      "\n",
      "Now, you'll see that after you'll see something like this:\n",
      "\n",
      "```py\n",
      "%%blocks\n",
      "\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    gr.Markdown(f\"# Greetings {args.name}!\")\n",
      "    inp = gr.Textbox()\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n",
      "```\n",
      "\n",
      "Notice that:\n",
      "\n",
      "- You do not need to launch your demo — Gradio does that for you automatically!\n",
      "\n",
      "- Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server. This means you'll see your changes _much, much faster_ than if you were rerunning the cell normally.\n",
      "\n",
      "Here's what it looks like in a jupyter notebook:\n",
      "\n",
      "![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter_reload.gif)\n",
      "\n",
      "🪄 This works in colab notebooks too! [Here's a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code!\n",
      "\n",
      "The Notebook Magic is now the author's preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio.\n",
      "\n",
      "---\n",
      "\n",
      "## Next Steps\n",
      "\n",
      "Now that you know how to develop quickly using Gradio, start building your own!\n",
      "\n",
      "If you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) 🤗\n",
      "\n",
      "Instead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n",
      "\n",
      "In the terminal, run `gradio run.py`. That's it!\n",
      "\n",
      "Now, you'll see that after you'll see something like this:\n",
      "\n",
      "```bash\n",
      "Watching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n",
      "\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "```\n",
      "\n",
      "The important part here is the line that says `Watching...` What's happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically 🥳\n",
      "\n",
      "⚠️ Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.\n",
      "\n",
      "There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks() as my_demo:\n",
      "    gr.Markdown(\"# Greetings from Gradio!\")\n",
      "    inp = gr.Textbox(placeholder=\"What is your name?\")\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n",
      "               inputs=inp,\n",
      "               outputs=out)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    my_demo.launch()\n",
      "```\n",
      "\n",
      "Then you would launch it in reload mode like this: `gradio run.py my_demo`.\n",
      "\n",
      "By default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n",
      "\n",
      "By default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:\n",
      "\n",
      "1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`\n",
      "2. Confirm that your code editor has identified that encoding format. \n",
      "3. Run like this: `gradio run.py --encoding cp1252`\n",
      "\n",
      "🔥 If your application accepts command line arguments, you can pass them in as well. Here's an example:\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "import argparse\n",
      "\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\"--name\", type=str, default=\"User\")\n",
      "args, unknown = parser.parse_known_args()\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    gr.Markdown(f\"# Greetings {args.name}!\")\n",
      "    inp = gr.Textbox()\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    demo.launch()\n",
      "```\n",
      "\n",
      "Which you could run like this: `gradio run.py --name Gretel`\n",
      "\n",
      "As a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) ✅\n",
      "\n",
      "## Jupyter Notebook Magic 🔮\n",
      "\n",
      "What about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!\n",
      "\n",
      "We've developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n",
      "\n",
      "`%load_ext gradio`\n",
      "\n",
      "Then, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:\n",
      "\n",
      "```py\n",
      "%%blocks\n",
      "\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    gr.Markdown(f\"# Greetings {args.name}!\")\n",
      "    inp = gr.Textbox()\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\n",
      "```\n",
      "\n",
      "Notice that:\n",
      "\n",
      "We will explore all of these concepts below.\n",
      "\n",
      "### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\n",
      "\n",
      "After you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook.\n",
      "\n",
      "```py\n",
      "import gradio as gr\n",
      "\n",
      "\n",
      "def flip_text(x):\n",
      "    return x[::-1]\n",
      "\n",
      "\n",
      "demo = gr.Blocks()\n",
      "\n",
      "with demo:\n",
      "    gr.Markdown(\n",
      "        \"\"\"\n",
      "    # Flip Text!\n",
      "    Start typing below to see the output.\n",
      "    \"\"\"\n",
      "    )\n",
      "    input = gr.Textbox(placeholder=\"Flip this text\")\n",
      "    output = gr.Textbox()\n",
      "\n",
      "    input.change(fn=flip_text, inputs=input, outputs=output)\n",
      "\n",
      "demo.launch()\n",
      "```\n",
      "\n",
      "<iframe src=\"https://course-demos-flip-text.hf.space\" frameBorder=\"0\" height=\"400\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n",
      "\n",
      "This simple example above introduces 4 concepts that underlie Blocks:\n",
      "\n",
      "1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\n",
      "<Tip>\n",
      "🙋If you're not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that 🤗\n",
      "</Tip>\n",
      "The order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)\n",
      "-----------------LLm ANSWER------------\n",
      "To run a Gradio Blocks app in reload mode using a Python IDE, you can change 1 word: `python` to `gradio`. In the terminal, run `gradio run.py`. This will automatically reload your app whenever you make changes to your code.\n",
      "\n",
      "For example, if your `run.py` file looks like this:\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    gr.Markdown(\"# Greetings from Gradio!\")\n",
      "    inp = gr.Textbox(placeholder=\"What is your name?\")\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n",
      "               inputs=inp,\n",
      "               outputs=out)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    demo.launch()\n",
      "```\n",
      "You would launch it in reload mode like this: `gradio run.py`. This will automatically reload your app whenever you make changes to your code.\n",
      "\n",
      "Note that Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. For example, if your `run.py` file looked like this:\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks() as my_demo:\n",
      "    gr.Markdown(\"# Greetings from Gradio!\")\n",
      "    inp = gr.Textbox(placeholder=\"What is your name?\")\n",
      "    out = gr.Textbox()\n",
      "\n",
      "    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n",
      "               inputs=inp,\n",
      "               outputs=out)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    my_demo.launch()\n",
      "```\n",
      "Then you would launch it in reload mode like this: `gradio run.py my_demo`.\n",
      "-----------------------------\n",
      "51\n",
      "huggingface/blog/blob/main/unity-api.md\n",
      "How can you install the Hugging Face Unity API in your Unity project?\n",
      "\n",
      "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n",
      "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/unity-api.md', 'huggingface/blog/blob/main/unity-api.md', 'huggingface/blog/blob/main/unity-api.md', 'huggingface/blog/blob/main/spacy.md', 'huggingface/blog/blob/main/spacy.md']\n",
      "--\n",
      "title: \"How to Install and Use the Hugging Face Unity API\"\n",
      "thumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\n",
      "authors:\n",
      "- user: dylanebert\n",
      "---\n",
      "\n",
      "# How to Install and Use the Hugging Face Unity API\n",
      "\n",
      "<!-- {authors} --> \n",
      "\n",
      "The [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n",
      "\n",
      "## Installation\n",
      "\n",
      "1. Open your Unity project\n",
      "2. Go to `Window` -> `Package Manager`\n",
      "3. Click `+` and select `Add Package from git URL`\n",
      "4. Enter `https://github.com/huggingface/unity-api.git`\n",
      "5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n",
      "\n",
      "<figure class=\"image text-center\">\n",
      "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n",
      "</figure> \n",
      "\n",
      "6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n",
      "7. Test the API key by clicking `Test API key` in the API Wizard.\n",
      "8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n",
      "9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n",
      "10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.\n",
      "\n",
      "<figure class=\"image text-center\">\n",
      "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\n",
      "</figure>\n",
      "\n",
      "Use the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n",
      "\n",
      "To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n",
      "\n",
      "## Usage Tips\n",
      "\n",
      "1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n",
      "2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!\n",
      "\n",
      "<figure class=\"image text-center\">\n",
      "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\n",
      "</figure> \n",
      "\n",
      "Now that the API is set up, you can make calls from your scripts to the API. Let's look at an example of performing a Sentence Similarity task:\n",
      "\n",
      "```\n",
      "using HuggingFace.API;\n",
      "\n",
      "/* other code */\n",
      "\n",
      "// Make a call to the API\n",
      "void Query() {\n",
      "    string inputText = \"I'm on my way to the forest.\";\n",
      "    string[] candidates = {\n",
      "        \"The player is going to the city\",\n",
      "        \"The player is going to the wilderness\",\n",
      "        \"The player is wandering aimlessly\"\n",
      "    };\n",
      "    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\n",
      "}\n",
      "\n",
      "// If successful, handle the result\n",
      "void OnSuccess(float[] result) {\n",
      "    foreach(float value in result) {\n",
      "        Debug.Log(value);\n",
      "    }\n",
      "}\n",
      "\n",
      "// Otherwise, handle the error\n",
      "void OnError(string error) {\n",
      "    Debug.LogError(error);\n",
      "}\n",
      "\n",
      "/* other code */\n",
      "```\n",
      "\n",
      "## Supported Tasks and Custom Models\n",
      "\n",
      "The Hugging Face Unity API also currently supports the following tasks:\n",
      "\n",
      "- [Conversation](https://huggingface.co/tasks/conversational)\n",
      "- [Text Generation](https://huggingface.co/tasks/text-generation)\n",
      "- [Text to Image](https://huggingface.co/tasks/text-to-image)\n",
      "- [Text Classification](https://huggingface.co/tasks/text-classification)\n",
      "- [Question Answering](https://huggingface.co/tasks/question-answering)\n",
      "- [Translation](https://huggingface.co/tasks/translation)\n",
      "- [Summarization](https://huggingface.co/tasks/summarization)\n",
      "- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\n",
      "\n",
      "Use the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n",
      "\n",
      "To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n",
      "\n",
      "## Usage Tips\n",
      "\n",
      "Try it out and share your models with the community!\n",
      "\n",
      "## Would you like to integrate your library to the Hub?\n",
      "\n",
      "This integration is possible thanks to the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you!\n",
      "\n",
      "### Using existing models\n",
      "\n",
      "All models from the Hub can be directly installed using `pip install`. \n",
      "\n",
      "\n",
      "```bash\n",
      "pip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\n",
      "```\n",
      "\n",
      "```python\n",
      "# Using spacy.load().\n",
      "import spacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "# Importing as module.\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "```\n",
      "\n",
      "When you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!\n",
      "\n",
      "![snippet](assets/23_spacy/snippet.png)\n",
      "![snippet](assets/23_spacy/snippet2.png)\n",
      "\n",
      "You can even make HTTP requests to call the models from the Inference API, which is useful in production settings. Here is an example of a simple request:\n",
      "\n",
      "```bash\n",
      "curl -X POST  --data '{\"inputs\": \"Hello, this is Omar\"}' https://api-inference.huggingface.co/models/spacy/en_core_web_sm\n",
      ">>> [{\"entity_group\":\"PERSON\",\"word\":\"Omar\",\"start\":15,\"end\":19,\"score\":1.0}]\n",
      "```\n",
      "\n",
      "And for larger-scale use cases, you can click \"Deploy > Accelerated Inference\" and see how to do this with Python.\n",
      "\n",
      "\n",
      "### Sharing your models\n",
      "\n",
      "But probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \n",
      "\n",
      "```bash\n",
      "huggingface-cli login\n",
      "python -m spacy package ./en_ner_fashion ./output --build wheel\n",
      "cd ./output/en_ner_fashion-0.0.0/dist\n",
      "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "```\n",
      "\n",
      "In just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n",
      "\n",
      "Try it out and share your models with the community!\n",
      "\n",
      "## Would you like to integrate your library to the Hub?\n",
      "-----------------LLm ANSWER------------\n",
      "To install the Hugging Face Unity API in your Unity project, follow these steps:\n",
      "\n",
      "1. Open your Unity project.\n",
      "2. Go to `Window` -> `Package Manager`.\n",
      "3. Click `+` and select `Add Package from git URL`.\n",
      "4. Enter `https://github.com/huggingface/unity-api.git`.\n",
      "5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`.\n",
      "6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n",
      "7. Test the API key by clicking `Test API key` in the API Wizard.\n",
      "8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n",
      "9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`.\n",
      "10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.\n",
      "-----------------------------\n",
      "52\n",
      "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md\n",
      "What is the pretraining objective of the Wav2Vec2 context network?\n",
      "\n",
      "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n",
      "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md']\n",
      "This model has four main components:\n",
      "\n",
      "1. A *feature encoder* takes the raw audio waveform, normalizes it to zero mean and unit variance, and converts it into a sequence of feature vectors that are each 20ms long.\n",
      "\n",
      "2. Waveforms are continuous by nature, so they can't be divided into separate units like a sequence of text can be split into words. That's why the feature vectors are passed to a *quantization module*, which aims to learn discrete speech units. The speech unit is chosen from a collection of codewords, known as a *codebook* (you can think of this as the vocabulary). From the codebook, the vector or speech unit, that best represents the continuous audio input is chosen and forwarded through the model.\n",
      "\n",
      "3. About half of the feature vectors are randomly masked, and the masked feature vector is fed to a *context network*, which is a Transformer encoder that also adds relative positional embeddings.\n",
      "\n",
      "4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n",
      "\n",
      "Now that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n",
      "\n",
      "### Audio classification\n",
      "\n",
      "To use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder's hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class.\n",
      "\n",
      "Ready to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio_classification) to learn how to finetune Wav2Vec2 and use it for inference!\n",
      "\n",
      "### Automatic speech recognition\n",
      "\n",
      "To use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription.\n",
      "\n",
      "Ready to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](tasks/asr) to learn how to finetune Wav2Vec2 and use it for inference!\n",
      "\n",
      "## Computer vision\n",
      "\n",
      "There are two ways to approach computer vision tasks:\n",
      "\n",
      "1. Split an image into a sequence of patches and process them in parallel with a Transformer.\n",
      "2. Use a modern CNN, like [ConvNeXT](model_doc/convnext), which relies on convolutional layers but adopts modern network designs.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "A third approach mixes Transformers with convolutions (for example, [Convolutional Vision Transformer](model_doc/cvt) or [LeViT](model_doc/levit)). We won't discuss those because they just combine the two approaches we examine here.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we'll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks.\n",
      "\n",
      "### Image classification\n",
      "\n",
      "ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.\n",
      "\n",
      "#### Transformer\n",
      "\n",
      "To explain how tasks are solved, we'll walk through what goes on inside the model to output useful predictions.\n",
      "\n",
      "- [Wav2Vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)\n",
      "- [Vision Transformer (ViT)](model_doc/vit) and [ConvNeXT](model_doc/convnext) for image classification\n",
      "- [DETR](model_doc/detr) for object detection\n",
      "- [Mask2Former](model_doc/mask2former) for image segmentation\n",
      "- [GLPN](model_doc/glpn) for depth estimation\n",
      "- [BERT](model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder\n",
      "- [GPT2](model_doc/gpt2) for NLP tasks like text generation that use a decoder\n",
      "- [BART](model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Before you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information! \n",
      "\n",
      "</Tip>\n",
      "\n",
      "## Speech and audio\n",
      "\n",
      "[Wav2Vec2](model_doc/wav2vec2) is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. \n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png\"/>\n",
      "</div>\n",
      "\n",
      "This model has four main components:\n",
      "\n",
      "1. A *feature encoder* takes the raw audio waveform, normalizes it to zero mean and unit variance, and converts it into a sequence of feature vectors that are each 20ms long.\n",
      "\n",
      "4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer perceptron head (MLP). ViT's pretraining objective is simply classification. Like other classification heads, the MLP head converts the output into logits over the class labels and calculates the cross-entropy loss to find the most likely class.\n",
      "\n",
      "Ready to try your hand at image classification? Check out our complete [image classification guide](tasks/image_classification) to learn how to finetune ViT and use it for inference!\n",
      "\n",
      "#### CNN\n",
      "\n",
      "<Tip>\n",
      "\n",
      "This section briefly explains convolutions, but it'd be helpful to have a prior understanding of how they change an image's shape and size. If you're unfamiliar with convolutions, check out the [Convolution Neural Networks chapter](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) from the fastai book!\n",
      "\n",
      "</Tip>\n",
      "\n",
      "[ConvNeXT](model_doc/convnext) is a CNN architecture that adopts new and modern network designs to improve performance. However, convolutions are still at the core of the model. From a high-level perspective, a [convolution](glossary#convolution) is an operation where a smaller matrix (*kernel*) is multiplied by a small window of the image pixels. It computes some features from it, such as a particular texture or curvature of a line. Then it slides over to the next window of pixels; the distance the convolution travels is known as the *stride*. \n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n",
      "</div>\n",
      "\n",
      "<small>A basic convolution without padding or stride, taken from <a href=\"https://arxiv.org/abs/1603.07285\">A guide to convolution arithmetic for deep learning.</a></small>\n",
      "\n",
      "GPT-2's pretraining objective is based entirely on [causal language modeling](glossary#causal-language-modeling), predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text.\n",
      "\n",
      "Ready to try your hand at text generation? Check out our complete [causal language modeling guide](tasks/language_modeling#causal-language-modeling) to learn how to finetune DistilGPT-2 and use it for inference!\n",
      "\n",
      "<Tip>\n",
      "\n",
      "For more information about text generation, check out the [text generation strategies](generation_strategies) guide!\n",
      "\n",
      "</Tip>\n",
      "\n",
      "### Summarization\n",
      "\n",
      "Encoder-decoder models like [BART](model_doc/bart) and [T5](model_doc/t5) are designed for the sequence-to-sequence pattern of a summarization task. We'll explain how BART works in this section, and then you can try finetuning T5 at the end.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png\"/>\n",
      "</div>\n",
      "\n",
      "1. BART's encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. The *text infilling* corruption strategy works the best though. In text infilling, a number of text spans are replaced with a **single** [`mask`] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn't add a final feedforward network at the end to predict a word.\n",
      "-----------------LLm ANSWER------------\n",
      "The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n",
      "-----------------------------\n",
      "53\n",
      "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md\n",
      "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "\n",
      "distilbert base uncased finetuned sst2 english\n",
      "distilbert base uncased finetuned sst2 english\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md', 'huggingface/course/blob/main/chapters/en/chapter1/10.mdx', 'huggingface/course/blob/main/chapters/en/chapter1/10.mdx', 'huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md', 'huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md']\n",
      "hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated.\n",
      "\n",
      "!-- DISABLE-FRONTMATTER-SECTIONS -->\n",
      "\n",
      "# End-of-chapter quiz[[end-of-chapter-quiz]]\n",
      "\n",
      "<CourseFloatingBanner\n",
      "    chapter={1}\n",
      "    classNames=\"absolute z-10 right-0 top-0\"\n",
      "/>\n",
      "\n",
      "This chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n",
      "\n",
      "First, though, let's test what you learned in this chapter!\n",
      "\n",
      "\n",
      "### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\n",
      "\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"Summarization\",\n",
      "\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Text classification\",\n",
      "\t\t\texplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called <em>natural language inference</em>.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"Text generation\",\n",
      "\t\t\texplain: \"Look again on the <a href=\\\"https://huggingface.co/roberta-large-mnli\\\">roberta-large-mnli page</a>.\"\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 2. What will the following code return?\n",
      "\n",
      "```py\n",
      "from transformers import pipeline\n",
      "\n",
      "ner = pipeline(\"ner\", grouped_entities=True)\n",
      "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
      "```\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return classification scores for this sentence, with labels \\\"positive\\\" or \\\"negative\\\".\",\n",
      "\t\t\texplain: \"This is incorrect — this would be a <code>sentiment-analysis</code> pipeline.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return a generated text completing this sentence.\",\n",
      "\t\t\texplain: \"This is incorrect — it would be a <code>text-generation</code> pipeline.\",\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"It will return the words representing persons, organizations or locations.\",\n",
      "\t\t\texplain: \"Furthermore, with <code>grouped_entities=True</code>, it will group together the words belonging to the same entity, like \\\"Hugging Face\\\".\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 3. What should replace ... in this code sample?\n",
      "\n",
      "```py\n",
      "from transformers import pipeline\n",
      "\n",
      "filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
      "result = filler(\"...\")\n",
      "```\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"This &#60;mask> has been waiting for you.\",\n",
      "\t\t\texplain: \"This is incorrect. Check out the <code>bert-base-cased</code> model card and try to spot your mistake.\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"This [MASK] has been waiting for you.\",\n",
      "\t\t\texplain: \"Correct! This model's mask token is [MASK].\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"This man has been waiting for you.\",\n",
      "\t\t\texplain: \"This is incorrect. This pipeline fills in masked words, so it needs a mask token somewhere.\"\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 4. Why will this code fail?\n",
      "\n",
      "```py\n",
      "from transformers import pipeline\n",
      "\n",
      "classifier = pipeline(\"zero-shot-classification\")\n",
      "result = classifier(\"This is a course about the Transformers library\")\n",
      "```\n",
      "\n",
      "<Question\n",
      "\tchoices={[\n",
      "\t\t{\n",
      "\t\t\ttext: \"This pipeline requires that labels be given to classify this text.\",\n",
      "\t\t\texplain: \"Right — the correct code needs to include <code>candidate_labels=[...]</code>.\",\n",
      "\t\t\tcorrect: true\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"This pipeline requires several sentences, not just one.\",\n",
      "\t\t\texplain: \"This is incorrect, though when properly used, this pipeline can take a list of sentences to process (like all other pipelines).\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"The 🤗 Transformers library is broken, as usual.\",\n",
      "\t\t\texplain: \"We won't dignify this answer with a comment!\"\n",
      "\t\t},\n",
      "\t\t{\n",
      "\t\t\ttext: \"This pipeline requires longer inputs; this one is too short.\",\n",
      "\t\t\texplain: \"This is incorrect. Note that a very long text will be truncated when processed by this pipeline.\"\n",
      "\t\t}\n",
      "\t]}\n",
      "/>\n",
      "\n",
      "### 5. What does \"transfer learning\" mean?\n",
      "\n",
      "<PipelineTag pipeline=\"text-classification\"/>\n",
      "\n",
      "\n",
      "- [`AlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification).\n",
      "\n",
      "\n",
      "- [`TFAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification).\n",
      "\n",
      "- [`FlaxAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).\n",
      "- Check the [Text classification task guide](../tasks/sequence_classification) on how to use the model.\n",
      "\n",
      "\n",
      "<PipelineTag pipeline=\"token-classification\"/>\n",
      "\n",
      "\n",
      "- [`AlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification).\n",
      "\n",
      "\n",
      "- [`TFAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n",
      "\n",
      "\n",
      "\n",
      "- [`FlaxAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).\n",
      "- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the 🤗 Hugging Face Course.\n",
      "- Check the [Token classification task guide](../tasks/token_classification) on how to use the model.\n",
      "\n",
      "<PipelineTag pipeline=\"fill-mask\"/>\n",
      "\n",
      "same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, the default checkpoint used by the sentiment analysis pipeline in the Transformers library is \"distilbert base uncased finetuned sst2 english\".\n",
      "-----------------------------\n",
      "54\n",
      "huggingface/optimum/blob/main/docs/source/notebooks.md\n",
      "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n",
      "\n",
      "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "-----------------Context------------\n",
      "['huggingface/optimum/blob/main/docs/source/notebooks.md', 'huggingface/optimum/blob/main/docs/source/notebooks.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md', 'huggingface/blog/blob/main/intel.md', 'huggingface/optimum/blob/main/docs/source/notebooks.md']\n",
      "| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |\n",
      "\n",
      "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# 🤗 Optimum notebooks\n",
      "\n",
      "You can find here a list of the notebooks associated with each accelerator in 🤗 Optimum.\n",
      "\n",
      "## Optimum Habana\n",
      "\n",
      "If your model fits onto a single GPU and you have enough space to fit a small batch size, you don't need to use DeepSpeed\n",
      "as it'll only slow things down. However, if the model doesn't fit onto a single GPU or you can't fit a small batch, you can \n",
      "leverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models. In this case, you need to separately\n",
      "[install the library](main_classes/deepspeed#installation), then follow one of the guides to create a configuration file \n",
      "and launch DeepSpeed: \n",
      " \n",
      "* For an in-depth guide on DeepSpeed integration with [`Trainer`], review [the corresponding documentation](main_classes/deepspeed), specifically the \n",
      "[section for a single GPU](main_classes/deepspeed#deployment-with-one-gpu). Some adjustments are required to use DeepSpeed in a notebook; please take a look at the [corresponding guide](main_classes/deepspeed#deployment-in-notebooks).\n",
      "* If you prefer to use 🤗 Accelerate, refer to [🤗 Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed).\n",
      "\n",
      "## Using torch.compile\n",
      "\n",
      "PyTorch 2.0 introduced a new compile function that doesn't require any modification to existing PyTorch code but can \n",
      "optimize your code by adding a single line of code: `model = torch.compile(model)`.\n",
      "\n",
      "If using [`Trainer`], you only need `to` pass the `torch_compile` option in the [`TrainingArguments`]: \n",
      "\n",
      "```python\n",
      "training_args = TrainingArguments(torch_compile=True, **default_args)\n",
      "```\n",
      "\n",
      "`torch.compile` uses Python's frame evaluation API to automatically create a graph from existing PyTorch programs. After \n",
      "capturing the graph, different backends can be deployed to lower the graph to an optimized engine. \n",
      "You can find more details and benchmarks in [PyTorch documentation](https://pytorch.org/get-started/pytorch-2.0/).\n",
      "\n",
      "`torch.compile` has a growing list of backends, which can be found in by calling `torchdynamo.list_backends()`, each of which with its optional dependencies.\n",
      "\n",
      "“*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics.\n",
      "\n",
      "In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs.\n",
      "\n",
      "The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it.\n",
      "\n",
      "## Get Peak Transformers Performance with Optimum Intel\n",
      "[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.\n",
      "\n",
      "| [How to fine-tune a model for summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)                         | Show how to fine-tune a T5 model on the BBC news corpus.                                                                                       | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)                |                [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb) |\n",
      "| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)                         | Show how to fine-tune a DeBERTa model on the squad.                                                                                       | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)                |                [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb) |\n",
      "-----------------LLm ANSWER------------\n",
      "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to show how to use DeepSpeed to pre-train and fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "-----------------------------\n",
      "55\n",
      "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md\n",
      "What command line module does PyTorch provide to run a script on multiple GPUs?\n",
      "\n",
      "torchrun\n",
      "torchrun\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md', 'huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md', 'huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md', 'huggingface/blog/blob/main/accelerate-library.md', 'huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md']\n",
      "Typically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook.\n",
      "\n",
      "However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n",
      "\n",
      "## PyTorch Distributed Data Parallelism\n",
      "\n",
      "As the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.\n",
      "\n",
      "To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n",
      "\n",
      "First a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\n",
      "\n",
      "> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\n",
      "\n",
      "```python\n",
      "import os\n",
      "import torch.distributed as dist\n",
      "\n",
      "def setup(rank, world_size):\n",
      "    \"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\n",
      "    os.environ[\"MASTER_ADDR\"] = 'localhost'\n",
      "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
      "\n",
      "    # Initialize the process group\n",
      "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
      "\n",
      "def cleanup():\n",
      "    \"Cleans up the distributed environment\"\n",
      "    dist.destroy_process_group()\n",
      "```\n",
      "\n",
      "The last piece of the puzzle is *how do I send my data and model to another GPU?*\n",
      "\n",
      "def cleanup():\n",
      "    \"Cleans up the distributed environment\"\n",
      "    dist.destroy_process_group()\n",
      "```\n",
      "\n",
      "The last piece of the puzzle is *how do I send my data and model to another GPU?*\n",
      "\n",
      "This is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step.\n",
      "\n",
      "Below is an example of our training setup, refactored as a function, with this capability:\n",
      "\n",
      "> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\n",
      "\n",
      "```python\n",
      "from torch.nn.parallel import DistributedDataParallel as DDP\n",
      "\n",
      "def train(model, rank, world_size):\n",
      "    setup(rank, world_size)\n",
      "    model = model.to(rank)\n",
      "    ddp_model = DDP(model, device_ids=[rank])\n",
      "    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\n",
      "    # Train for one epoch\n",
      "    model.train()\n",
      "    for batch_idx, (data, target) in enumerate(train_loader):\n",
      "        data, target = data.to(device), target.to(device)\n",
      "        output = model(data)\n",
      "        loss = F.nll_loss(output, target)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        optimizer.zero_grad()\n",
      "    cleanup()\n",
      "```\n",
      "\n",
      "The optimizer needs to be declared based on the model *on the specific device* (so `ddp_model` and not `model`) for all of the gradients to properly be calculated.\n",
      "\n",
      "Lastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n",
      "\n",
      "```bash\n",
      "torchrun --nproc_per_node=2 --nnodes=1 example_script.py\n",
      "```\n",
      "\n",
      "The above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n",
      "\n",
      "```bash\n",
      "torchrun --nproc_per_node=2 --nnodes=1 example_script.py\n",
      "```\n",
      "\n",
      "The above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n",
      "\n",
      "Now let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n",
      "\n",
      "## 🤗 Accelerate\n",
      "\n",
      "[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well.\n",
      "\n",
      "First, let's wrap all of the above code we just performed into a single function, to help us visualize the difference:\n",
      "\n",
      "```python\n",
      "def train_ddp(rank, world_size):\n",
      "    setup(rank, world_size)\n",
      "    # Build DataLoaders\n",
      "    transform = transforms.Compose([\n",
      "        transforms.ToTensor(),\n",
      "        transforms.Normalize((0.1307), (0.3081))\n",
      "    ])\n",
      "\n",
      "    train_dset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
      "    test_dset = datasets.MNIST('data', train=False, transform=transform)\n",
      "\n",
      "    train_loader = torch.utils.data.DataLoader(train_dset, shuffle=True, batch_size=64)\n",
      "    test_loader = torch.utils.data.DataLoader(test_dset, shuffle=False, batch_size=64)\n",
      "\n",
      "    # Build model\n",
      "    model = model.to(rank)\n",
      "    ddp_model = DDP(model, device_ids=[rank])\n",
      "\n",
      "    # Build optimizer\n",
      "    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\n",
      "\n",
      "model = torch.nn.Transformer().to(device)\n",
      "+ model = DistributedDataParallel(model)  \n",
      "  optim = torch.optim.Adam(model.parameters())\n",
      "\n",
      "  dataset = load_dataset('my_dataset')\n",
      "+ sampler = DistributedSampler(dataset)\n",
      "- data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
      "+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)\n",
      "\n",
      "  model.train()\n",
      "  for epoch in range(10):\n",
      "+     sampler.set_epoch(epoch)  \n",
      "      for source, targets in data:\n",
      "          source = source.to(device)\n",
      "          targets = targets.to(device)\n",
      "\n",
      "          optimizer.zero_grad()\n",
      "\n",
      "          output = model(source)\n",
      "          loss = F.cross_entropy(output, targets)\n",
      "\n",
      "          loss.backward()\n",
      "\n",
      "          optimizer.step()\n",
      "```\n",
      "\n",
      "These changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere). Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\n",
      "- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible.\n",
      "- to have the same functions work for any distributed setup, so only have to learn one API.\n",
      "\n",
      "### How does it work?\n",
      "\n",
      "To see how the library works in practice, let's have a look at each line of code we need to add to a training loop.\n",
      "\n",
      "```python\n",
      "accelerator = Accelerator()\n",
      "```\n",
      "\n",
      "On top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script.\n",
      "\n",
      "```python\n",
      "model, optim, data = accelerator.prepare(model, optim, data)\n",
      "```\n",
      "\n",
      "trainer.train()\n",
      "\n",
      "notebook_launcher(train_trainer_ddp, args=(), num_processes=2)\n",
      "```\n",
      "\n",
      "## Resources\n",
      "\n",
      "To learn more about PyTorch Distributed Data Parallelism, check out the documentation [here](https://pytorch.org/docs/stable/distributed.html)\n",
      "\n",
      "To learn more about 🤗 Accelerate, check out the documentation [here](https://huggingface.co/docs/accelerate)\n",
      "\n",
      "To learn more about 🤗 Transformers, check out the documentation [here](https://huggingface.co/docs/transformers)\n",
      "-----------------LLm ANSWER------------\n",
      "PyTorch provides the `torchrun` command line module to run a script on multiple GPUs.\n",
      "-----------------------------\n",
      "56\n",
      "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md\n",
      "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n",
      "\n",
      "google/vit-base-patch16-224\n",
      "google/vit-base-patch16-224\n",
      "-----------------Context------------\n",
      "['gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'huggingface/transformers/blob/main/docs/source/en/tasks_explained.md', 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md']\n",
      "Image Classification with Vision Transformers\n",
      "\n",
      "Related spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\n",
      "Tags: VISION, TRANSFORMERS, HUB\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\n",
      "\n",
      "State-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio's _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\n",
      "\n",
      "Let's get started!\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "Make sure you have the `gradio` Python package already [installed](/getting_started).\n",
      "\n",
      "## Step 1 — Choosing a Vision Image Classification Model\n",
      "\n",
      "First, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\n",
      "\n",
      "Expand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n",
      "\n",
      "At the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n",
      "\n",
      "## Step 2 — Loading the Vision Transformer Model with Gradio\n",
      "\n",
      "### Image classification\n",
      "\n",
      "ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.\n",
      "\n",
      "#### Transformer\n",
      "\n",
      "[ViT](model_doc/vit) replaces convolutions entirely with a pure Transformer architecture. If you're familiar with the original Transformer, then you're already most of the way toward understanding ViT.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"/>\n",
      "</div>\n",
      "\n",
      "The main change ViT introduced was in how images are fed to a Transformer:\n",
      "\n",
      "1. An image is split into square non-overlapping patches, each of which gets turned into a vector or *patch embedding*. The patch embeddings are generated from a convolutional 2D layer which creates the proper input dimensions (which for a base Transformer is 768 values for each patch embedding). If you had a 224x224 pixel image, you could split it into 196 16x16 image patches. Just like how text is tokenized into words, an image is \"tokenized\" into a sequence of patches.\n",
      "\n",
      "2. A *learnable embedding* - a special `[CLS]` token - is added to the beginning of the patch embeddings just like BERT. The final hidden state of the `[CLS]` token is used as the input to the attached classification head; other outputs are ignored. This token helps the model learn how to encode a representation of the image.\n",
      "\n",
      "3. The last thing to add to the patch and learnable embeddings are the *position embeddings* because the model doesn't know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.\n",
      "\n",
      "To explain how tasks are solved, we'll walk through what goes on inside the model to output useful predictions.\n",
      "\n",
      "- [Wav2Vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)\n",
      "- [Vision Transformer (ViT)](model_doc/vit) and [ConvNeXT](model_doc/convnext) for image classification\n",
      "- [DETR](model_doc/detr) for object detection\n",
      "- [Mask2Former](model_doc/mask2former) for image segmentation\n",
      "- [GLPN](model_doc/glpn) for depth estimation\n",
      "- [BERT](model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder\n",
      "- [GPT2](model_doc/gpt2) for NLP tasks like text generation that use a decoder\n",
      "- [BART](model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Before you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you're just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information! \n",
      "\n",
      "</Tip>\n",
      "\n",
      "## Speech and audio\n",
      "\n",
      "[Wav2Vec2](model_doc/wav2vec2) is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. \n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png\"/>\n",
      "</div>\n",
      "\n",
      "This model has four main components:\n",
      "\n",
      "1. A *feature encoder* takes the raw audio waveform, normalizes it to zero mean and unit variance, and converts it into a sequence of feature vectors that are each 20ms long.\n",
      "\n",
      "Ready to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio_classification) to learn how to finetune Wav2Vec2 and use it for inference!\n",
      "\n",
      "### Automatic speech recognition\n",
      "\n",
      "To use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription.\n",
      "\n",
      "Ready to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](tasks/asr) to learn how to finetune Wav2Vec2 and use it for inference!\n",
      "\n",
      "## Computer vision\n",
      "\n",
      "There are two ways to approach computer vision tasks:\n",
      "\n",
      "1. Split an image into a sequence of patches and process them in parallel with a Transformer.\n",
      "2. Use a modern CNN, like [ConvNeXT](model_doc/convnext), which relies on convolutional layers but adopts modern network designs.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "A third approach mixes Transformers with convolutions (for example, [Convolutional Vision Transformer](model_doc/cvt) or [LeViT](model_doc/levit)). We won't discuss those because they just combine the two approaches we examine here.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we'll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks.\n",
      "\n",
      "### Image classification\n",
      "\n",
      "ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.\n",
      "\n",
      "#### Transformer\n",
      "\n",
      "## Step 2 — Loading the Vision Transformer Model with Gradio\n",
      "\n",
      "When using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.\n",
      "All of these are automatically inferred from the model tags.\n",
      "\n",
      "Besides the import statement, it only takes a single line of Python to load and launch the demo.\n",
      "\n",
      "We use the `gr.Interface.load()` method and pass in the path to the model including the `huggingface/` to designate that it is from the Hugging Face Hub.\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "gr.Interface.load(\n",
      "             \"huggingface/google/vit-base-patch16-224\",\n",
      "             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\n",
      "```\n",
      "\n",
      "Notice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples.\n",
      "\n",
      "This produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction. Try uploading your own image!\n",
      "\n",
      "<gradio-app space=\"gradio/vision-transformer\">\n",
      "\n",
      "---\n",
      "\n",
      "And you're done! In one line of code, you have built a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, the most popular vision transformer model on the Hugging Face Model Hub for image classification is `google/vit-base-patch16-224`.\n",
      "-----------------------------\n",
      "57\n",
      "huggingface/hub-docs/blob/main/docs/hub/espnet.md\n",
      "What is the command to upload an ESPnet model to a Hugging Face repository?\n",
      "\n",
      "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "-----------------Context------------\n",
      "['huggingface/hub-docs/blob/main/docs/hub/espnet.md', 'huggingface/hub-docs/blob/main/docs/hub/espnet.md', 'huggingface/blog/blob/main/skops.md', 'huggingface/blog/blob/main/spacy.md', 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md']\n",
      "```py\n",
      "import soundfile\n",
      "from espnet2.bin.tts_inference import Text2Speech\n",
      "\n",
      "text2speech = Text2Speech.from_pretrained(\"model_name\")\n",
      "speech = text2speech(\"foobar\")[\"wav\"]\n",
      "soundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n",
      "```\n",
      "\n",
      "If you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\n",
      "<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\n",
      "</div>\n",
      "\n",
      "## Sharing your models\n",
      "\n",
      "`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\n",
      "\n",
      "The `run.sh` script allows to upload a given model to a Hugging Face repository.\n",
      "\n",
      "```bash\n",
      "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "```\n",
      "\n",
      "## Additional resources\n",
      "\n",
      "* ESPnet [docs](https://espnet.github.io/espnet/index.html).\n",
      "* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\n",
      "* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).\n",
      "\n",
      "Using ESPnet at Hugging Face\n",
      "\n",
      "`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n",
      "\n",
      "## Exploring ESPnet in the Hub\n",
      "\n",
      "You can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n",
      "\n",
      "All models on the Hub come up with useful features:\n",
      "1. An automatically generated model card with a description, a training configuration, licenses and more.\n",
      "2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n",
      "3. An interactive widget you can use to play out with the model directly in the browser.\n",
      "4. An Inference API that allows to make inference requests.\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"/>\n",
      "<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png\"/>\n",
      "</div>\n",
      "\n",
      "## Using existing models\n",
      "\n",
      "For a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)). \n",
      "\n",
      "If you're interested in doing inference, different classes for different tasks have a `from_pretrained` method that allows loading models from the Hub. For example:\n",
      "* `Speech2Text` for Automatic Speech Recognition.\n",
      "* `Text2Speech` for Text to Speech.\n",
      "* `SeparateSpeech` for Audio Source Separation.\n",
      "\n",
      "Here is an inference example:\n",
      "\n",
      "```py\n",
      "import soundfile\n",
      "from espnet2.bin.tts_inference import Text2Speech\n",
      "\n",
      "text2speech = Text2Speech.from_pretrained(\"model_name\")\n",
      "speech = text2speech(\"foobar\")[\"wav\"]\n",
      "soundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n",
      "```\n",
      "\n",
      "We can also add any plot of our choice to the card using `add_plot` like below.\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "from pathlib import Path\n",
      "# we will create a confusion matrix\n",
      "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
      "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
      "disp.plot()\n",
      "\n",
      "# save the plot\n",
      "plt.savefig(Path(local_repo) / \"confusion_matrix.png\")\n",
      "\n",
      "# the plot will be written to the model card under the name confusion_matrix\n",
      "# we pass the path of the plot itself\n",
      "model_card.add_plot(confusion_matrix=\"confusion_matrix.png\")\n",
      "```\n",
      "\n",
      "Let's save the model card in the local repository. The file name here should be `README.md` since it is what Hugging Face Hub expects.\n",
      "```python\n",
      "model_card.save(Path(local_repo) / \"README.md\")\n",
      "```\n",
      "\n",
      "We can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either  `notebook_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI.\n",
      "\n",
      "```python\n",
      "# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\n",
      "repo_id = \"skops-user/my-awesome-model\"\n",
      "hub_utils.push(\n",
      "    repo_id=repo_id,\n",
      "    source=local_repo,\n",
      "    token=token,\n",
      "    commit_message=\"pushing files to the repo from the example!\",\n",
      "    create_remote=True,\n",
      ")\n",
      "```\n",
      "\n",
      "Once we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n",
      "\n",
      "```python\n",
      "download_repo = \"downloaded-model\"\n",
      "hub_utils.download(repo_id=repo_id, dst=download_repo)\n",
      "```\n",
      "\n",
      "The inference widget is enabled to make predictions in the repository.\n",
      "\n",
      "![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n",
      "\n",
      "### Using existing models\n",
      "\n",
      "All models from the Hub can be directly installed using `pip install`. \n",
      "\n",
      "\n",
      "```bash\n",
      "pip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\n",
      "```\n",
      "\n",
      "```python\n",
      "# Using spacy.load().\n",
      "import spacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "# Importing as module.\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "```\n",
      "\n",
      "When you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!\n",
      "\n",
      "![snippet](assets/23_spacy/snippet.png)\n",
      "![snippet](assets/23_spacy/snippet2.png)\n",
      "\n",
      "You can even make HTTP requests to call the models from the Inference API, which is useful in production settings. Here is an example of a simple request:\n",
      "\n",
      "```bash\n",
      "curl -X POST  --data '{\"inputs\": \"Hello, this is Omar\"}' https://api-inference.huggingface.co/models/spacy/en_core_web_sm\n",
      ">>> [{\"entity_group\":\"PERSON\",\"word\":\"Omar\",\"start\":15,\"end\":19,\"score\":1.0}]\n",
      "```\n",
      "\n",
      "And for larger-scale use cases, you can click \"Deploy > Accelerated Inference\" and see how to do this with Python.\n",
      "\n",
      "\n",
      "### Sharing your models\n",
      "\n",
      "But probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \n",
      "\n",
      "```bash\n",
      "huggingface-cli login\n",
      "python -m spacy package ./en_ner_fashion ./output --build wheel\n",
      "cd ./output/en_ner_fashion-0.0.0/dist\n",
      "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "```\n",
      "\n",
      "In just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n",
      "\n",
      "Try it out and share your models with the community!\n",
      "\n",
      "## Would you like to integrate your library to the Hub?\n",
      "\n",
      "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Create and manage a repository\n",
      "\n",
      "The Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\n",
      "development to easily version projects when working collaboratively. This guide will show you how to interact with the\n",
      "repositories on the Hub, especially:\n",
      "\n",
      "- Create and delete a repository.\n",
      "- Manage branches and tags. \n",
      "- Rename your repository.\n",
      "- Update your repository visibility.\n",
      "- Manage a local copy of your repository.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\n",
      "might be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\n",
      "(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\n",
      "not share the same requirements and workflows. Model repositories might maintain large model weight files for different\n",
      "frameworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\n",
      "a result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\n",
      "explanation page for more details.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\n",
      "[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\n",
      "\n",
      "## Repo creation and deletion\n",
      "\n",
      "The first step is to know how to create and delete repositories. You can only manage repositories that you own (under\n",
      "your username namespace) or from organizations in which you have write permissions.\n",
      "\n",
      "### Create a repository\n",
      "-----------------LLm ANSWER------------\n",
      "The command to upload an ESPnet model to a Hugging Face repository is:\n",
      "\n",
      "```bash\n",
      "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "```\n",
      "-----------------------------\n",
      "58\n",
      "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx\n",
      "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n",
      "\n",
      "requirements.txt\n",
      "requirements.txt\n",
      "-----------------Context------------\n",
      "['huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx', 'huggingface/blog/blob/main/skops.md', 'huggingface/blog/blob/main/skops.md', 'huggingface/blog/blob/main/skops.md', 'huggingface/blog/blob/main/spacy.md']\n",
      "Add custom Dependencies\n",
      "\n",
      "Inference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n",
      "\n",
      "* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n",
      "* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\n",
      "\n",
      "To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n",
      "\n",
      "```bash\n",
      "optimum[onnxruntime]==1.2.3\n",
      "mkl-include\n",
      "mkl\n",
      "```\n",
      "\n",
      "Check out the `requirements.txt` files in the following model repositories for examples:\n",
      "\n",
      "* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n",
      "* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\n",
      "\n",
      "For more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference.\n",
      "\n",
      "```python\n",
      "download_repo = \"downloaded-model\"\n",
      "hub_utils.download(repo_id=repo_id, dst=download_repo)\n",
      "```\n",
      "\n",
      "The inference widget is enabled to make predictions in the repository.\n",
      "\n",
      "![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n",
      "\n",
      "If the requirements of your project have changed, you can use `update_env` to update the environment.\n",
      "\n",
      "```python\n",
      "hub_utils.update_env(path=local_repo, requirements=[\"scikit-learn\"])\n",
      "```\n",
      "\n",
      "You can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example).\n",
      "We have prepared two examples to show how to save your models and use model card utilities. You can find them in the resources section below.\n",
      "\n",
      "\n",
      "## Resources\n",
      "- [Model card tutorial](https://skops.readthedocs.io/en/latest/auto_examples/plot_model_card.html)\n",
      "- [hub_utils tutorial](https://skops.readthedocs.io/en/latest/auto_examples/plot_hf_hub.html)\n",
      "- [skops documentation](https://skops.readthedocs.io/en/latest/modules/classes.html)\n",
      "\n",
      "We can also add any plot of our choice to the card using `add_plot` like below.\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "from pathlib import Path\n",
      "# we will create a confusion matrix\n",
      "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
      "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
      "disp.plot()\n",
      "\n",
      "# save the plot\n",
      "plt.savefig(Path(local_repo) / \"confusion_matrix.png\")\n",
      "\n",
      "# the plot will be written to the model card under the name confusion_matrix\n",
      "# we pass the path of the plot itself\n",
      "model_card.add_plot(confusion_matrix=\"confusion_matrix.png\")\n",
      "```\n",
      "\n",
      "Let's save the model card in the local repository. The file name here should be `README.md` since it is what Hugging Face Hub expects.\n",
      "```python\n",
      "model_card.save(Path(local_repo) / \"README.md\")\n",
      "```\n",
      "\n",
      "We can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either  `notebook_login` if you're logging in from a notebook, or `huggingface-cli login` if you're logging in from the CLI.\n",
      "\n",
      "```python\n",
      "# if the repository doesn't exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\n",
      "repo_id = \"skops-user/my-awesome-model\"\n",
      "hub_utils.push(\n",
      "    repo_id=repo_id,\n",
      "    source=local_repo,\n",
      "    token=token,\n",
      "    commit_message=\"pushing files to the repo from the example!\",\n",
      "    create_remote=True,\n",
      ")\n",
      "```\n",
      "\n",
      "Once we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n",
      "\n",
      "```python\n",
      "download_repo = \"downloaded-model\"\n",
      "hub_utils.download(repo_id=repo_id, dst=download_repo)\n",
      "```\n",
      "\n",
      "The inference widget is enabled to make predictions in the repository.\n",
      "\n",
      "![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n",
      "\n",
      "```python\n",
      "from skops import hub_utils\n",
      "import pickle\n",
      "\n",
      "# let's save the model\n",
      "model_path = \"example.pkl\"\n",
      "local_repo = \"my-awesome-model\"\n",
      "with open(model_path, mode=\"bw\") as f:\n",
      "    pickle.dump(model, file=f)\n",
      "\n",
      "# we will now initialize a local repository\n",
      "hub_utils.init(\n",
      "    model=model_path, \n",
      "    requirements=[f\"scikit-learn={sklearn.__version__}\"], \n",
      "    dst=local_repo,\n",
      "    task=\"tabular-classification\",\n",
      "    data=X_test,\n",
      ")\n",
      "```\n",
      "\n",
      "The repository now contains the serialized model and the configuration file. \n",
      "The configuration contains the following:\n",
      "- features of the model,\n",
      "- the requirements of the model,\n",
      "- an example input taken from `X_test` that we've passed,\n",
      "- name of the model file,\n",
      "- name of the task to be solved here.\n",
      "\n",
      "We will now create the model card. The card should match the expected Hugging Face Hub format: a markdown part and a metadata section, which is a `yaml` section at the top. The keys to the metadata section are defined [here](https://huggingface.co/docs/hub/models-cards#model-card-metadata) and are used for the discoverability of the models. \n",
      "The content of the model card is determined by a template that has a:\n",
      "- `yaml` section on top for metadata (e.g. model license, library name, and more)\n",
      "- markdown section with free text and sections to be filled (e.g. simple description of the model),\n",
      "The following sections are extracted by `skops` to fill in the model card:\n",
      "- Hyperparameters of the model,\n",
      "- Interactive diagram of the model,\n",
      "- For metadata, library name, task identifier (e.g. tabular-classification), and information required by the inference widget are filled.\n",
      "\n",
      "We will walk you through how to programmatically pass information to fill the model card. You can check out our documentation on the default template provided by `skops`, and its sections [here](https://skops.readthedocs.io/en/latest/model_card.html) to see what the template expects and what it looks like [here](https://github.com/skops-dev/skops/blob/main/skops/card/default_template.md).\n",
      "\n",
      "### Using existing models\n",
      "\n",
      "All models from the Hub can be directly installed using `pip install`. \n",
      "\n",
      "\n",
      "```bash\n",
      "pip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\n",
      "```\n",
      "\n",
      "```python\n",
      "# Using spacy.load().\n",
      "import spacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "# Importing as module.\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "```\n",
      "\n",
      "When you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!\n",
      "\n",
      "![snippet](assets/23_spacy/snippet.png)\n",
      "![snippet](assets/23_spacy/snippet2.png)\n",
      "\n",
      "You can even make HTTP requests to call the models from the Inference API, which is useful in production settings. Here is an example of a simple request:\n",
      "\n",
      "```bash\n",
      "curl -X POST  --data '{\"inputs\": \"Hello, this is Omar\"}' https://api-inference.huggingface.co/models/spacy/en_core_web_sm\n",
      ">>> [{\"entity_group\":\"PERSON\",\"word\":\"Omar\",\"start\":15,\"end\":19,\"score\":1.0}]\n",
      "```\n",
      "\n",
      "And for larger-scale use cases, you can click \"Deploy > Accelerated Inference\" and see how to do this with Python.\n",
      "\n",
      "\n",
      "### Sharing your models\n",
      "\n",
      "But probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \n",
      "\n",
      "```bash\n",
      "huggingface-cli login\n",
      "python -m spacy package ./en_ner_fashion ./output --build wheel\n",
      "cd ./output/en_ner_fashion-0.0.0/dist\n",
      "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "```\n",
      "\n",
      "In just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n",
      "\n",
      "Try it out and share your models with the community!\n",
      "\n",
      "## Would you like to integrate your library to the Hub?\n",
      "-----------------LLm ANSWER------------\n",
      "To install custom Python dependencies for Inference Endpoints, you should add a `requirements.txt` file to a model repository.\n",
      "-----------------------------\n",
      "59\n",
      "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md\n",
      "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n",
      "\n",
      "3-5 images\n",
      "3-5 images\n",
      "-----------------Context------------\n",
      "['huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md', 'huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md', 'gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md', 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md', 'huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md']\n",
      "Distillation for quantization on Textual Inversion models to personalize text2image\n",
      "\n",
      "[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\n",
      "The `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n",
      "We have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n",
      "\n",
      "## Installing the dependencies\n",
      "\n",
      "Before running the scripts, make sure to install the library's training dependencies:\n",
      "\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "## Prepare Datasets\n",
      "\n",
      "One picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n",
      "\n",
      "<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\n",
      "    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\n",
      "</a>\n",
      "\n",
      "## Get a FP32 Textual Inversion model\n",
      "\n",
      "Use the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n",
      "\n",
      "```bash\n",
      "export MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\n",
      "export DATA_DIR=\"./dicoo\"\n",
      "\n",
      "accelerate launch textual_inversion.py \\\n",
      "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
      "  --train_data_dir=$DATA_DIR \\\n",
      "  --learnable_property=\"object\" \\\n",
      "  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\n",
      "  --resolution=512 \\\n",
      "  --train_batch_size=1 \\\n",
      "  --gradient_accumulation_steps=4 \\\n",
      "  --max_train_steps=3000 \\\n",
      "  --learning_rate=5.0e-04 --scale_lr \\\n",
      "  --lr_scheduler=\"constant\" \\\n",
      "  --lr_warmup_steps=0 \\\n",
      "  --output_dir=\"dicoo_model\"\n",
      "```\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# Textual Inversion\n",
      "\n",
      "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\n",
      "\n",
      "[`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "To learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading_adapters#textual-inversion) loading guide.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "## TextualInversionLoaderMixin\n",
      "\n",
      "[[autodoc]] loaders.textual_inversion.TextualInversionLoaderMixin\n",
      "\n",
      "ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "\n",
      "- [Stable Diffusion](./stable_diffusion/overview)\n",
      "- [ControlNet](./controlnet)\n",
      "- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n",
      "- [DeepFloyd IF](./deepfloyd_if)\n",
      "- [Kandinsky 2.1](./kandinsky)\n",
      "- [Kandinsky 2.2](./kandinsky_v22)\n",
      "\n",
      "\n",
      "## AutoPipelineForText2Image\n",
      "\n",
      "[[autodoc]] AutoPipelineForText2Image\n",
      "\t- all\n",
      "\t- from_pretrained\n",
      "\t- from_pipe\n",
      "\n",
      "## AutoPipelineForImage2Image\n",
      "\n",
      "[[autodoc]] AutoPipelineForImage2Image\n",
      "\t- all\n",
      "\t- from_pretrained\n",
      "\t- from_pipe\n",
      "\n",
      "## AutoPipelineForInpainting\n",
      "\n",
      "[[autodoc]] AutoPipelineForInpainting\n",
      "\t- all\n",
      "\t- from_pretrained\n",
      "\t- from_pipe\n",
      "\n",
      "# Don't forget to save the exported model\n",
      "pipeline.save_pretrained(\"openvino-sd-v1-5\")\n",
      "```\n",
      "\n",
      "To further speed-up inference, statically reshape the model. If you change any parameters such as the outputs height or width, you’ll need to statically reshape your model again.\n",
      "\n",
      "```python\n",
      "# Define the shapes related to the inputs and desired outputs\n",
      "batch_size, num_images, height, width = 1, 1, 512, 512\n",
      "\n",
      "# Statically reshape the model\n",
      "pipeline.reshape(batch_size, height, width, num_images)\n",
      "# Compile the model before inference\n",
      "pipeline.compile()\n",
      "\n",
      "image = pipeline(\n",
      "    prompt,\n",
      "    height=height,\n",
      "    width=width,\n",
      "    num_images_per_prompt=num_images,\n",
      ").images[0]\n",
      "```\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png\">\n",
      "</div>\n",
      "\n",
      "You can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting.\n",
      "\n",
      "## Stable Diffusion XL\n",
      "\n",
      "To load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\n",
      "\n",
      "```python\n",
      "from optimum.intel import OVStableDiffusionXLPipeline\n",
      "\n",
      "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
      "pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\n",
      "prompt = \"sailing ship in storm by Rembrandt\"\n",
      "image = pipeline(prompt).images[0]\n",
      "```\n",
      "\n",
      "To further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section.\n",
      "\n",
      "You can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image.\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided context, 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion.\n",
      "-----------------------------\n",
      "60\n",
      "huggingface/transformers/blob/main/docs/source/en/big_models.md\n",
      "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "\n",
      "10GB\n",
      "10GB\n",
      "-----------------Context------------\n",
      "['huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/transformers/blob/main/docs/source/en/big_models.md', 'huggingface/peft/blob/main/docs/source/accelerate/fsdp.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md']\n",
      "</Tip>\n",
      "\n",
      "In this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n",
      "\n",
      "## Sharded checkpoints\n",
      "\n",
      "Since version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n",
      "\n",
      "You can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n",
      "\n",
      "```py\n",
      "from transformers import AutoModel\n",
      "\n",
      "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
      "```\n",
      "\n",
      "If you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n",
      "\n",
      "```py\n",
      ">>> import os\n",
      ">>> import tempfile\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir)\n",
      "...     print(sorted(os.listdir(tmp_dir)))\n",
      "['config.json', 'pytorch_model.bin']\n",
      "```\n",
      "\n",
      "Now let's use a maximum shard size of 200MB:\n",
      "\n",
      "```py\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     print(sorted(os.listdir(tmp_dir)))\n",
      "['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n",
      "```\n",
      "\n",
      "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n",
      "\n",
      "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n",
      "\n",
      "```py\n",
      ">>> from transformers.modeling_utils import load_sharded_checkpoint\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     load_sharded_checkpoint(model, tmp_dir)\n",
      "```\n",
      "\n",
      "## Low memory loading\n",
      "\n",
      "Sharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n",
      "\n",
      "Please read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\n",
      "\n",
      "On top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\n",
      "\n",
      "```py\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     new_model = AutoModel.from_pretrained(tmp_dir)\n",
      "```\n",
      "\n",
      "The main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n",
      "\n",
      "Behind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n",
      "\n",
      "```py\n",
      ">>> import json\n",
      "\n",
      ">>> with tempfile.TemporaryDirectory() as tmp_dir:\n",
      "...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n",
      "...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n",
      "...         index = json.load(f)\n",
      "\n",
      ">>> print(index.keys())\n",
      "dict_keys(['metadata', 'weight_map'])\n",
      "```\n",
      "\n",
      "The metadata just consists of the total size of the model for now. We plan to add other information in the future:\n",
      "\n",
      "```py\n",
      ">>> index[\"metadata\"]\n",
      "{'total_size': 433245184}\n",
      "```\n",
      "\n",
      "The weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it's stored in:\n",
      "\n",
      "```py\n",
      ">>> index[\"weight_map\"]\n",
      "{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n",
      " 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n",
      " ...\n",
      "```\n",
      "\n",
      "If you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\n",
      "\n",
      "!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n",
      "rendered properly in your Markdown viewer.\n",
      "-->\n",
      "\n",
      "# Fully Sharded Data Parallel\n",
      "\n",
      "[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "Currently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "FSDP is supported in 🤗 Accelerate, and you can use it with 🤗 PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py). You'll configure the script to train a large model for conditional generation.\n",
      "\n",
      "## Configuration\n",
      "\n",
      "Begin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp) with 🤗 Accelerate. Use the `--config_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default_config.yaml` file in the 🤗 Accelerate cache.\n",
      "\n",
      "The configuration file is used to set the default options when you launch the training script.\n",
      "\n",
      "```bash\n",
      "accelerate config --config_file fsdp_config.yaml\n",
      "```\n",
      "\n",
      "In the above example, your effective batch size becomes 4. \n",
      "\n",
      "Alternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example \n",
      "[further down in this guide](#using-accelerate).\n",
      "\n",
      "While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can \n",
      "result in a more pronounced training slowdown. Consider the following example. Let's say, the `per_device_train_batch_size=4` \n",
      "without gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the \n",
      "`per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` \n",
      "and set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of \n",
      "the available GPU resources.\n",
      "\n",
      "For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\n",
      "and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\n",
      "\n",
      "## Gradient Checkpointing\n",
      "\n",
      "Some large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. \n",
      "This is because there are other components that also require memory storage.\n",
      "\n",
      "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in \n",
      "significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed \n",
      "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
      "-----------------LLm ANSWER------------\n",
      "According to the provided context, the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n",
      "-----------------------------\n",
      "61\n",
      "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md\n",
      "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n",
      "\n",
      "To track their machine learning experiments at every stage, from training to production.\n",
      "To track their machine learning experiments at every stage, from training to production.\n",
      "-----------------Context------------\n",
      "['gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md']\n",
      "Gradio and W&B Integration\n",
      "\n",
      "Related spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\n",
      "Tags: WANDB, SPACES\n",
      "Contributed by Gradio team\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In this Guide, we'll walk you through:\n",
      "\n",
      "- Introduction of Gradio, and Hugging Face Spaces, and Wandb\n",
      "- How to setup a Gradio demo using the Wandb integration for JoJoGAN\n",
      "- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\n",
      "\n",
      "\n",
      "## What is Wandb?\n",
      "\n",
      "Weights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\n",
      "\n",
      "<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\n",
      "\n",
      "## What are Hugging Face Spaces & Gradio?\n",
      "\n",
      "### Gradio\n",
      "\n",
      "Gradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n",
      "\n",
      "Get started [here](https://gradio.app/getting_started)\n",
      "\n",
      "### Hugging Face Spaces\n",
      "\n",
      "Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\n",
      "\n",
      "## Setting up a Gradio Demo for JoJoGAN\n",
      "\n",
      "Now, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&B and Gradio for the purposes of this tutorial.\n",
      "\n",
      "Let's get started!\n",
      "\n",
      "1. Create a W&B account\n",
      "\n",
      "## Setting up a Gradio Demo for JoJoGAN\n",
      "\n",
      "Now, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&B and Gradio for the purposes of this tutorial.\n",
      "\n",
      "Let's get started!\n",
      "\n",
      "1. Create a W&B account\n",
      "\n",
      "   Follow [these quick instructions](https://app.wandb.ai/login) to create your free account if you don’t have one already. It shouldn't take more than a couple minutes. Once you're done (or if you've already got an account), next, we'll run a quick colab.\n",
      "\n",
      "2. Open Colab Install Gradio and W&B\n",
      "\n",
      "   We'll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively.\n",
      "\n",
      "   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)\n",
      "\n",
      "   Install Gradio and Wandb at the top:\n",
      "\n",
      "```sh\n",
      "\n",
      "pip install gradio wandb\n",
      "```\n",
      "\n",
      "3. Finetune StyleGAN and W&B experiment tracking\n",
      "\n",
      "   This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here's the code you need for that:\n",
      "\n",
      "   ```python\n",
      "\n",
      "   alpha =  1.0\n",
      "   alpha = 1-alpha\n",
      "\n",
      "   preserve_color = True\n",
      "   num_iter = 100\n",
      "   log_interval = 50\n",
      "\n",
      "\n",
      "   samples = []\n",
      "   column_names = [\"Reference (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\n",
      "\n",
      "   wandb.init(project=\"JoJoGAN\")\n",
      "   config = wandb.config\n",
      "   config.num_iter = num_iter\n",
      "   config.preserve_color = preserve_color\n",
      "   wandb.log(\n",
      "   {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\n",
      "   step=0)\n",
      "\n",
      "   # load discriminator for perceptual loss\n",
      "   discriminator = Discriminator(1024, 2).eval().to(device)\n",
      "   ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
      "   discriminator.load_state_dict(ckpt[\"d\"], strict=False)\n",
      "\n",
      "   # reset generator\n",
      "   del generator\n",
      "   generator = deepcopy(original_generator)\n",
      "\n",
      "demo.launch(share=True)\n",
      "```\n",
      "\n",
      "6. Integrate Gradio into your W&B Dashboard\n",
      "\n",
      "   The last step—integrating your Gradio demo with your W&B dashboard—is just one extra line:\n",
      "\n",
      "```python\n",
      "\n",
      "demo.integrate(wandb=wandb)\n",
      "```\n",
      "\n",
      "    Once you call integrate, a demo will be created and you can integrate it into your dashboard or report\n",
      "\n",
      "    Outside of W&B with Web components, using the gradio-app tags allows anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:\n",
      "\n",
      "```html\n",
      "<gradio-app space=\"akhaliq/JoJoGAN\"> </gradio-app>\n",
      "```\n",
      "\n",
      "7. (Optional) Embed W&B plots in your Gradio App\n",
      "\n",
      "   It's also possible to embed W&B plots within Gradio apps. To do so, you can create a W&B Report of your plots and\n",
      "   embed them within your Gradio app within a `gr.HTML` block.\n",
      "\n",
      "   The Report will need to be public and you will need to wrap the URL within an iFrame like this:\n",
      "\n",
      "```python\n",
      "\n",
      "import gradio as gr\n",
      "\n",
      "def wandb_report(url):\n",
      "    iframe = f'<iframe src={url} style=\"border:none;height:1024px;width:100%\">'\n",
      "    return gr.HTML(iframe)\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    report_url = 'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx'\n",
      "    report = wandb_report(report_url)\n",
      "\n",
      "demo.launch(share=True)\n",
      "```\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "We hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! Thanks for making it to the end. To recap:\n",
      "\n",
      "- Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.\n",
      "\n",
      "- W&B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.\n",
      "\n",
      "- Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web.\n",
      "\n",
      "## How to contribute Gradio demos on HF spaces on the Wandb organization\n",
      "\n",
      "torch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\n",
      "\n",
      "files.download('your-model-name.pt')\n",
      "\n",
      "latent_dim = 512\n",
      "device=\"cuda\"\n",
      "model_path_s = hf_hub_download(repo_id=\"akhaliq/jojogan-stylegan2-ffhq-config-f\", filename=\"stylegan2-ffhq-config-f.pt\")\n",
      "original_generator = Generator(1024, latent_dim, 8, 2).to(device)\n",
      "ckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\n",
      "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
      "mean_latent = original_generator.mean_latent(10000)\n",
      "\n",
      "generator = deepcopy(original_generator)\n",
      "\n",
      "ckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\", map_location=lambda storage, loc: storage)\n",
      "generator.load_state_dict(ckpt[\"g\"], strict=False)\n",
      "generator.eval()\n",
      "\n",
      "plt.rcParams['figure.dpi'] = 150\n",
      "\n",
      "\n",
      "\n",
      "transform = transforms.Compose(\n",
      "    [\n",
      "        transforms.Resize((1024, 1024)),\n",
      "        transforms.ToTensor(),\n",
      "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
      "    ]\n",
      ")\n",
      "\n",
      "\n",
      "def inference(img):\n",
      "    img.save('out.jpg')\n",
      "    aligned_face = align_face('out.jpg')\n",
      "\n",
      "    my_w = e4e_projection(aligned_face, \"out.pt\", device).unsqueeze(0)\n",
      "    with torch.no_grad():\n",
      "        my_sample = generator(my_w, input_is_latent=True)\n",
      "\n",
      "\n",
      "    npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()\n",
      "    imageio.imwrite('filename.jpeg', npimage)\n",
      "    return 'filename.jpeg'\n",
      "````\n",
      "\n",
      "5. Build a Gradio Demo\n",
      "\n",
      "```python\n",
      "\n",
      "import gradio as gr\n",
      "\n",
      "title = \"JoJoGAN\"\n",
      "description = \"Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\n",
      "\n",
      "demo = gr.Interface(\n",
      "    inference,\n",
      "    gr.Image(type=\"pil\"),\n",
      "    gr.Image(type=\"file\"),\n",
      "    title=title,\n",
      "    description=description\n",
      ")\n",
      "\n",
      "demo.launch(share=True)\n",
      "```\n",
      "\n",
      "6. Integrate Gradio into your W&B Dashboard\n",
      "\n",
      "   The last step—integrating your Gradio demo with your W&B dashboard—is just one extra line:\n",
      "\n",
      "```python\n",
      "\n",
      "demo.integrate(wandb=wandb)\n",
      "```\n",
      "\n",
      "img = generator(in_latent, input_is_latent=True)\n",
      "\n",
      "    with torch.no_grad():\n",
      "        real_feat = discriminator(targets)\n",
      "    fake_feat = discriminator(img)\n",
      "\n",
      "    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n",
      "\n",
      "\n",
      "    wandb.log({\"loss\": loss}, step=idx)\n",
      "    if idx % log_interval == 0:\n",
      "        generator.eval()\n",
      "        my_sample = generator(my_w, input_is_latent=True)\n",
      "        generator.train()\n",
      "        my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\n",
      "        wandb.log(\n",
      "        {\"Current stylization\": [wandb.Image(my_sample)]},\n",
      "        step=idx)\n",
      "    table_data = [\n",
      "            wandb.Image(transforms.ToPILImage()(target_im)),\n",
      "            wandb.Image(img),\n",
      "            wandb.Image(my_sample),\n",
      "        ]\n",
      "    samples.append(table_data)\n",
      "\n",
      "    g_optim.zero_grad()\n",
      "    loss.backward()\n",
      "    g_optim.step()\n",
      "\n",
      "out_table = wandb.Table(data=samples, columns=column_names)\n",
      "wandb.log({\"Current Samples\": out_table})\n",
      "\n",
      "````\n",
      "\n",
      "4. Save, Download, and Load Model\n",
      "\n",
      "    Here's how to save and download your model.\n",
      "\n",
      "```python\n",
      "\n",
      "from PIL import Image\n",
      "import torch\n",
      "torch.backends.cudnn.benchmark = True\n",
      "from torchvision import transforms, utils\n",
      "from util import *\n",
      "import math\n",
      "import random\n",
      "import numpy as np\n",
      "from torch import nn, autograd, optim\n",
      "from torch.nn import functional as F\n",
      "from tqdm import tqdm\n",
      "import lpips\n",
      "from model import *\n",
      "from e4e_projection import projection as e4e_projection\n",
      "\n",
      "from copy import deepcopy\n",
      "import imageio\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import torchvision.transforms as transforms\n",
      "from argparse import Namespace\n",
      "from e4e.models.psp import pSp\n",
      "from util import *\n",
      "from huggingface_hub import hf_hub_download\n",
      "from google.colab import files\n",
      "\n",
      "torch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\n",
      "\n",
      "files.download('your-model-name.pt')\n",
      "-----------------LLm ANSWER------------\n",
      "According to the context, Weights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard.\n",
      "-----------------------------\n",
      "62\n",
      "huggingface/blog/blob/main/intel.md\n",
      "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n",
      "\n",
      "Optimum\n",
      "Optimum\n",
      "-----------------Context------------\n",
      "['huggingface/blog/blob/main/intel.md', 'huggingface/blog/blob/main/accelerate-library.md', 'huggingface/blog/blob/main/intel.md', 'huggingface/optimum/blob/main/docs/source/notebooks.md', 'huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md']\n",
      "“*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics.\n",
      "\n",
      "In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs.\n",
      "\n",
      "The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it.\n",
      "\n",
      "## Get Peak Transformers Performance with Optimum Intel\n",
      "[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.\n",
      "\n",
      "--\n",
      "title: \"Introducing 🤗 Accelerate\"\n",
      "thumbnail: /blog/assets/20_accelerate_library/accelerate_diff.png\n",
      "authors:\n",
      "- user: sgugger\n",
      "---\n",
      "\n",
      "# Introducing 🤗 Accelerate\n",
      "\n",
      "\n",
      "## 🤗 Accelerate\n",
      "\n",
      "Run your **raw** PyTorch training scripts on any kind of device.\n",
      "\n",
      "Most high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. 🤗 Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism.\n",
      "\n",
      "It provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups.\n",
      "\n",
      "### Easy integration!\n",
      "\n",
      "Let's first have a look at an example:\n",
      "\n",
      "```diff\n",
      "  import torch\n",
      "  import torch.nn.functional as F\n",
      "  from datasets import load_dataset\n",
      "+ from accelerate import Accelerator\n",
      "\n",
      "+ accelerator = Accelerator()\n",
      "- device = 'cpu'\n",
      "+ device = accelerator.device\n",
      "\n",
      "  model = torch.nn.Transformer().to(device)\n",
      "  optim = torch.optim.Adam(model.parameters())\n",
      "\n",
      "  dataset = load_dataset('my_dataset')\n",
      "  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
      "\n",
      "+ model, optim, data = accelerator.prepare(model, optim, data)\n",
      "\n",
      "  model.train()\n",
      "  for epoch in range(10):\n",
      "      for source, targets in data:\n",
      "          source = source.to(device)\n",
      "          targets = targets.to(device)\n",
      "\n",
      "          optimizer.zero_grad()\n",
      "\n",
      "          output = model(source)\n",
      "          loss = F.cross_entropy(output, targets)\n",
      "\n",
      "-         loss.backward()\n",
      "+         accelerator.backward(loss)\n",
      "\n",
      "          optimizer.step()\n",
      "```\n",
      "\n",
      "--\n",
      "title: \"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\"\n",
      "thumbnail: /blog/assets/80_intel/01.png\n",
      "authors:\n",
      "- user: juliensimon\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "![image](assets/80_intel/01.png)\n",
      "\n",
      "The mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption.\n",
      "\n",
      "Today, we're excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware).  Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers.\n",
      "\n",
      "Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.\n",
      "\n",
      "Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.\n",
      "\n",
      "!--Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# 🤗 Optimum notebooks\n",
      "\n",
      "You can find here a list of the notebooks associated with each accelerator in 🤗 Optimum.\n",
      "\n",
      "## Optimum Habana\n",
      "\n",
      "**Inference-only backend**s:\n",
      "* `dynamo.optimize(\"ofi\")` -  Uses Torchscript optimize_for_inference.  [Read more](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)\n",
      "* `dynamo.optimize(\"fx2trt\")` -  Uses NVIDIA TensorRT for inference optimizations.  [Read more](https://pytorch.org/TensorRT/tutorials/getting_started_with_fx_path.html)\n",
      "* `dynamo.optimize(\"onnxrt\")` -  Uses ONNXRT for inference on CPU/GPU.  [Read more](https://onnxruntime.ai/)\n",
      "* `dynamo.optimize(\"ipex\")` -  Uses IPEX for inference on CPU.  [Read more](https://github.com/intel/intel-extension-for-pytorch)\n",
      "\n",
      "For an example of using `torch.compile` with 🤗 Transformers, check out this [blog post on fine-tuning a BERT model for Text Classification using the newest PyTorch 2.0 features](https://www.philschmid.de/getting-started-pytorch-2-0-transformers)\n",
      "\n",
      "## Using 🤗 Accelerate\n",
      "\n",
      "With [🤗 Accelerate](https://huggingface.co/docs/accelerate/index) you can use the above methods while gaining full \n",
      "control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. \n",
      "\n",
      "Suppose you have combined the methods in the [`TrainingArguments`] like so:\n",
      "\n",
      "```py\n",
      "training_args = TrainingArguments(\n",
      "    per_device_train_batch_size=1,\n",
      "    gradient_accumulation_steps=4,\n",
      "    gradient_checkpointing=True,\n",
      "    fp16=True,\n",
      "    **default_args,\n",
      ")\n",
      "```\n",
      "\n",
      "The full example training loop with 🤗 Accelerate is only a handful of lines of code long:\n",
      "\n",
      "```py\n",
      "from accelerate import Accelerator\n",
      "from torch.utils.data.dataloader import DataLoader\n",
      "\n",
      "dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n",
      "\n",
      "if training_args.gradient_checkpointing:\n",
      "    model.gradient_checkpointing_enable()\n",
      "\n",
      "accelerator = Accelerator(fp16=training_args.fp16)\n",
      "model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n",
      "-----------------LLm ANSWER------------\n",
      "The name of the open-source library created by Hugging Face to simplify Transformer acceleration is \"Optimum\".\n",
      "-----------------------------\n",
      "63\n",
      "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md\n",
      "What parameter is used to ensure that elements in a row have the same height in Gradio?\n",
      "\n",
      "equal_height\n",
      "equal_height\n",
      "-----------------Context------------\n",
      "['huggingface/course/blob/main/chapters/en/chapter9/7.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/7.mdx', 'gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md', 'huggingface/course/blob/main/chapters/en/chapter9/7.mdx', 'huggingface/course/blob/main/chapters/en/chapter9/7.mdx']\n",
      "Here's what you should keep in mind: any components created under a `Column` (this is also the default) will be laid out vertically. Any component created under a `Row` will be laid out horizontally, similar to the [flexbox model in web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\n",
      "\n",
      "Finally, you can also create tabs for your demo by using the `with gradio.Tabs()` context manager. Within this context, you can create multiple tabs by specifying `with gradio.TabItem(name_of_tab):` children. Any component created inside of a `with gradio.TabItem(name_of_tab):` context appears in that tab.\n",
      "\n",
      "Now let's add a `flip_image()` function to our demo and add a new tab that flips images. Below is an example with 2 tabs and also uses a Row:\n",
      "\n",
      "```py\n",
      "import numpy as np\n",
      "import gradio as gr\n",
      "\n",
      "demo = gr.Blocks()\n",
      "\n",
      "\n",
      "def flip_text(x):\n",
      "    return x[::-1]\n",
      "\n",
      "\n",
      "def flip_image(x):\n",
      "    return np.fliplr(x)\n",
      "\n",
      "\n",
      "with demo:\n",
      "    gr.Markdown(\"Flip text or image files using this demo.\")\n",
      "    with gr.Tabs():\n",
      "        with gr.TabItem(\"Flip Text\"):\n",
      "            with gr.Row():\n",
      "                text_input = gr.Textbox()\n",
      "                text_output = gr.Textbox()\n",
      "            text_button = gr.Button(\"Flip\")\n",
      "        with gr.TabItem(\"Flip Image\"):\n",
      "            with gr.Row():\n",
      "                image_input = gr.Image()\n",
      "                image_output = gr.Image()\n",
      "            image_button = gr.Button(\"Flip\")\n",
      "\n",
      "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
      "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
      "\n",
      "demo.launch()\n",
      "```\n",
      "\n",
      "We will explore all of these concepts below.\n",
      "\n",
      "### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\n",
      "\n",
      "After you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook.\n",
      "\n",
      "```py\n",
      "import gradio as gr\n",
      "\n",
      "\n",
      "def flip_text(x):\n",
      "    return x[::-1]\n",
      "\n",
      "\n",
      "demo = gr.Blocks()\n",
      "\n",
      "with demo:\n",
      "    gr.Markdown(\n",
      "        \"\"\"\n",
      "    # Flip Text!\n",
      "    Start typing below to see the output.\n",
      "    \"\"\"\n",
      "    )\n",
      "    input = gr.Textbox(placeholder=\"Flip this text\")\n",
      "    output = gr.Textbox()\n",
      "\n",
      "    input.change(fn=flip_text, inputs=input, outputs=output)\n",
      "\n",
      "demo.launch()\n",
      "```\n",
      "\n",
      "<iframe src=\"https://course-demos-flip-text.hf.space\" frameBorder=\"0\" height=\"400\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n",
      "\n",
      "This simple example above introduces 4 concepts that underlie Blocks:\n",
      "\n",
      "1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\n",
      "<Tip>\n",
      "🙋If you're not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that 🤗\n",
      "</Tip>\n",
      "The order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)\n",
      "\n",
      "控制布局 (Controlling Layout)\n",
      "\n",
      "默认情况下，块中的组件是垂直排列的。让我们看看如何重新排列组件。在幕后，这种布局结构使用了[Web 开发的 flexbox 模型](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox)。\n",
      "\n",
      "## Row 行\n",
      "\n",
      "`with gr.Row` 下的元素将水平显示。例如，要并排显示两个按钮：\n",
      "\n",
      "```python\n",
      "with gr.Blocks() as demo:\n",
      "    with gr.Row():\n",
      "        btn1 = gr.Button(\"按钮1\")\n",
      "        btn2 = gr.Button(\"按钮2\")\n",
      "```\n",
      "\n",
      "要使行中的每个元素具有相同的高度，请使用 `style` 方法的 `equal_height` 参数。\n",
      "\n",
      "```python\n",
      "with gr.Blocks() as demo:\n",
      "    with gr.Row(equal_height=True):\n",
      "        textbox = gr.Textbox()\n",
      "        btn2 = gr.Button(\"按钮2\")\n",
      "```\n",
      "\n",
      "可以通过每个组件中存在的 `scale` 和 `min_width` 参数来控制行中元素的宽度。\n",
      "\n",
      "- `scale` 是一个整数，定义了元素在行中的占用空间。如果将 scale 设置为 `0`，则元素不会扩展占用空间。如果将 scale 设置为 `1` 或更大，则元素将扩展。行中的多个元素将按比例扩展。在下面的示例中，`btn1` 将比 `btn2` 扩展两倍，而 `btn0` 将根本不会扩展：\n",
      "\n",
      "```python\n",
      "with gr.Blocks() as demo:\n",
      "    with gr.Row():\n",
      "        btn0 = gr.Button(\"按钮0\", scale=0)\n",
      "        btn1 = gr.Button(\"按钮1\", scale=1)\n",
      "        btn2 = gr.Button(\"按钮2\", scale=2)\n",
      "```\n",
      "\n",
      "- `min_width` 将设置元素的最小宽度。如果没有足够的空间满足所有的 `min_width` 值，行将换行。\n",
      "\n",
      "在[文档](https://gradio.app/docs/#row)中了解有关行的更多信息。\n",
      "\n",
      "## 列和嵌套 (Columns and Nesting)\n",
      "\n",
      "列中的组件将垂直放置在一起。由于默认布局对于块应用程序来说是垂直布局，因此为了有用，列通常嵌套在行中。例如：\n",
      "\n",
      "$code_rows_and_columns\n",
      "$demo_rows_and_columns\n",
      "\n",
      "查看第一列如何垂直排列两个文本框。第二列垂直排列图像和按钮。注意两列的相对宽度由 `scale` 参数设置。具有两倍 `scale` 值的列占据两倍的宽度。\n",
      "\n",
      "在[文档](https://gradio.app/docs/#column)中了解有关列的更多信息。\n",
      "\n",
      "## 选项卡和手风琴 (Tabs and Accordions)\n",
      "\n",
      "您还可以使用 `with gr.Tab('tab_name'):` 语句创建选项卡。在 `with gr.Tab('tab_name'):` 上下文中创建的任何组件都将显示在该选项卡中。连续的 Tab 子句被分组在一起，以便一次只能选择一个选项卡，并且只显示该选项卡上下文中的组件。\n",
      "\n",
      "例如：\n",
      "\n",
      "$code_blocks_flipper\n",
      "$demo_blocks_flipper\n",
      "\n",
      "还请注意本示例中的 `gr.Accordion('label')`。手风琴是一种可以切换打开或关闭的布局。与 `Tabs` 一样，它是可以选择性隐藏或显示内容的布局元素。在 `with gr.Accordion('label'):` 内定义的任何组件在单击手风琴的切换图标时都会被隐藏或显示。\n",
      "\n",
      "在文档中了解有关[Tabs](https://gradio.app/docs/#tab)和[Accordions](https://gradio.app/docs/#accordion)的更多信息。\n",
      "\n",
      "## 可见性 (Visibility)\n",
      "\n",
      "组件和布局元素都有一个 `visible` 参数，可以在初始时设置，并使用 `gr.update()` 进行更新。在 Column 上设置 `gr.update(visible=...)` 可用于显示或隐藏一组组件。\n",
      "\n",
      "$code_blocks_form\n",
      "$demo_blocks_form\n",
      "\n",
      "### Updating Component Properties[[updating-component-properties]]\n",
      "\n",
      "So far, we have seen how to create events to update the value of another component. But what happens if you want to change other properties of a component, like the visibility of a textbox or the choices in a radio button group? You can do this by returning a component class's `update()` method instead of a regular return value from your function.\n",
      "\n",
      "This is most easily illustrated with an example:\n",
      "\n",
      "```py\n",
      "import gradio as gr\n",
      "\n",
      "\n",
      "def change_textbox(choice):\n",
      "    if choice == \"short\":\n",
      "        return gr.Textbox.update(lines=2, visible=True)\n",
      "    elif choice == \"long\":\n",
      "        return gr.Textbox.update(lines=8, visible=True)\n",
      "    else:\n",
      "        return gr.Textbox.update(visible=False)\n",
      "\n",
      "\n",
      "with gr.Blocks() as block:\n",
      "    radio = gr.Radio(\n",
      "        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\n",
      "    )\n",
      "    text = gr.Textbox(lines=2, interactive=True)\n",
      "\n",
      "    radio.change(fn=change_textbox, inputs=radio, outputs=text)\n",
      "    block.launch()\n",
      "```\n",
      "\n",
      "<iframe src=\"https://course-demos-blocks-update-component-properties.hf.space\" frameBorder=\"0\" height=\"300\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n",
      "\n",
      "We just explored all the core concepts of `Blocks`! Just like with `Interfaces`, you can create cool demos that can be shared by using `share=True` in the `launch()` method or deployed on [Hugging Face Spaces](https://huggingface.co/spaces).\n",
      "\n",
      "### Creating multi-step demos[[creating-multi-step-demos]]\n",
      "\n",
      "In some cases, you might want a _multi-step demo_, in which you reuse the output of one function as the input to the next. This is really easy to do with `Blocks`, as you can use a component for the input of one event trigger but the output of another. Take a look at the text component in the example below, its value is the result of a speech-to-text model, but also gets passed into a sentiment analysis model:\n",
      "\n",
      "```py\n",
      "from transformers import pipeline\n",
      "\n",
      "import gradio as gr\n",
      "\n",
      "asr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\n",
      "classifier = pipeline(\"text-classification\")\n",
      "\n",
      "\n",
      "def speech_to_text(speech):\n",
      "    text = asr(speech)[\"text\"]\n",
      "    return text\n",
      "\n",
      "\n",
      "def text_to_sentiment(text):\n",
      "    return classifier(text)[0][\"label\"]\n",
      "\n",
      "\n",
      "demo = gr.Blocks()\n",
      "\n",
      "with demo:\n",
      "    audio_file = gr.Audio(type=\"filepath\")\n",
      "    text = gr.Textbox()\n",
      "    label = gr.Label()\n",
      "\n",
      "    b1 = gr.Button(\"Recognize Speech\")\n",
      "    b2 = gr.Button(\"Classify Sentiment\")\n",
      "\n",
      "    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n",
      "    b2.click(text_to_sentiment, inputs=text, outputs=label)\n",
      "\n",
      "demo.launch()\n",
      "```\n",
      "\n",
      "<iframe src=\"https://course-demos-blocks-multi-step.hf.space\" frameBorder=\"0\" height=\"600\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n",
      "\n",
      "### Updating Component Properties[[updating-component-properties]]\n",
      "-----------------LLm ANSWER------------\n",
      "According to the text, the `equal_height` parameter is used to ensure that elements in a row have the same height in Gradio.\n",
      "-----------------------------\n",
      "64\n",
      "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md\n",
      "What is the command to install the latest version of Optimum with OpenVINO support?\n",
      "\n",
      "pip install --upgrade-strategy eager optimum[\"openvino\"]\n",
      "pip install --upgrade-strategy eager optimum[\"openvino\"]\n",
      "-----------------Context------------\n",
      "['huggingface/optimum/blob/main/docs/source/notebooks.md', 'huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md', 'huggingface/optimum/blob/main/docs/source/notebooks.md', 'huggingface/optimum/blob/main/docs/source/notebooks.md', 'huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md']\n",
      "## Optimum Intel\n",
      "\n",
      "### OpenVINO\n",
      "\n",
      "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n",
      "the License. You may obtain a copy of the License at\n",
      "\n",
      "http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n",
      "an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
      "specific language governing permissions and limitations under the License.\n",
      "-->\n",
      "\n",
      "# OpenVINO\n",
      "\n",
      "🤗 [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\n",
      "\n",
      "You'll need to install 🤗 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n",
      "\n",
      "```bash\n",
      "pip install --upgrade-strategy eager optimum[\"openvino\"]\n",
      "```\n",
      "\n",
      "This guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n",
      "\n",
      "## Stable Diffusion\n",
      "\n",
      "To load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n",
      "\n",
      "```python\n",
      "from optimum.intel import OVStableDiffusionPipeline\n",
      "\n",
      "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      "pipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\n",
      "prompt = \"sailing ship in storm by Rembrandt\"\n",
      "image = pipeline(prompt).images[0]\n",
      "\n",
      "# Don't forget to save the exported model\n",
      "pipeline.save_pretrained(\"openvino-sd-v1-5\")\n",
      "```\n",
      "\n",
      "## Optimum ONNX Runtime\n",
      "\n",
      "| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|\n",
      "| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb) | Show how to apply post-training quantization on a question answering model using [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with OpenVINO| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)|\n",
      "\n",
      "# Don't forget to save the exported model\n",
      "pipeline.save_pretrained(\"openvino-sd-v1-5\")\n",
      "```\n",
      "\n",
      "To further speed-up inference, statically reshape the model. If you change any parameters such as the outputs height or width, you’ll need to statically reshape your model again.\n",
      "\n",
      "```python\n",
      "# Define the shapes related to the inputs and desired outputs\n",
      "batch_size, num_images, height, width = 1, 1, 512, 512\n",
      "\n",
      "# Statically reshape the model\n",
      "pipeline.reshape(batch_size, height, width, num_images)\n",
      "# Compile the model before inference\n",
      "pipeline.compile()\n",
      "\n",
      "image = pipeline(\n",
      "    prompt,\n",
      "    height=height,\n",
      "    width=width,\n",
      "    num_images_per_prompt=num_images,\n",
      ").images[0]\n",
      "```\n",
      "<div class=\"flex justify-center\">\n",
      "    <img src=\"https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png\">\n",
      "</div>\n",
      "\n",
      "You can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting.\n",
      "\n",
      "## Stable Diffusion XL\n",
      "\n",
      "To load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\n",
      "\n",
      "```python\n",
      "from optimum.intel import OVStableDiffusionXLPipeline\n",
      "\n",
      "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
      "pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\n",
      "prompt = \"sailing ship in storm by Rembrandt\"\n",
      "image = pipeline(prompt).images[0]\n",
      "```\n",
      "\n",
      "To further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section.\n",
      "\n",
      "You can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image.\n",
      "-----------------LLm ANSWER------------\n",
      "The command to install the latest version of Optimum with OpenVINO support is:\n",
      "\n",
      "```\n",
      "pip install --upgrade-strategy eager optimum[\"openvino\"]\n",
      "```\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, row in fl.iterrows():\n",
    "    print(idx)\n",
    "    print(row[\"source_doc\"])\n",
    "    print(row[\"question\"])\n",
    "    print(row[\"answer\"])\n",
    "    print(row[\"answer\"])\n",
    "    print(\"-----------------Context------------\")\n",
    "    print(row[\"Retrieved_Doc_IDs\"])\n",
    "    print(row[\"Retrieved_Contexts\"])\n",
    "    print(\"-----------------LLm ANSWER------------\")\n",
    "    print(row[\"LLM_Answer\"])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvembed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
