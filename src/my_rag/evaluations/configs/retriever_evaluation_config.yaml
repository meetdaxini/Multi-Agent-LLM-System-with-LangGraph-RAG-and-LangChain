datasets:
  - name: "HuggingFace QA Dataset"
    type: "parquet"
    path: "hf://datasets/m-ric/huggingface_doc_qa_eval/data/train-00000-of-00001.parquet"
    context_field: "context"
    question_field: "question"
    doc_id_field: "source_doc"
    answer_field: "answer"
  - name: "PubMed filtered Dataset"
    type: "csv_pdf"
    path: "/home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/src/data_mining/filtered_dataset.csv"
    context_field: "context"
    question_field: "Question"
    doc_id_field: "PDF Reference"
    answer_field: "Ideal Answer"
    pdf_dir: "/home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/filtered_dataset_csv_pdfs"

models:
  - name: "sentence-transformers/all-MiniLM-L6-v2"
    batch_size: 100
    instruction: "Instruct: Represent this passage to effectively represent its content for retrieval in response to relevant questions. Focus on capturing key details and context.\nPassage:"
    query_instruction: "Instruct: Represent this query to retrieve the most relevant passages capable of answering it. Ensure clarity and specificity of the question.\nQuery:"
  - name: "mixedbread-ai/mxbai-embed-large-v1"
    batch_size: 100
    instruction: "Instruct: Represent this passage to effectively represent its content for retrieval in response to relevant questions. Focus on capturing key details and context.\nPassage:"
    query_instruction: "Instruct: Represent this query to retrieve the most relevant passages capable of answering it. Ensure clarity and specificity of the question.\nQuery:"
  - name: "nvidia/NV-Embed-v2"
    batch_size: 2
    instruction: "Instruct: Represent this passage to effectively represent its content for retrieval in response to relevant questions. Focus on capturing key details and context.\nPassage:"
    query_instruction: "Instruct: Represent this query to retrieve the most relevant passages capable of answering it. Ensure clarity and specificity of the question.\nQuery:"
    model_kwargs:
      trust_remote_code: true
      load_in_8bit: true
      max_length: 32768
  - name: "dunzhang/stella_en_1.5B_v5"
    batch_size: 2
    instruction: "Instruct: Represent this passage to effectively represent its content for retrieval in response to relevant questions. Focus on capturing key details and context.\nPassage:"
    query_instruction: "Instruct: Represent this query to retrieve the most relevant passages capable of answering it. Ensure clarity and specificity of the question.\nQuery:"
    model_kwargs:
      trust_remote_code: true
  - name: "dunzhang/stella_en_1.5B_v5"
    batch_size: 2
    instruction: "Instruct: Represent this passage to effectively represent its content for retrieval in response to relevant questions. Focus on capturing key details and context.\nPassage:"
    query_instruction: "Instruct: Represent this query to retrieve the most relevant passages capable of answering it. Ensure clarity and specificity of the question.\nQuery:"
    model_kwargs:
      trust_remote_code: true
      load_in_8bit: true
  - name: "amazon.titan-embed-text-v2:0"
    instruction: "Instruct: Represent this passage to effectively represent its content for retrieval in response to relevant questions. Focus on capturing key details and context.\nPassage:"
    query_instruction: "Instruct: Represent this query to retrieve the most relevant passages capable of answering it. Ensure clarity and specificity of the question.\nQuery:"
    model_kwargs:
      aws: true
      aws_creds_file: /home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/config/config.ini
      aws_config_name: BedRock_LLM_API
  - name: "sentence-transformers/all-MiniLM-L6-v2"
    batch_size: 100
  - name: "mixedbread-ai/mxbai-embed-large-v1"
    batch_size: 100
  - name: "nvidia/NV-Embed-v2"
    batch_size: 2
    model_kwargs:
      trust_remote_code: true
      load_in_8bit: true
      max_length: 32768
  - name: "dunzhang/stella_en_1.5B_v5"
    batch_size: 2
    model_kwargs:
      trust_remote_code: true
  - name: "amazon.titan-embed-text-v2:0"
    model_kwargs:
      aws: true
      aws_creds_file: /home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/config/config.ini
      aws_config_name: BedRock_LLM_API



max_k: 10
chunk_size: 4000
chunk_overlap: 200
output_path: "results/retriever_evaluation_results.xlsx"