{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(row):\n",
    "    \"\"\"Calculate accuracy based on the overlap between actual and retrieved documents.\"\"\"\n",
    "    actual_docs = eval(row[\"document_id\"])\n",
    "    retrieved_docs = eval(row[\"retrieved_document_ids\"])\n",
    "    overlap = len(set(actual_docs) & set(retrieved_docs))\n",
    "    accuracy = overlap / len(actual_docs)\n",
    "    return accuracy\n",
    "\n",
    "def process_file(filepath):\n",
    "    \"\"\"Load the file and calculate accuracy.\"\"\"\n",
    "    data = pd.read_excel(filepath)\n",
    "    data[\"accuracy\"] = data.apply(calculate_accuracy, axis=1)\n",
    "    avg_accuracy = data[\"accuracy\"].mean()\n",
    "    return avg_accuracy\n",
    "\n",
    "def extract_components_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract embedding, LLM, and dataset names from the file name using regex.\n",
    "    \"\"\"\n",
    "    embedding_pattern = r\"embedder_(.*?)_llm_\"\n",
    "    llm_pattern = r\"_llm_(.*?)_dataset_\"\n",
    "    dataset_pattern = r\"_dataset_(.*?)\\.xlsx\"\n",
    "\n",
    "    # Extract components using regex\n",
    "    embedding_match = re.search(embedding_pattern, filename)\n",
    "    llm_match = re.search(llm_pattern, filename)\n",
    "    dataset_match = re.search(dataset_pattern, filename)\n",
    "\n",
    "    # Get the matched groups\n",
    "    embedding = embedding_match.group(1) if embedding_match else None\n",
    "    llm = llm_match.group(1) if llm_match else None\n",
    "    dataset = dataset_match.group(1) if dataset_match else None\n",
    "\n",
    "    return embedding, llm, dataset\n",
    "\n",
    "# def process_directory(directory):\n",
    "#     \"\"\"Process all files in the directory and calculate accuracy.\"\"\"\n",
    "#     results = []\n",
    "#     for file in os.listdir(directory):\n",
    "#         if file.startswith(\n",
    "#             \"rag_evaluations_with_reank\"\n",
    "#         ) and file.endswith(\".xlsx\"):\n",
    "#             filepath = os.path.join(directory, file)\n",
    "#             embedding, llm, dataset = extract_components_from_filename(file)\n",
    "#             avg_accuracy = process_file(filepath)\n",
    "#             results.append(\n",
    "#                 {\n",
    "#                     \"filename\": file,\n",
    "#                     \"embedding_model\": embedding,\n",
    "#                     \"llm_model\": llm,\n",
    "#                     \"dataset\": dataset,\n",
    "#                     \"avg_accuracy\": avg_accuracy,\n",
    "#                 }\n",
    "#             )\n",
    "#     return results\n",
    "\n",
    "# directory_path = \"/home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/src/my_rag/evaluations/results\"\n",
    "# results = process_directory(directory_path)\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(\"accuracy_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read(\n",
    "    f\"/home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/config/config.ini\"\n",
    ")\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=config[\"BedRock_LLM_API\"][\"aws_access_key_id\"],\n",
    "    aws_secret_access_key=config[\"BedRock_LLM_API\"][\"aws_secret_access_key\"],\n",
    "    aws_session_token=config[\"BedRock_LLM_API\"][\"aws_session_token\"],\n",
    ")\n",
    "bedrock_client = session.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(question, ideal_answer, llm_generated_answer):\n",
    "    \"\"\"Send LLM answers to Claude API for evaluation.\"\"\"\n",
    "    content = f\"\"\"\n",
    "    Question: {question}\n",
    "\n",
    "    Compare and evaluate the following answer for correctness:\n",
    "    \n",
    "    Ideal Answer: {ideal_answer}\n",
    "    LLM Generated Answer: {llm_generated_answer}\n",
    "\n",
    "    Classification Criteria:\n",
    "    1. CORRECT: Contains all specific information that directly answers the question\n",
    "    2. PARTIALLY CORRECT: Contains only a subset of the required information, but all included information is accurate\n",
    "    3. INCORRECT: Either fails to address the question, misses critical details, or contains inaccurate information\n",
    "\n",
    "    IMPORTANT: Respond with exactly one word - either \"correct\", \"partially\" or \"incorrect\".\n",
    "    \"\"\"\n",
    "\n",
    "    body_content = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 1,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
    "    }\n",
    "    # time.sleep(5)\n",
    "    try:\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id, body=json.dumps(body_content)\n",
    "        )\n",
    "        response_body = response[\"body\"].read().decode()\n",
    "        return json.loads(response_body)[\"content\"][0][\"text\"].strip().lower()\n",
    "    except ClientError as e:\n",
    "        print(f\"Error invoking Claude API: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def classify_response(text):\n",
    "    \"\"\"Classify the Claude response as correct, partially correct, or incorrect.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    if \"incorrect\" in text_lower:\n",
    "        return \"incorrect\"\n",
    "    elif \"partially\" in text_lower:\n",
    "        return \"partially_correct\"\n",
    "    elif \"correct\" in text_lower:\n",
    "        return \"correct\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# def process_files(directory):\n",
    "#     \"\"\"Process all files, evaluate answers, and save results.\"\"\"\n",
    "#     results = []\n",
    "#     for file in os.listdir(directory):\n",
    "#         if file.startswith(\n",
    "#             \"rag_evaluations\"\n",
    "#         ) and file.endswith(\".xlsx\"):\n",
    "#             filepath = os.path.join(directory, file)\n",
    "#             data = pd.read_excel(filepath)\n",
    "\n",
    "#             embedding, llm, dataset = extract_components_from_filename(file)\n",
    "#             evaluations = []\n",
    "\n",
    "#             for _, row in data.iterrows():\n",
    "#                 question = row[\"question\"]\n",
    "#                 ideal_answer = row[\"ideal_answer\"]\n",
    "#                 llm_generated_answer = row[\"llm_generated_answer\"]\n",
    "#                 response = evaluate_answer(question, ideal_answer, llm_generated_answer)\n",
    "#                 classification = classify_response(response) if response else \"error\"\n",
    "#                 evaluations.append(\n",
    "#                     {\n",
    "#                         \"embedding_model\": embedding,\n",
    "#                         \"reranking_model\": \"colbert\" if \"_with_reank\" in file else \"\",\n",
    "#                         \"llm_model\": llm,\n",
    "#                         \"dataset\": dataset,\n",
    "#                         \"question\": question,\n",
    "#                         \"ideal_answer\": ideal_answer,\n",
    "#                         \"llm_answer\": llm_generated_answer,\n",
    "#                         \"classification_and_explanation_text_by_claude\": response,\n",
    "#                         \"classification\": classification,\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             # Append evaluations to results\n",
    "#             results.extend(evaluations)\n",
    "\n",
    "#     # Save results to CSV\n",
    "#     output_file = os.path.join(directory, \"evaluated_results.csv\")\n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     results_df.to_csv(output_file, index=False)\n",
    "#     print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(directory):\n",
    "    \"\"\"Process all files, evaluate answers, and save results.\"\"\"\n",
    "    output_file = os.path.join(directory, \"classify_raptor_rag_eval_results.csv\")\n",
    "    processed_models = {}\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "        print(existing_df)\n",
    "        existing_df = existing_df.fillna(\"\")\n",
    "        processed_models = {\n",
    "            (embedding, llm, reranking_model, dataset)\n",
    "             for (\n",
    "                embedding,\n",
    "                llm,\n",
    "                reranking_model,\n",
    "                dataset,\n",
    "            ), group in existing_df.groupby(\n",
    "                [\"embedding_model\", \"llm_model\", \"reranking_model\", \"dataset\"]\n",
    "            )\n",
    "        }\n",
    "    for file in os.listdir(directory):\n",
    "        if (\n",
    "            file.startswith(\"raptor_rag_evaluations\")\n",
    "            and file.endswith(\".xlsx\")\n",
    "        ):\n",
    "            print(file)\n",
    "            filepath = os.path.join(directory, file)\n",
    "            data = pd.read_excel(filepath)\n",
    "\n",
    "            embedding, llm, dataset = extract_components_from_filename(file)\n",
    "            reranking_model =  \"colbert\" if \"_with_reank\" in file else \"\"\n",
    "            if (\n",
    "                os.path.exists(output_file) and (embedding, llm, reranking_model, dataset)\n",
    "                in processed_models\n",
    "            ):\n",
    "                # print((embedding, llm, reranking_model, dataset))\n",
    "                # print((embedding, llm, reranking_model, dataset))\n",
    "                continue\n",
    "            evaluations = []\n",
    "            print(\"processing\")\n",
    "            print((embedding, llm, reranking_model, dataset))\n",
    "\n",
    "            for _, row in data.iterrows():\n",
    "                question = row[\"question\"]\n",
    "                ideal_answer = row[\"ideal_answer\"]\n",
    "                llm_generated_answer = row[\"llm_generated_answer\"]\n",
    "                response = evaluate_answer(question, ideal_answer, llm_generated_answer)\n",
    "                classification = classify_response(response) if response else \"error\"\n",
    "                evaluations.append(\n",
    "                    {\n",
    "                        \"embedding_model\": embedding,\n",
    "                        \"reranking_model\": reranking_model,\n",
    "                        \"llm_model\": llm,\n",
    "                        \"dataset\": dataset,\n",
    "                        \"question\": question,\n",
    "                        \"ideal_answer\": ideal_answer,\n",
    "                        \"llm_answer\": llm_generated_answer,\n",
    "                        \"classification_and_explanation_text_by_claude\": response,\n",
    "                        \"classification\": classification,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Save or append results to CSV\n",
    "            output_file = os.path.join(\n",
    "                directory, \"classify_raptor_rag_eval_results.csv\"\n",
    "            )\n",
    "            if os.path.exists(output_file):\n",
    "                existing_df = pd.read_csv(output_file)\n",
    "                existing_df = existing_df.fillna(\"\")\n",
    "                new_results_df = pd.DataFrame(evaluations)\n",
    "                new_results_df = new_results_df.fillna(\"\")\n",
    "                combined_df = pd.concat([existing_df, new_results_df], ignore_index=True)\n",
    "                combined_df.drop_duplicates(\n",
    "                    subset=[\n",
    "                        \"embedding_model\",\n",
    "                        \"llm_model\",\n",
    "                        \"reranking_model\",\n",
    "                        \"dataset\",\n",
    "                        \"question\",\n",
    "                        \"ideal_answer\",\n",
    "                        \"llm_answer\",\n",
    "                    ],\n",
    "                    inplace=True,\n",
    "                )\n",
    "                combined_df.to_csv(output_file, index=False)\n",
    "            else:\n",
    "                results_df = pd.DataFrame(evaluations)\n",
    "                results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       embedding_model  reranking_model  \\\n",
      "0   mixedbread-ai_mxbai-embed-large-v1              NaN   \n",
      "1   mixedbread-ai_mxbai-embed-large-v1              NaN   \n",
      "2   mixedbread-ai_mxbai-embed-large-v1              NaN   \n",
      "3   mixedbread-ai_mxbai-embed-large-v1              NaN   \n",
      "4   mixedbread-ai_mxbai-embed-large-v1              NaN   \n",
      "..                                 ...              ...   \n",
      "86          dunzhang_stella_en_1.5B_v5              NaN   \n",
      "87          dunzhang_stella_en_1.5B_v5              NaN   \n",
      "88          dunzhang_stella_en_1.5B_v5              NaN   \n",
      "89          dunzhang_stella_en_1.5B_v5              NaN   \n",
      "90          dunzhang_stella_en_1.5B_v5              NaN   \n",
      "\n",
      "                                    llm_model                  dataset  \\\n",
      "0   anthropic.claude-3-5-sonnet-20240620-v1_0  PubMed filtered Dataset   \n",
      "1   anthropic.claude-3-5-sonnet-20240620-v1_0  PubMed filtered Dataset   \n",
      "2   anthropic.claude-3-5-sonnet-20240620-v1_0  PubMed filtered Dataset   \n",
      "3   anthropic.claude-3-5-sonnet-20240620-v1_0  PubMed filtered Dataset   \n",
      "4   anthropic.claude-3-5-sonnet-20240620-v1_0  PubMed filtered Dataset   \n",
      "..                                        ...                      ...   \n",
      "86  anthropic.claude-3-5-sonnet-20240620-v1_0   HuggingFace QA Dataset   \n",
      "87  anthropic.claude-3-5-sonnet-20240620-v1_0   HuggingFace QA Dataset   \n",
      "88  anthropic.claude-3-5-sonnet-20240620-v1_0   HuggingFace QA Dataset   \n",
      "89  anthropic.claude-3-5-sonnet-20240620-v1_0   HuggingFace QA Dataset   \n",
      "90  anthropic.claude-3-5-sonnet-20240620-v1_0   HuggingFace QA Dataset   \n",
      "\n",
      "                                             question  \\\n",
      "0                   Are long non coding RNAs spliced?   \n",
      "1        Has Denosumab (Prolia) been approved by FDA?   \n",
      "2   Is Hirschsprung disease a mendelian or a multi...   \n",
      "3                   Is RANKL secreted from the cells?   \n",
      "4   Is the monoclonal antibody Trastuzumab (Hercep...   \n",
      "..                                                ...   \n",
      "86  What type of license is the HuggingFace Team's...   \n",
      "87  What type of security certification does Huggi...   \n",
      "88  What type of test should typically accompany a...   \n",
      "89  Where can the full code for the Stable Diffusi...   \n",
      "90  Where can you access the logs of your Endpoint...   \n",
      "\n",
      "                                         ideal_answer  \\\n",
      "0   Long non coding RNAs appear to be spliced thro...   \n",
      "1     Yes, Denosumab was approved by the FDA in 2010.   \n",
      "2   Coding sequence mutations in RET, GDNF, EDNRB,...   \n",
      "3   Receptor activator of nuclear factor ÎºB ligand...   \n",
      "4   Although is still controversial, Trastuzumab (...   \n",
      "..                                                ...   \n",
      "86                        Apache License, Version 2.0   \n",
      "87                              SOC2 Type 2 certified   \n",
      "88                                  Dynamic code test   \n",
      "89  https://hf.co/spaces/stabilityai/stable-diffus...   \n",
      "90  In the \"Logs\" tab of your Endpoint through the...   \n",
      "\n",
      "                                           llm_answer  \\\n",
      "0   Based on the information provided in the paper...   \n",
      "1   Yes, Denosumab (brand name Prolia) has been ap...   \n",
      "2   Based on the information provided in the paper...   \n",
      "3   Based on the information provided in the summa...   \n",
      "4   Based on the information provided in this stud...   \n",
      "..                                                ...   \n",
      "86  Based on the context provided, the HuggingFace...   \n",
      "87  According to the context provided, Hugging Fac...   \n",
      "88  According to Gradio's test strategy document, ...   \n",
      "89  According to the context provided, the full co...   \n",
      "90  According to the context provided, you can acc...   \n",
      "\n",
      "   classification_and_explanation_text_by_claude     classification  \n",
      "0                                        correct            correct  \n",
      "1                                        correct            correct  \n",
      "2                                      partially  partially_correct  \n",
      "3                                      incorrect          incorrect  \n",
      "4                                        correct            correct  \n",
      "..                                           ...                ...  \n",
      "86                                       correct            correct  \n",
      "87                                       correct            correct  \n",
      "88                                       correct            correct  \n",
      "89                                       correct            correct  \n",
      "90                                       correct            correct  \n",
      "\n",
      "[91 rows x 9 columns]\n",
      "raptor_rag_evaluations_embedder_mixedbread-ai_mxbai-embed-large-v1_llm_anthropic.claude-3-5-sonnet-20240620-v1_0_dataset_HuggingFace QA Dataset.xlsx\n",
      "processing\n",
      "('mixedbread-ai_mxbai-embed-large-v1', 'anthropic.claude-3-5-sonnet-20240620-v1_0', '', 'HuggingFace QA Dataset')\n",
      "raptor_rag_evaluations_embedder_mixedbread-ai_mxbai-embed-large-v1_llm_anthropic.claude-3-5-sonnet-20240620-v1_0_dataset_PubMed filtered Dataset.xlsx\n",
      "raptor_rag_evaluations_embedder_dunzhang_stella_en_1.5B_v5_llm_anthropic.claude-3-5-sonnet-20240620-v1_0_dataset_HuggingFace QA Dataset.xlsx\n",
      "raptor_rag_evaluations_embedder_dunzhang_stella_en_1.5B_v5_llm_anthropic.claude-3-5-sonnet-20240620-v1_0_dataset_PubMed filtered Dataset.xlsx\n",
      "Results saved to /home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/src/my_rag/evaluations/results/classify_raptor_rag_eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"/home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/src/my_rag/evaluations/results\"  # Replace with your directory path\n",
    "process_files(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification percentages saved to raptor_classification_percentages.csv\n"
     ]
    }
   ],
   "source": [
    "def calculate_classification_percentages(input_csv, output_csv):\n",
    "    \"\"\"Calculate classification percentages with additional correct, incorrect, and partially_correct details.\"\"\"\n",
    "    # Load the evaluated results\n",
    "    data = pd.read_csv(input_csv)\n",
    "    data = data.fillna(\"\")\n",
    "\n",
    "    # Group by embedding_model, llm_model, reranking_model, and dataset\n",
    "    grouped = data.groupby(\n",
    "        [\"embedding_model\", \"llm_model\", \"reranking_model\", \"dataset\"]\n",
    "    )\n",
    "\n",
    "    # Calculate percentages\n",
    "    results = []\n",
    "    for (embedding, llm, reranking_model, dataset), group in grouped:\n",
    "        total = len(group)\n",
    "        counts = group[\"classification\"].value_counts().to_dict()\n",
    "\n",
    "        # Extract counts\n",
    "        correct = counts.get(\"correct\", 0)\n",
    "        incorrect = counts.get(\"incorrect\", 0)\n",
    "        partially_correct = counts.get(\"partially_correct\", 0)\n",
    "\n",
    "        # Calculate percentages and round to 2 decimals\n",
    "        correct_percentage = round((correct / total) * 100, 2) if total > 0 else 0\n",
    "        incorrect_percentage = round((incorrect / total) * 100, 2) if total > 0 else 0\n",
    "        partially_correct_percentage = (\n",
    "            round((partially_correct / total) * 100, 2) if total > 0 else 0\n",
    "        )\n",
    "\n",
    "        # Prepare result dictionary\n",
    "        result = {\n",
    "            \"embedding_model\": embedding,\n",
    "            \"llm_model\": llm,\n",
    "            \"reranking_model\": reranking_model,\n",
    "            \"dataset\": dataset,\n",
    "            \"total\": total,\n",
    "            \"correct\": correct,\n",
    "            \"incorrect\": incorrect,\n",
    "            \"partially_correct\": partially_correct,\n",
    "            \"correct_percentage\": correct_percentage,\n",
    "            \"incorrect_percentage\": incorrect_percentage,\n",
    "            \"partially_correct_percentage\": partially_correct_percentage,\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Convert results to a DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Classification percentages saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_csv = \"/home/ubuntu/Multi-Agent-LLM-System-with-LangGraph-RAG-and-LangChain/src/my_rag/evaluations/results/classify_raptor_rag_eval_results.csv\"  # Replace with the path to your evaluated results CSV\n",
    "output_csv = (\n",
    "    \"raptor_classification_percentages.csv\"  # Replace with the desired output file path\n",
    ")\n",
    "calculate_classification_percentages(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvembed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
