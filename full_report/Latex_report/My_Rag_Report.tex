% THIS TEMPLATE IS A WORK IN PROGRESS
% Adapted from an original template by faculty at Reykjavik University, Iceland

\documentclass{scrartcl}
\input{File_Setup.tex}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,epsfig}
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage[inkscapelatex=false]{svg}
\usepackage{bbm}


\hypersetup{
   colorlinks   = true,                               %Colours links instead of ugly boxes
   urlcolor     = blue,                               %Colour for external hyper links
   linkcolor    = blue,                               %Colour of internal links
   citecolor    = red,                                %Colour of citations
   setpagesize  = false,
   linktocpage  = true,
}
\graphicspath{ {fig/} }



\renewenvironment{abstract}{
    \centering
    \textbf{Abstract}
    \vspace{0.5cm}
    \par\itshape
    \begin{minipage}{0.7\linewidth}}{\end{minipage}
    \noindent\ignorespaces
}
% ------------------------------------------------------------------------------------------------------------------------

\begin{document}
%Title of the report, name of coworkers and dates (of experiment and of report).
\begin{titlepage}
	\centering
	\includegraphics[width=0.6\textwidth]{GW_logo.eps}\par
	\vspace{2cm}
	%%%% COMMENT OUT irrelevant lines below: Data Science OR Computer Science OR none
	{\scshape\LARGE Data Science Program \par}
	\vspace{1cm}
	{\scshape\Large Capstone Report - Fall 2024\par}
	\vspace{1.5cm}
	{\huge\bfseries MyRAG - Advancing Retrieval-Augmented Generation with Comparative Analysis of Standard RAG, ColBERT Reranking, and RAPTOR Architectures \par}
	\vspace{2cm}
	%%%% AUTHOR(S)
	{\Large\itshape Meet Daxini \\}\par
	\vspace{1.5cm}
	supervised by\par
	%%%% SUPERVISOR(S)
	Amir Jafari
\newpage

	\vfill
	\begin{abstract}
Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for providing personalized, relevant, and current information to user queries by combining large language models (LLMs) with external retrieval modules. This paper presents \textit{MyRAG}, an open-source system that unifies and compares different state-of-the-art RAG architectures and retrieval techniques. This paper explores embeddings, vector databases, and advanced retrieval methods, including ColBERT-based reranking and the hierarchical summarization-based RAPTOR architecture. By evaluating different datasets, it demonstrates how different configurations impact retrieval accuracy and downstream QA performance. The results offer insights into optimizing RAG pipelines, guiding both practitioners and researchers toward more efficient and effective retrieval-augmented generation.
	\end{abstract}
	\vfill
% Bottom of the page
\end{titlepage}
\tableofcontents
\newpage
% ------------------------------------------------------------------------------------------------------------------------
\section{Introduction}

The exponential growth of digital information has increased the demand for intelligent systems that can efficiently retrieve relevant context and answer complex questions. Large Language Models (LLMs) often possess extensive parametric knowledge, but may lack reliable, up-to-date information. Retrieval-Augmented Generation (RAG) has gained prominence as a solution to this challenge, bridging large-scale language understanding with external retrieval components to produce more grounded and accurate responses \cite{lewis2020retrieval, guu2020realm}.

In RAG pipelines, the LLM accesses external knowledge sources, retrieving relevant documents or chunks to augment its prompt. This approach enhances the modelâ€™s factual accuracy, reduces hallucinations, and updates knowledge without retraining the entire model. As the sizes of the context window continue to expand \cite{liu2023lost}, it is increasingly practical to provide LLMs with larger and more diverse sets of the retrieved context.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{StandardRag.pdf} 
    \caption{Flow chart illustrating the standard RAG pipeline.}
    \label{fig:standard_rag_flow}
\end{figure}


Yet, not all retrieval pipelines are created equal. Various embedding models, databases (like graph, vector, etc), and advanced retrieval techniques can be combined to form a RAG system. For instance, This paper explores How Standard Rag with ColBERT \cite{khattab2020colbert} re-ranking can refine retrieved documents, while the RAPTOR architecture \cite{wu2021recursively, raptor2024} organizes and summarizes documents hierarchically. Understanding the trade-offs between these methods is key to building robust and scalable RAG systems.

This paper introduces \textit{MyRAG}, an open-source framework designed to compare multiple retrieval and augmented generation strategies systematically. We evaluate embeddings, vector stores, and advanced retrieval enhancements like ColBERT re-ranking and RAPTOR summarization, applying them on the BioASQ \cite{bioasq2023} and Hugging Face Document QA datasets \cite{huggingface2024docqa}. Our experiments aim to provide a clearer understanding of how these components influence retrieval accuracy and end-to-end QA performance.

%----------------- Problem Statement -----------------%
\section{Problem Statement}

While the Massive Text Embedding Benchmark (MTEB) \cite{muennighoff2022mteb} provides valuable insights into embedding model performance across diverse tasks, there remains a critical need for comprehensive evaluation of end-to-end RAG architectures. The current landscape lacks:

\begin{enumerate}
    \item \textbf{Architecture-Specific Evaluation:} Unlike MTEB's focus on embedding quality, MyRAG provides systematic comparison of different RAG architectures - ColBERT reranking \cite{khattab2020colbert}, and RAPTOR hierarchical retrieval \cite{wu2021recursively, raptor2024} - using consistent datasets, metrics, vector stores (Chroma and DeepLake) and quantization strategies.
    
    \item \textbf{Resource-Conscious Assessment:} MyRAG evaluates both 8-bit quantized and full-precision versions of popular embedding models addressing practical deployment considerations not covered by MTEB's leaderboard.
    
    \item \textbf{Intuitive Real-World Metrics:} MyRAG introduces straightforward measures beyond MTEB's technical metrics:
    \begin{itemize}
        \item Correct/Partial/Incorrect answer classification
        \item Multi-document per question retrieval accuracy 
        \item Response generation quality with context
        \item Resource utilization across architectures
    \end{itemize}
\end{enumerate}

Through this comprehensive evaluation framework, MyRAG helps practitioners select optimal combinations of embedding models (guided by MTEB's leaderboard), retrieval architectures, and implementation approaches. The framework supports both detailed technical assessment and simple, interpretable metrics that organizations need to optimize their RAG deployments across use cases and resource constraints. By offering comparison of standard RAG, ColBERT reranking, and RAPTOR approaches on multtiple datasets, MyRAG provides actionable insights for building more effective retrieval-augmented generation systems.


\textit{MyRAG} addresses this need by providing an open-source code framework that enables direct, end-to-end comparisons of RAG approaches. By incorporating multiple embedding models, vector databases, reranking strategy (ColBERT), and hierarchical retrieval architecture (RAPTOR), \textit{MyRAG} provides a clear, cohesive platform for empirical evaluation. This approach ensures that even non-experts can understand performance trade-offs through accessible and practical metrics, ultimately guiding practitioners and researchers toward more optimal and tailored RAG solutions.
%----------------- MyRag System -----------------%
\section{Methodology}
MyRAG builds on the works by offering a unified platform to compare multiple embedding models, vector stores, and rag architectures including ColBERT and RAPTOR and provides a flexible pipeline to switch between embedding models, vector stores, and retrieval techniques. Key components include:

\subsection{Parsing Documents and Chunking}

MyRAG employs a robust text processing pipeline for document parsing and chunking, leveraging the \texttt{RecursiveCharacterTextSplitter} from LangChain \cite{langchain2024}. This method enables efficient handling of large textual data by splitting documents into manageable, contextually coherent chunks while preserving overlap for improved information retrieval. The pipeline includes the following key functionalities:

\begin{itemize}
    \item \textbf{Customizable Chunking Parameters:} Users can specify the \texttt{chunk\_size} (default: 2000 characters) and \texttt{chunk\_overlap} (default: 250 characters) to control the size and redundancy of chunks. This ensures that relevant information is not lost at chunk boundaries, a critical factor for maintaining context in retrieval-augmented pipelines.

    \item \textbf{Dynamic Splitting Based on Separators:} The chunking process supports a hierarchy of separators, including paragraph breaks (\texttt{"\textbackslash n\textbackslash n"}), line breaks (\texttt{"\textbackslash n"}), sentence endings (\texttt{". "}), and whitespace (\texttt{" "}). These separators allow adaptive splitting tailored to the structure of the input text, ensuring logical segmentation while minimizing disruption to the content.

    \item \textbf{Metadata Annotation:} Each chunk is annotated with rich metadata, such as document identifiers, chunk indices, total chunk count, and source type. This metadata facilitates traceability and enables fine-grained analysis of retrieval and generation performance.


    \item \textbf{Integration with PDF Parsing:} The system seamlessly integrates with a custom \texttt{PDFLoader} module, which extracts text content from PDF files. This module supports single and batch PDF loading, ensuring compatibility with diverse document formats often encountered in real-world applications.

\end{itemize}

This sophisticated parsing and chunking approach not only ensures the efficient handling of long-form text but also lays the foundation for effective retrieval and augmentation workflows within the MyRAG framework.
\subsection{Embedding Models}
MyRAG supports a range of embedding models that integrate seamlessly with AWS Bedrock and Hugging Face Transformers \cite{huggingfacedocs, AmazonBedrockAPI2024}. These models translate textual inputs into high-dimensional vector representations, which serve as the foundation for semantic similarity-based retrieval. The system accommodates various architectures and parameter scales, allowing flexible trade-offs between accuracy, computational cost, and resource constraints.

\begin{itemize}
    \item \textbf{Hugging Face-Based Models:} Models such as NV-Embed-v2, Stella 1.5B, MXBai Large, and all-MiniLM-L6-v2 are supported via the Hugging Face Transformers library. The implementation includes automatic device management (CPU/CUDA), 8-bit quantization support, and configurable device mapping for efficient resource utilization. Key features include:
    \begin{itemize}
        \item Automatic normalization of embeddings using L2 normalization
        \item Batch processing with customizable batch sizes for memory efficiency
        \item Support for instruction-based embedding generation
        \item Explicit memory management with CUDA cache clearing and garbage collection
        \item Optional model configurations including trust\_remote\_code and load\_in\_8bit for quantization and memory optimization
    \end{itemize}
    
    \item \textbf{Amazon Bedrock Integration:} The Amazon Titan embedding model is accessed through AWS Bedrock's runtime client api. 
\end{itemize}

Each embedding model implementation follows a consistent interface defined by the BaseEmbedding abstract class, ensuring uniform integration with the broader RAG pipeline. The system supports dynamic switching between models and handles cleanup operations to maintain efficient resource usage throughout the embedding process.

These embedding strategies form a core component of MyRAG's retrieval workflow. By evaluating models across benchmark datasets, it becomes possible to determine which embedding configurations yield the best balance of retrieval accuracy, computational efficiency, and downstream QA performance.


\subsection{Vector Databases}

MyRAG integrates support for Chroma \cite{chroma} and DeepLake \cite{deeplake}, two advanced vector databases specifically designed to optimize similarity search. These databases leverage \textbf{Hierarchical Navigable Small World (HNSW)} indices \cite{malkov2016efficient}, which are highly efficient for approximate nearest neighbor search. HNSW organizes vectors in a spatial domain into hierarchical clusters, enabling scalable similarity comparisons.

The key functionality of HNSW is its ability to cluster vectors in \(n\)-dimensional space based on proximity. For instance, given a dataset of 5000 vectors, HNSW groups similar vectors into clusters, each represented by a centroid or a representative vector. Assuming an average cluster size of 100 vectors, this process reduces the search space to 50 representative vectors. Consequently, the computational complexity of similarity comparisons is significantly reduced, shifting from 5000 pairwise comparisons to just 50. This hierarchical organization ensures efficient retrieval without compromising accuracy.

In addition to HNSW indices, both Chroma and DeepLake support similarity metrics like cosine and \(L_2\) distance, making them versatile for a wide range of use cases. Furthermore, these vector databases handle rich metadata alongside the vector representations, allowing for more nuanced queries and contextual filtering. These features collectively enhance the performance and scalability of retrieval pipelines within the MyRAG framework.


\subsection{LLMs}
MyRAG supports integration with both AWS Bedrock and Hugging Face Transformers-based models for text generation \cite{huggingfacedocs, AmazonBedrockAPI2024}. These Large Language Models (LLMs) serve as the backbone of the retrieval-augmented generation workflow, consuming retrieved documents as context and producing coherent, contextually grounded responses. The system implements a common interface through the BaseLLM abstract class, enabling seamless switching between different LLM backends.

\begin{itemize}
    \item \textbf{AWS Bedrock LLMs:} Models such as Anthropic's Claude variants are accessed through AWS Bedrock's runtime client API. 
    
    \item \textbf{Hugging Face Transformers LLMs:} Integration of models like Meta-Llama-3-8B-Instruct with features including:
    \begin{itemize}
        \item \textbf{Resource Management:} 
            \begin{itemize}
                \item Automatic CPU/CUDA device selection
                \item 8-bit quantization support
                \item Configurable device mapping for distributed inference
                \item Automatic CUDA cache clearing and garbage collection
            \end{itemize}
    \end{itemize}
\end{itemize}


The system maintains consistent cleanup procedures across implementations, ensuring efficient resource management particularly important for GPU-based deployments. This standardized approach enables systematic evaluation of different LLM configurations within the broader RAG pipeline.



\subsection{ColBERT Reranking}
ColBERT \cite{khattab2020colbert} provides a reranking framework that refines retrieval results by comparing query tokens against all tokens in candidate documents, rather than relying on a single vector embedding per document. Unlike traditional embeddings, which compress each document into a single vector, ColBERT applies a late interaction mechanism to handle token-level embeddings. This approach computes similarities between each query token embedding and every token embedding in a document, ensuring that important context within a document is not lost due to over-compression.

The implementation in MyRAG leverages the RAGatouille library \cite{ragatouille} to integrate ColBERT v2.0 as a re-ranking step following the initial retrieval process. After retrieving a candidate set of documents, the RAGatouille-based ColBERT implementation scores each document based on fine-grained token alignments with the query. These fine-grained comparisons enable the system to re-prioritize documents that contain subtle but crucial semantic clues. The refined ranking supports more accurate downstream retrieval-augmented generation (RAG) tasks, enhancing both precision and robustness. Through this method, MyRAG gains the ability to emphasize relevant sections of documents that might otherwise be overshadowed by broad embedding representations.

The re-ranking step is implemented as a modular pipeline component, allowing for easy integration with different retrieval configurations. This modularity enables MyRAG to compare performance with and without reranking, providing insights into the impact of fine-grained token matching on overall system performance. Figures below shows how ColBert embedding document scoring for top K differs from standard rag and how it is integrated in the MyRAG pipeline


\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{StandardRAGDocumentScoring.pdf}
	\caption{Flow chart of Standard RAG Embeddings Document Scoring within vector database}
	\label{fig:standard_doc_scoring}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Colbert.pdf}
	\caption{Flow chart of ColBert Document Scoring}
	\label{fig:colbert}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{StandardRagWithReRanking.pdf}
	\caption{Flow chart of Standard Rag with ColBert Reranking as integrated in MyRag}
	\label{fig:reranking_rag}
\end{figure}


\subsection{RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval}
RAPTOR \cite{wu2021recursively, raptor2024} implements a hierarchical retrieval architecture that organizes documents into a multi-level tree structure. Our implementation adapts the open-source code from the RAPTOR 2024 paper \cite{raptor2024}, particularly their approach to global and local clustering hierarchies. The implementation consists of several key components:

1) \textbf{Embedding and Dimensionality Reduction:} Following the RAPTOR architecture, documents are first embedded using a chosen embedding model. The system then employs UMAP (Uniform Manifold Approximation and Projection) for both global and local dimensionality reduction, with different neighborhood parameters for each level. The global clustering uses a square root-based neighborhood size, while local clustering maintains a fixed neighborhood of 10, as specified in the original RAPTOR implementation.

2) \textbf{Cluster Formation:} Adopting the RAPTOR paper's clustering strategy, the system uses Gaussian Mixture Models (GMM) with Bayesian Information Criterion (BIC) to automatically determine the optimal number of clusters. Documents can belong to multiple clusters based on a probability threshold, allowing for overlapping topic coverage. The number of clusters is dynamically optimized up to a maximum of 50, adapting to the document structure.

3) \textbf{Hierarchical Summarization:} At each level, documents within the same cluster are summarized using a Language Model (LLM). The summarization process is recursive, with each level building upon the summaries from the previous level. This creates a hierarchy of increasingly abstract representations, with the number of levels configurable through the \texttt{n\_levels} parameter.

4) \textbf{Integration with RAG:} The system maintains both the original documents and their summaries in the vector store, enabling flexible retrieval at different levels of abstraction. This allows the system to match queries with either specific details from source documents or broader thematic summaries, depending on the query's nature.

The implementation employs RAPTOR's combination of local and global clustering strategies, with careful memory management and batch processing to handle large document collections efficiently. This approach enables the system to capture both fine-grained relationships between similar documents and broader thematic connections across the corpus.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Raptor.pdf}
	\caption{Flow chart of RAPTOR Architecture as integrated in MyRAG}
	\label{fig:raptor}
\end{figure}

\subsection{Pipeline Implementation}

The retrieval-augmented generation pipeline is implemented as a modular sequence of clearly defined steps, each encapsulated in a \texttt{PipelineStep} class. This design allows components such as document processing, embedding generation, retrieval, reranking, and LLM-based generation that were discussed above to be easily composed, replaced, or extended.

\begin{enumerate}
    \item \textbf{Data Abstraction:} All information (documents, embeddings, queries, metadata, and results) is passed through the pipeline in a centralized \texttt{PipelineData} object. This shared state container prevents the need for extensive inter-step dependencies.

    \item \textbf{Document Processing and Embedding:} The pipeline begins by applying the previously described chunking and metadata annotation to raw documents via a \texttt{DocumentProcessor}. Next, a \texttt{DocumentEmbedder} step uses the selected embedding model to encode these chunks into vector representations.

    \item \textbf{Query Embedding:} A \texttt{QueryEmbedder} step similarly encodes input queries into vector form, ensuring consistency between document and query representations.

    \item \textbf{Retrieval and Reranking:} Using the chosen vector database and similarity search as described above, the \texttt{Retriever} step identifies the top-$k$ relevant chunks. If ColBERT-based reranking is enabled, a subsequent \texttt{RerankerStep} refines the ranking of these retrieved chunks, emphasizing token-level alignments for improved relevance.

    \item \textbf{LLM Generation:} Finally, the \texttt{Generator} step forwards both the retrieved (and optionally reranked) contexts and the original query to the LLM for response generation, resulting in a final answer that leverages the combined knowledge from the external documents.

    \item \textbf{RAPTOR Option:} When using RAPTOR, the pipeline replaces the standard \texttt{DocumentProcessor} with a \texttt{RaptorProcessor}, which recursively clusters and summarizes content at multiple hierarchy levels before embeddings and retrieval. This ensures scalable handling of large corpora and improved retrieval focus.

\end{enumerate}

This modular composition, defined as a \texttt{RAGPipeline}, enables experimentation and fine-grained comparison across various embedding models, LLMs, retrieval strategies, and architectures like RAPTOR. The codeâ€™s design ensures easy configuration, reproducibility, and extension, supporting a wide array of use cases in retrieval-augmented generation.

\subsection{Demo GUI}
MyRAG includes an interactive web interface built with Streamlit that allows users to experiment with different RAG Pipeline configurations. The GUI offers two main interfaces:

\begin{itemize}
    \item \textbf{Simple RAG:} Allows testing of standard RAG architecture with configurable parameters including:
    \begin{itemize}
        \item Choice of embedding models (NV-Embed-v2, Stella 1.5B, MXBai Large, etc.)
        \item LLM selection (Claude 3.5 Sonnet, Llama 3 8B Instruct)
        \item Customizable system messages
        \item Adjustable pipeline parameters (chunk size, overlap, retrieved documents $k$, etc.)
    \end{itemize}
    
    \item \textbf{RAG with Reranking:} Extends the simple RAG interface with ColBERT reranking capabilities, maintaining all base configuration options while adding reranking-specific parameters. 
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{MyRAG_UI_Process_docs.png}
    \caption{MyRAG Process Documents with Simple RAG}
    \label{fig:process_docs}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{MyRAG_UI_QA.png}
    \caption{MyRAG Question-Answering with Simple RAG}
    \label{fig:qa_interface}
\end{figure}

 RAPTOR implementation is in progress and will be updated soon. The interface provides visualization of the MyRAG pipeline's performance.

%----------------- Evaluations and Results -----------------%
\section{Evaluations and Results}

%----------------- Datasets -----------------%
\subsection{Datasets}

\textbf{1. Hugging Face Document QA Evaluation Dataset:}

The \textit{huggingface\_doc\_qa\_eval} dataset is a synthetic dataset consisting of question-answer pairs extracted from the markdown files of the Hugging Face repository on GitHub. It is specifically designed for evaluating Retrieval-Augmented Generation (RAG) systems \cite{huggingface2024docqa}. The dataset contains a total of 65 unique questions, each paired with its corresponding answer, context, and source document path.

Out of the ten available columns in the dataset, the following four were used for evaluation:
\begin{itemize}
    \item \textbf{Context:} The content retrieved from the corresponding markdown file.
    \item \textbf{Question:} The query posed to the system.
    \item \textbf{Answer:} The ideal answer to the question.
    \item \textbf{Source\_doc:} The path to the markdown file in the Hugging Face GitHub repository.
\end{itemize}

The dataset was loaded in Parquet format using the Hugging Face API. Each question-answer pair was derived from a unique markdown file, ensuring there was no overlap in the source documents. This design allows for precise evaluation of RAG systems in retrieving relevant content from specific documents.

\textbf{2. BioASQ11 Challenge Dataset:}

The \textit{BioASQ11} dataset was obtained from the 2023 iteration of the BioASQ challenge, specifically Task Set B for biomedical question answering \cite{bioasq2023}. The training set (11b) consists of a total of 4,719 questions, each provided with gold-standard annotations, including concepts, article URLs, snippets, RDF triples, "exact" answers, and "ideal" answers.

Due to the size of the dataset and restrictions on accessing articles from certain URLs, a filtering process was applied:
\begin{itemize}
    \item Articles that required special accounts or credentials for access were excluded.
    \item To comply with server limitations on large-scale downloads, only a subset of 13 questions with 42 associated PDFs was retained for the evaluation.
\end{itemize}

For each retained question, answers are derived from 1 to 7 different PDFs. To facilitate evaluation, a CSV file was created for this filtered questions with the following columns:
\begin{itemize}
    \item \textbf{Question ID:} A unique identifier for each question.
    \item \textbf{Question:} The query posed.
    \item \textbf{Ideal Answer:} The gold-standard answer for the question.
    \item \textbf{Download Link:} The URL for downloading the article PDF.
    \item \textbf{PDF Reference:} The specific reference to the PDFs used for the answer.
\end{itemize}

A web scraping script was developed to retrieve PDF file URLs from the provided article links. This processed dataset enables systematic evaluation of retrieval accuracy and relevance in biomedical contexts.

\subsection{TOP-K Retrieval Accuracies}

Retrieval accuracy for each embedding model is evaluated using a standard top-$k$ retrieval metric. Given $N$ queries, let $\text{retrieved}_i@k$ denote the set of top-$k$ retrieved documents for the $i$-th query, and let $\text{relevant}_i$ represent the set of relevant documents for that query. The top-$k$ retrieval accuracy, $\text{Accuracy@k}$, is computed as follows:

\begin{equation}
\text{Accuracy@k} = \frac{1}{N} \sum_{i=1}^{N} 
    \begin{cases}
      \mathbbm{1}(\exists d \in \text{retrieved}_i@k : d \in \text{relevant}_i), & \text{if } |\text{relevant}_i| = 1 \text{ or } k = 1 \\[6pt]
      \frac{|\text{retrieved}_i@k \cap \text{relevant}_i|}{\min(k, |\text{relevant}_i|)}, & \text{otherwise}
    \end{cases}
\end{equation}

The evaluations are conducted on both the BioASQ and Hugging Face Document QA datasets. All runs use the common retrieval parameters shown in Table~\ref{table:retrieval_params}. Each query is processed using a given embedding model, and the resulting top-$k$ candidates are examined according to the equation above.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Retrieval Parameter} & \textbf{Value} \\
\hline
Chunk Size & 4000 tokens \\
Chunk Overlap & 200 tokens \\
Top-$k$ & 1,2,3,4,5,7,8,10 \\
Query Instruction & "Instruct: Given a question, retrieve passages that answer the question. Query:" for NV-Embed-v2 \\ 
~ & "Instruct: Given a web search query, retrieve relevant passages that answer the query. Query:" for Stella \\
~ & None for all others \\
\hline
\end{tabular}
\caption{Common retrieval parameters and instructions used across experiments.}
\label{table:retrieval_params}
\end{table}

Table~\ref{table:model_specs} presents the model specifications. The \texttt{mxbai-embed-large-v1} model is extremely resource-efficient and still provides competitive performance, making it suitable for environments with limited computational capacity. In contrast, \texttt{NV-Embed-v2 (8-bit)} is notably larger, yet still manageable due to quantization, and it achieves performance comparable to much larger models. The \texttt{stella\_en\_1.5B\_v5} model is larger and performs slightly better than \texttt{NV-Embed-v2 (8-bit)}, but at the cost of higher resource usage. Despite its closed-source nature and unknown parameters, \texttt{titan-embed-text-v2} does not surpass the best open-source embeddings. The \texttt{all-MiniLM-L6-v2} model, while very lightweight, trails behind the top performers.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Embedding Model} & \textbf{Quantization} & \textbf{Size (GB)} & \textbf{Parameters (M)} \\ \hline
mxbai-embed-large-v1      & None   & 1.25 & 335.14 \\ \hline
stella\_en\_1.5B\_v5 (8-bit) & 8-bit & 3.48 & 1543.27 \\ \hline
stella\_en\_1.5B\_v5         & None  & 9.25 & 1543.27 \\ \hline
NV-Embed-v2 (8-bit)       & 8-bit & 7.44 & 7851.02 \\ \hline
all-MiniLM-L6-v2          & None  & 0.08 & 22.71 \\ \hline
titan-embed-text-v2       & Unknown & Unknown & Unknown \\ \hline
\end{tabular}
\caption{Specifications of embedding models evaluated.}
\label{table:model_specs}
\end{table}

Table~\ref{table:standard_rag_results} shows the top-$k$ retrieval accuracy under standard RAG (no reranking). It can be observed that \texttt{NV-Embed-v2 (8-bit)}, \texttt{mxbai-embed-large-v1}, and \texttt{stella\_en\_1.5B\_v5} achieve strong accuracy scores. Although \texttt{NV-Embed-v2 (8-bit)} is large, quantization allows it to be memory-efficient. The \texttt{mxbai-embed-large-v1} model stands out as both lightweight and high-performing, while \texttt{all-MiniLM-L6-v2} remains efficient but lags in accuracy. The closed-source \texttt{titan-embed-text-v2} does not outperform the open-source models.

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c c c c}
\hline
\textbf{Model} & \textbf{Dataset} & \textbf{k=1} & \textbf{k=2} & \textbf{k=3} & \textbf{k=4} & \textbf{k=5} & \textbf{k=7} & \textbf{k=8} & \textbf{k=10} \\
\hline
\multirow{2}{*}{NV-Embed-v2 (8-bit)} 
 & BioASQ & 1.00 & 0.85 & 0.79 & 0.76 & 0.77 & 0.79 & 0.80 & 0.80 \\
 & HF QA  & 0.91 & 0.94 & 0.98 & 0.98 & 0.98 & 1.00 & 1.00 & 1.00 \\
\hline
\multirow{2}{*}{all-MiniLM-L6-v2} 
 & BioASQ & 0.62 & 0.58 & 0.51 & 0.47 & 0.57 & 0.54 & 0.57 & 0.61 \\
 & HF QA  & 0.58 & 0.66 & 0.72 & 0.74 & 0.77 & 0.80 & 0.82 & 0.83 \\
\hline
\multirow{2}{*}{mxbai-embed-large-v1} 
 & BioASQ & 1.00 & 0.92 & 0.77 & 0.70 & 0.74 & 0.77 & 0.79 & 0.79 \\
 & HF QA  & 0.95 & 0.95 & 0.95 & 0.95 & 0.95 & 0.98 & 0.98 & 0.98 \\
\hline
\multirow{2}{*}{stella\_en\_1.5B\_v5} 
 & BioASQ & 1.00 & 0.73 & 0.72 & 0.71 & 0.72 & 0.74 & 0.75 & 0.79 \\
 & HF QA  & 0.95 & 0.98 & 0.98 & 0.98 & 1.00 & 1.00 & 1.00 & 1.00 \\
\hline
\multirow{2}{*}{stella\_en\_1.5B\_v5 (8-bit)} 
 & BioASQ & 0.31 & 0.42 & 0.64 & 0.66 & 0.68 & 0.74 & 0.74 & 0.78 \\
 & HF QA  & 0.88 & 0.91 & 0.91 & 0.91 & 0.92 & 0.94 & 0.94 & 0.94 \\
\hline
\multirow{2}{*}{titan-embed-text-v2} 
 & BioASQ & 1.00 & 0.81 & 0.69 & 0.67 & 0.69 & 0.67 & 0.68 & 0.73 \\
 & HF QA  & 0.89 & 0.91 & 0.95 & 0.97 & 0.98 & 0.98 & 0.98 & 0.98 \\
\hline
\end{tabular}
\caption{Top-$k$ retrieval accuracy for standard RAG. All evaluations are performed with the parameters and instructions listed in Table~\ref{table:retrieval_params}.}
\label{table:standard_rag_results}
\end{table}

ColBERT v2.0 reranking is then introduced to refine the top-10 results from an initial retrieval of 15 documents. The reranking parameters used are shown in Table~\ref{table:rerank_params}, and Table~\ref{table:rerank_results} presents the corresponding accuracy scores. The ColBERT reranking model, with a size of only 0.41 GB and 110M parameters, quickly rescored candidates without significant overhead.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Reranking Parameter} & \textbf{Value} \\
\hline
Initial Retrieval Top $k$ & 15 \\
Reranking Top $k$ & 10 \\
Chunk Size & 4000 tokens \\
Chunk Overlap & 200 tokens \\
\hline
\end{tabular}
\caption{Reranking parameters for experiments involving ColBERT v2.0.}
\label{table:rerank_params}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c c c c}
\hline
\textbf{Model} & \textbf{Dataset} & \textbf{k=1} & \textbf{k=2} & \textbf{k=3} & \textbf{k=4} & \textbf{k=5} & \textbf{k=7} & \textbf{k=8} & \textbf{k=10} \\
\hline
\multirow{2}{*}{NV-Embed-v2 (8-bit)} 
 & BioASQ & 1.00 & 0.81 & 0.74 & 0.77 & 0.78 & 0.80 & 0.80 & 0.82 \\
 & HF QA  & 0.97 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 1.00 & 1.00 \\
\hline
\multirow{2}{*}{all-MiniLM-L6-v2} 
 & BioASQ & 0.85 & 0.77 & 0.72 & 0.67 & 0.69 & 0.72 & 0.72 & 0.73 \\
 & HF QA  & 0.88 & 0.89 & 0.89 & 0.89 & 0.91 & 0.91 & 0.91 & 0.91 \\
\hline
\multirow{2}{*}{mxbai-embed-large-v1} 
 & BioASQ & 1.00 & 0.88 & 0.77 & 0.73 & 0.74 & 0.80 & 0.82 & 0.82 \\
 & HF QA  & 0.92 & 0.97 & 0.97 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 \\
\hline
\multirow{2}{*}{stella\_en\_1.5B\_v5} 
 & BioASQ & 1.00 & 0.85 & 0.72 & 0.67 & 0.73 & 0.79 & 0.79 & 0.79 \\
 & HF QA  & 0.95 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 1.00 \\
\hline
\multirow{2}{*}{stella\_en\_1.5B\_v5 (8-bit)} 
 & BioASQ & 0.92 & 0.81 & 0.72 & 0.67 & 0.68 & 0.69 & 0.70 & 0.72 \\
 & HF QA  & 0.77 & 0.77 & 0.77 & 0.77 & 0.78 & 0.78 & 0.78 & 0.82 \\
\hline
\multirow{2}{*}{titan-embed-text-v2} 
 & BioASQ & 1.00 & 0.85 & 0.74 & 0.71 & 0.73 & 0.75 & 0.77 & 0.79 \\
 & HF QA  & 0.92 & 0.95 & 0.95 & 0.95 & 0.95 & 0.95 & 0.97 & 0.98 \\
\hline
\end{tabular}
\caption{Top-$k$ retrieval accuracy after ColBERT reranking.}
\label{table:rerank_results}
\end{table}

ColBERT reranking generally improves performance, especially for models that had weaker initial retrieval results. For instance, \texttt{all-MiniLM-L6-v2} and \texttt{titan-embed-text-v2} show notable gains, indicating that fine-grained token-level scoring helps surface more relevant candidates at top ranks. For strong models like \texttt{NV-Embed-v2 (8-bit)}, reranking raises the HF QA $k=1$ accuracy from 0.91 to 0.97, but the gains diminish at higher $k$ values, suggesting that reranking is most beneficial for improving the very top results rather than broader sets.

The RAPTOR-based retriever is not included in these top-$k$ evaluations as it involves hierarchical summaries that complicate direct comparison to standard retrieval methods. Future iterations will incorporate RAPTOR retrieval accuracies, enabling comparisons across the full spectrum of retrieval strategies.

\subsection{TOP-K Retrieval Accuracies}

To evaluate retrieval performance, we measure the accuracy of the top-\(k\) retrieved documents against the known relevant set of documents. For each query \(i\), let \(\text{retrieved}_i@k\) denote the set of top-\(k\) retrieved documents and \(\text{relevant}_i\) denote the set of all relevant documents. The top-\(k\) retrieval accuracy, \(\text{Accuracy@k}\), is calculated as:

\begin{equation}
\text{Accuracy@k} = \frac{1}{N} \sum_{i=1}^{N} 
    \begin{cases}
      \mathbbm{1}(\exists d \in \text{retrieved}_i@k : d \in \text{relevant}_i), & \text{if } |\text{relevant}_i| = 1 \text{ or } k = 1 \\[6pt]
      \frac{|\text{retrieved}_i@k \cap \text{relevant}_i|}{\min(k, |\text{relevant}_i|)}, & \text{otherwise}
    \end{cases}
\end{equation}

Here:
\begin{itemize}
    \item \(N\) is the total number of queries.
    \item \(\text{retrieved}_i@k\) are the top-\(k\) retrieved documents for the \(i\)-th query.
    \item \(\text{relevant}_i\) are the relevant documents for the \(i\)-th query.
    \item \(\mathbbm{1}\) is the indicator function that equals 1 if the condition is true and 0 otherwise.
\end{itemize}


\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c c c c}
\hline
\textbf{Model} & \textbf{Dataset} & \textbf{k=1} & \textbf{k=2} & \textbf{k=3} & \textbf{k=4} & \textbf{k=5} & \textbf{k=7} & \textbf{k=8} & \textbf{k=10} \\
\hline
\multirow{2}{*}{\texttt{NV-Embed-v2 (8-bit)}} 
 & BioASQ & 1.00 & 0.85 & 0.79 & 0.76 & 0.77 & 0.79 & 0.80 & 0.80 \\
 & HF QA  & 0.91 & 0.94 & 0.98 & 0.98 & 0.98 & 1.00 & 1.00 & 1.00 \\
\hline
\multirow{2}{*}{\texttt{all-MiniLM-L6-v2}} 
 & BioASQ & 0.62 & 0.58 & 0.51 & 0.47 & 0.57 & 0.54 & 0.57 & 0.61 \\
 & HF QA  & 0.58 & 0.66 & 0.72 & 0.74 & 0.77 & 0.80 & 0.82 & 0.83 \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & BioASQ & 1.00 & 0.92 & 0.77 & 0.70 & 0.74 & 0.77 & 0.79 & 0.79 \\
 & HF QA  & 0.95 & 0.95 & 0.95 & 0.95 & 0.95 & 0.98 & 0.98 & 0.98 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & BioASQ & 1.00 & 0.73 & 0.72 & 0.71 & 0.72 & 0.74 & 0.75 & 0.79 \\
 & HF QA  & 0.95 & 0.98 & 0.98 & 0.98 & 1.00 & 1.00 & 1.00 & 1.00 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5 (8-bit)}} 
 & BioASQ & 0.31 & 0.42 & 0.64 & 0.66 & 0.68 & 0.74 & 0.74 & 0.78 \\
 & HF QA  & 0.88 & 0.91 & 0.91 & 0.91 & 0.92 & 0.94 & 0.94 & 0.94 \\
\hline
\multirow{2}{*}{\texttt{titan-embed-text-v2}} 
 & BioASQ & 1.00 & 0.81 & 0.69 & 0.67 & 0.69 & 0.67 & 0.68 & 0.73 \\
 & HF QA  & 0.89 & 0.91 & 0.95 & 0.97 & 0.98 & 0.98 & 0.98 & 0.98 \\
\hline
\end{tabular}
\caption{Top-k Retrieval Accuracy Comparison of Embedding Models in Standard Rag}
\label{table:top_k_retrieval_accuracy_table}
\end{table}

As illustrated in Table \ref{table:top_k_retrieval_accuracy_table}, the \texttt{NV-Embed-v2 (8-bit)} model, which utilizes 7.44 GB of GPU memory, performs comparably to the \texttt{stella\_en\_1.5B\_v5} model that consumes 9.25 GB. However, \texttt{stella\_en\_1.5B\_v5} achieves slightly better scores overall. Notably, the lightweight \texttt{mxbai-embed-large-v1}, which requires only 1.25 GB of GPU memory, performs equally well and, in some cases, even slightly better than \texttt{stella\_en\_1.5B\_v5}. While the \texttt{all-MiniLM-L6-v2} is also very lightweight, it does not perform as well. Additionally, the closed-source \texttt{titan-embed-text-v2} model from Amazon, does not outperform the open-source models. The retrieval parameters used for each model are detailed in Table \ref{table:retrieval_params}.

\subsubsection{Embedding Model Specifications}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model Name} & \textbf{Quantization} & \textbf{Size (GB)} & \textbf{Parameters (Millions)} \\ \hline
mxbai-embed-large-v1 & None              & 1.25 & 335.14 \\ \hline
stella\_en\_1.5B\_v5 (8-bit) & 8-bit          & 3.48 & 1543.27 \\ \hline
stella\_en\_1.5B\_v5         & None           & 9.25 & 1543.27 \\ \hline
NV-Embed-v2 (8-bit)            & 8-bit          & 7.44 & 7851.02 \\ \hline
all-MiniLM-L6-v2 & None          & 0.08 & 22.71 \\ \hline
titan-embed-text-v2                   & Unknown        & Unknown & Unknown \\ \hline
Meta-Llama-3-8B-Instruct (8-bit) & 8-bit   & 10.42 & 8030.26 \\ \hline
claude-3-5-sonnet-20240620-v1      & Unknown           & Unknown & Unknown \\ \hline
colbertv2.0       & None                  & 0.41 & 110               \\ \hline
\end{tabular}
\caption{Specifications of Models used in MyRag}
\label{table:models_specification}
\end{table}


\subsubsection{Retrieval Parameters}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Chunk Size & 4000 tokens \\
Chunk Overlap & 200 tokens \\
Top k & 10 \\
\hline
\end{tabular}
\caption{Standard RAG Retrieval Parameters}
\label{table:retrieval_params}
\end{table}


\subsubsection{Reranking Parameters}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Initial Retrieval Top k & 15 \\
Reranking Top k & 10 \\
Chunk Size & 4000 tokens \\
Chunk Overlap & 200 tokens \\
\hline
\end{tabular}
\caption{Standard Rag with ColBERT Reranking Parameters}
\label{table:rerank_params}
\end{table}

\subsubsection{Query Instruction used}

\begin{table}[H]
\centering
\begin{tabular}{|l|p{10cm}|} % Adjust the width of the column as needed
\hline
\textbf{Embedding model} & \textbf{Query Instruction} \\
\hline
NV-Embed-v2 & "Instruct: Given a question, retrieve passages that answer the question. Query:" \\
mxbai-embed-large-v1 & None \\
stella\_en\_1.5B\_v5 & "Instruct: Given a web search query, retrieve relevant passages that answer the query. Query:" \\
all-MiniLM-L6-v2 & None \\
titan-embed-text-v2 & None \\
colbertv2.0(ColBert re-ranking) & None \\
\hline
\end{tabular}
\caption{Embedding model Specific Query Instructions used across all results mentioned in this paper}
\label{table:model_instructions}
\end{table}


\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c c c c}
\hline
\textbf{Model} & \textbf{Dataset} & \textbf{k=1} & \textbf{k=2} & \textbf{k=3} & \textbf{k=4} & \textbf{k=5} & \textbf{k=7} & \textbf{k=8} & \textbf{k=10} \\
\hline
\multirow{2}{*}{\texttt{NV-Embed-v2 (8-bit)}} 
 & BioASQ & 1.00 & 0.81 & 0.74 & 0.77 & 0.78 & 0.80 & 0.80 & 0.82 \\
 & HF QA  & 0.97 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 1.00 & 1.00 \\
\hline
\multirow{2}{*}{\texttt{all-MiniLM-L6-v2}} 
 & BioASQ & 0.85 & 0.77 & 0.72 & 0.67 & 0.69 & 0.72 & 0.72 & 0.73 \\
 & HF QA  & 0.88 & 0.89 & 0.89 & 0.89 & 0.91 & 0.91 & 0.91 & 0.91 \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & BioASQ & 1.00 & 0.88 & 0.77 & 0.73 & 0.74 & 0.80 & 0.82 & 0.82 \\
 & HF QA  & 0.92 & 0.97 & 0.97 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & BioASQ & 1.00 & 0.85 & 0.72 & 0.67 & 0.73 & 0.79 & 0.79 & 0.79 \\
 & HF QA  & 0.95 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 1.00 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5 (8-bit)}} 
 & BioASQ & 0.92 & 0.81 & 0.72 & 0.67 & 0.68 & 0.69 & 0.70 & 0.72 \\
 & HF QA  & 0.77 & 0.77 & 0.77 & 0.77 & 0.78 & 0.78 & 0.78 & 0.82 \\
\hline
\multirow{2}{*}{\texttt{titan-embed-text-v2}} 
 & BioASQ & 1.00 & 0.85 & 0.74 & 0.71 & 0.73 & 0.75 & 0.77 & 0.79 \\
 & HF QA  & 0.92 & 0.95 & 0.95 & 0.95 & 0.95 & 0.95 & 0.97 & 0.98 \\
\hline
\end{tabular}
\caption{Top-k Retrieval Accuracy Comparison of Embedding Models in Standard Rag with ColBert Re-ranking }
\label{table:top_k_reranking_retrieval_accuracy}
\end{table}

The top 15 documents were retrieved using each embedding model, followed by the reranking of the top 10 documents with the ColBERT reranking model. As illustrated in Table, \ref{table:top_k_reranking_retrieval_accuracy}, the application of ColBERT reranking enhances retrieval performance across various embedding models. Specifically, the \texttt{all-MiniLM-L6-v2} and \texttt{titan-embed-text-v2} models, which initially exhibited lower retrieval accuracies in the standard RAG setup, showed significant improvements after re-ranking, achieving higher accuracy scores across both datasets. 
For high-performing models such as \texttt{NV-Embed-v2 (8-bit)}, ColBERT reranking resulted in a significant improvement in HF QA scores from 0.91 to 0.97 at \(k=1\). However, at higher values of \(k\), the standard RAG setup without reranking maintained higher accuracy, indicating that reranking has limited benefits beyond the top retrieval results for these models. Parameters used while reranking in appendix


The Raptor rag retriever could not have the top k retrieval accuracy as the standard rag retriever  and colbert reranking rag retriever because it contains summaries of multiple documents in future version it will be better to meausre raptors retriever accuracy accros different embedding models and llm models

\subsection{RAG Evaluation}

We assess end-to-end performance by feeding retrieved documents into an LLM. I initially tried with Meta-Llama-3-8B-Instruct (8-bit) but due to not being generating effective answers as well limited computational resources claude-3-5-sonnet-20240620-v1 from aws bedrock was only selected and run You can check the 

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c c c}
\hline

\textbf{Embedding Model} & \textbf{Dataset} & \textbf{Total} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Partially Correct} & \textbf{Correct (\%)}  \\
\hline
\multirow{2}{*}{\texttt{NV-Embed-v2 (8-bit)}} 
 & HF QA  & 65 & 64 & 1 & 0 & 98.46 \\
 & BioASQ & 13 & 9  & 1 & 3 & 69.23 \\
\hline
\multirow{2}{*}{\texttt{all-MiniLM-L6-v2}} 
 & HF QA  & 65 & 50 & 15 & 0 & 76.92 \\
 & BioASQ & 13 & 6  & 3 & 4 & 46.15  \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & HF QA  & 65 & 62 & 3 & 0 & 95.38 \\
 & BioASQ & 13 & 8  & 1 & 4 & 61.54 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & HF QA  & 65 & 64 & 1 & 0 & 98.46 \\
 & BioASQ & 13 & 9  & 0 & 4 & 69.23 \\
\hline
\multirow{2}{*}{\texttt{titan-embed-text-v2}} 
 & HF QA  & 65 & 63 & 2 & 0 & 96.92 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54\\
\hline
\end{tabular}
\caption{Evaluation Results of Standard RAG by Embedding Models}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c}
\hline
\textbf{Embedding Model} & \textbf{Dataset} & \textbf{Total} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Partially Correct} & \textbf{Correct (\%)} \\
\hline
\multirow{2}{*}{\texttt{NV-Embed-v2 (8-bit)}} 
 & HF QA  & 65 & 63 & 2 & 0 & 96.92 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54 \\
\hline
\multirow{2}{*}{\texttt{all-MiniLM-L6-v2}} 
 & HF QA  & 65 & 58 & 7 & 0 & 89.23 \\
 & BioASQ & 13 & 5  & 3 & 5 & 38.46 \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & HF QA  & 65 & 63 & 2 & 0 & 96.92 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & HF QA  & 65 & 63 & 2 & 0 & 96.92 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54 \\
\hline
\multirow{2}{*}{\texttt{titan-embed-text-v2}} 
 & HF QA  & 65 & 62 & 3 & 0 & 95.38 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54 \\
\hline
\end{tabular}
\caption{Evaluation Results of Standard RAG with ColBert Reranking by Embedding Models}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c}
\hline
\textbf{Embedding Model} & \textbf{Dataset} & \textbf{Total} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Partially Correct} & \textbf{Correct (\%)} \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & HF QA  & 65 & 64 & 1 & 0 & 98.46 \\
 & BioASQ & 13 & 9  & 1 & 3 & 69.23 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & HF QA  & 65 & 65 & 0 & 0 & 100.00 \\
 & BioASQ & 13 & 10 & 0 & 3 & 76.92 \\
\hline
\end{tabular}
\caption{Evaluation Results of Raptor RAG with LLM as Claude v3.5 Sonnet by Embedding Models}
\label{table:raptor_eval}
\end{table}



RAPTOR improves handling of long, complex and multiple documents by recursively summarizing and clustering information. As shown in Table \ref{table:raptor_eval}, RAPTOR-enabled retrieval combinations yield higher correctness, especially on biomedical QA (BioASQ) tasks where detailed reasoning is essential and the answer is from multiple documents. Raptor architecture could not be run with NV-embed-2 due to limited computational resources and with titan embeddings v2 from amazon and all minilm-l6v2  due to limitation in max tokens exceeded   

%----------------- Challenges and Future Work -----------------%
\section{Challenges and Future Work}
\begin{itemize}
    \item \textbf{Evaluation Metrics:} Current retriever evaluation focuses on document-level accuracy. Future work includes more fine-grained evaluation (e.g., chunk level), automated scoring methods for retriever, and domain expert assessments of entire QA RAG to better understand retrieval quality in specialized domains.
    
    \item \textbf{Document Parsing:} MyRAG currently processes textual data. Extending it to multimodal inputs (PDFs with figures, images, or even audio transcripts) would demand integrating vision and ASR models, enabling comprehensive retrieval across diverse data formats.
    
    \item \textbf{Multi-agent Systems:} A future direction involves multi-agent LLM architectures where one agent refines user queries and another specializes in retrieval. Iterative query clarification and refinement would ensure that only the most relevant information is retrieved for the LLM to answer.
\end{itemize}

%----------------- Conclusion -----------------%
\section{Conclusion}

This paper introduced MyRAG, an open-source RAG system that benchmarks and compares a range of retrieval architectures, embeddings, vector databases, and reranking methods. By integrating ColBERT reranking and the RAPTOR hierarchical retrieval architecture, MyRAG provides insights into building scalable and efficient retrieval-augmented generation pipelines. Experiments on BioASQ and Hugging Face Document QA datasets demonstrate that advanced retrieval methods can significantly improve both retrieval accuracy and end-to-end QA performance.

Future efforts will enhance evaluation methods, integrate multimodal capabilities, explore hybrid retrieval models, and design multi-agent query refinement to push the frontier of retrieval-augmented generation systems.

%----------------- References -----------------%
\bibliographystyle{IEEEtran}
\bibliography{references}

% \appendix
% Additional details, code excerpts, and extended results can be placed here.
% Appendices for MyRag
%Put hyper parameters tables top k used, settings used , temperature used, instruction used
% Human Evaluation 
% setup Details I used inference of all local models on aws  ec2 instuance g5 2xl and test  
\end{document}