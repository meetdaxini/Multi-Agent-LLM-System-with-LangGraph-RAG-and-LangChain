% THIS TEMPLATE IS A WORK IN PROGRESS
% Adapted from an original template by faculty at Reykjavik University, Iceland

\documentclass{scrartcl}
\input{File_Setup.tex}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,epsfig}
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage[inkscapelatex=false]{svg}


\hypersetup{
   colorlinks   = true,                               %Colours links instead of ugly boxes
   urlcolor     = blue,                               %Colour for external hyper links
   linkcolor    = blue,                               %Colour of internal links
   citecolor    = red,                                %Colour of citations
   setpagesize  = false,
   linktocpage  = true,
}
\graphicspath{ {fig/} }



\renewenvironment{abstract}{
    \centering
    \textbf{Abstract}
    \vspace{0.5cm}
    \par\itshape
    \begin{minipage}{0.7\linewidth}}{\end{minipage}
    \noindent\ignorespaces
}
% ------------------------------------------------------------------------------------------------------------------------

\begin{document}
%Title of the report, name of coworkers and dates (of experiment and of report).
\begin{titlepage}
	\centering
	\includegraphics[width=0.6\textwidth]{GW_logo.eps}\par
	\vspace{2cm}
	%%%% COMMENT OUT irrelevant lines below: Data Science OR Computer Science OR none
	{\scshape\LARGE Data Science Program \par}
	\vspace{1cm}
	{\scshape\Large Capstone Report - Fall 2024\par}
	\vspace{1.5cm}
	{\huge\bfseries MyRAG - Advancing Retrieval-Augmented Generation with Comparative Analysis of Standard RAG, ColBERT Reranking, and RAPTOR Architectures \par}
	\vspace{2cm}
	%%%% AUTHOR(S)
	{\Large\itshape Meet Daxini \\}\par
	\vspace{1.5cm}
	supervised by\par
	%%%% SUPERVISOR(S)
	Amir Jafari
\newpage

	\vfill
	\begin{abstract}
Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for providing personalized, relevant, and current information to user queries by combining large language models (LLMs) with external retrieval modules. This paper presents \textit{MyRAG}, an open-source system that unifies and compares different state-of-the-art RAG architectures and retrieval techniques. This paper explores embeddings, vector databases, and advanced retrieval methods, including ColBERT-based reranking and the hierarchical summarization-based RAPTOR architecture. By evaluating MyRAG on the BioASQ and Hugging Face Document QA datasets, demonstrates how different configurations impact retrieval accuracy and downstream QA performance. The results offer insights into optimizing RAG pipelines, guiding both practitioners and researchers toward more efficient and effective retrieval-augmented generation.
	\end{abstract}
	\vfill
% Bottom of the page
\end{titlepage}
\tableofcontents
\newpage
% ------------------------------------------------------------------------------------------------------------------------
\section{Introduction}

The exponential growth of digital information has increased the demand for intelligent systems that can efficiently retrieve relevant context and answer complex questions. Large Language Models (LLMs) often possess extensive parametric knowledge, but may lack reliable, up-to-date information. Retrieval-Augmented Generation (RAG) has gained prominence as a solution to this challenge, bridging large-scale language understanding with external retrieval components to produce more grounded and accurate responses \cite{lewis2020retrieval, guu2020realm}.

In RAG pipelines, the LLM accesses external knowledge sources, retrieving relevant documents or chunks to augment its prompt. This approach enhances the model’s factual accuracy, reduces hallucinations, and updates knowledge without retraining the entire model. As the sizes of the context window continue to expand \cite{liu2023lost}, it is increasingly practical to provide LLMs with larger and more diverse sets of the retrieved context.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{StandardRag.pdf} 
    \caption{Flow chart illustrating the standard RAG pipeline.}
    \label{fig:standard_rag_flow}
\end{figure}


Yet, not all retrieval pipelines are created equal. Various embedding models, vector databases, and advanced retrieval techniques can be combined to form a RAG system. For instance, This paper explores How Standard Rag with ColBERT \cite{khattab2020colbert} re-ranking can refine retrieved documents, while the RAPTOR architecture \cite{wu2021recursively, raptor2024} organizes and summarizes documents hierarchically. Understanding the trade-offs between these methods is key to building robust and scalable RAG systems.

This paper introduces \textit{MyRAG}, an open-source framework designed to compare multiple retrieval and augmented generation strategies systematically. We evaluate embeddings, vector stores, and advanced retrieval enhancements like ColBERT re-ranking and RAPTOR summarization, applying them on the BioASQ \cite{bioasq2023} and Hugging Face Document QA datasets \cite{huggingface2024docqa}. Our experiments aim to provide a clearer understanding of how these components influence retrieval accuracy and end-to-end QA performance.

%----------------- Problem Statement -----------------%
\section{Problem Statement}

While the Massive Text Embedding Benchmark (MTEB) \cite{muennighoff2022mteb} provides valuable insights into embedding model performance across diverse tasks, there remains a critical need for comprehensive evaluation of end-to-end RAG architectures. The current landscape lacks:

\begin{enumerate}
    \item \textbf{Architecture-Specific Evaluation:} Unlike MTEB's focus on embedding quality, MyRAG provides systematic comparison of different RAG architectures - ColBERT reranking \cite{khattab2020colbert}, and RAPTOR hierarchical retrieval \cite{wu2021recursively, raptor2024} - using consistent datasets, metrics, vector stores (Chroma and DeepLake) and quantization strategies.
    
    \item \textbf{Resource-Conscious Assessment:} MyRAG evaluates both 8-bit quantized and full-precision versions of popular embedding models addressing practical deployment considerations not covered by MTEB's leaderboard.
    
    \item \textbf{Intuitive Real-World Metrics:} MyRAG introduces straightforward measures beyond MTEB's technical metrics:
    \begin{itemize}
        \item Correct/Partial/Incorrect answer classification
        \item Multi-document per question retrieval accuracy 
        \item Response generation quality with context
        \item Resource utilization across architectures
    \end{itemize}
\end{enumerate}

Through this comprehensive evaluation framework, MyRAG helps practitioners select optimal combinations of embedding models (guided by MTEB's leaderboard), retrieval architectures, and implementation approaches. The framework supports both detailed technical assessment and simple, interpretable metrics that organizations need to optimize their RAG deployments across use cases and resource constraints. By offering comparison of standard RAG, ColBERT reranking, and RAPTOR approaches on multtiple datasets, MyRAG provides actionable insights for building more effective retrieval-augmented generation systems.


\textit{MyRAG} addresses this need by providing an open-source code framework that enables direct, end-to-end comparisons of RAG approaches. By incorporating multiple embedding models, vector databases (e.g., Chroma and DeepLake), reranking strategies (e.g., ColBERT), and hierarchical retrieval architectures (e.g., RAPTOR), \textit{MyRAG} provides a clear, cohesive platform for empirical evaluation. This approach ensures that even non-experts can understand performance trade-offs through accessible and practical metrics, ultimately guiding practitioners and researchers toward more optimal and tailored RAG solutions.

%----------------- Related Work -----------------%
\section{Related Work}

\textbf{RAG Systems:} Retrieval-Augmented Generation models like RAG \cite{lewis2020retrieval} and REALM \cite{guu2020realm} have demonstrated improved factual accuracy and adaptability. These models typically employ neural or sparse retrieval followed by cross-attention fusion into the LLM. 

\textbf{Embedding Models:} Sentence-BERT \cite{reimers2019sentence} and domain-specific embedding models have enhanced semantic retrieval. Dense Passage Retrieval (DPR) \cite{karpukhin2020dense} and ColBERT \cite{khattab2020colbert} leverage contextualized embeddings for more accurate retrieval.

\textbf{Vector Databases:} Efficient vector search libraries like FAISS \cite{johnson2019billion} and specialized stores such as Chroma and DeepLake facilitate large-scale, low-latency retrieval. Recent advances have combined these with approximate nearest neighbor search algorithms for scalability.

\textbf{Document-Level Reasoning:} RAPTOR \cite{wu2021recursively, raptor2024} introduces recursive summarization and hierarchical retrieval. Similarly, LlamaIndex \cite{liu2022llamaindex} explores summarizing documents at varying levels of abstraction. Such approaches aim to provide more contextually rich retrieval, reducing hallucinations and improving multi-hop reasoning.

MyRAG builds on these works by offering a unified platform to compare multiple embeddings, vector stores, and retrieval strategies, including ColBERT and RAPTOR, on common benchmarks.

%----------------- MyRag System -----------------%
\section{MyRAG}

The MyRAG system provides a flexible pipeline to switch between embedding models, vector stores, and retrieval techniques. Key components include:

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{Raptor.pdf}
	\caption{Flow chart of RAPTOR Architecture as integrated in MyRAG}
	\label{fig:raptor}
\end{figure}

\subsection{Embedding Models}

We integrate several embedding models:
\begin{itemize}
    \item NV-Embed-v2, Stella 1.5B, MXBai Large, Amazon Titan, all-MiniLM-L6-v2
\end{itemize}
These models differ in size, parameters, and performance trade-offs. We evaluate how embedding choice affects retrieval accuracy and QA outcomes.

\subsection{Vector Databases}

MyRAG supports Chroma and DeepLake, two vector databases optimized for similarity search with support for HNSW indices and cosine or L2 metrics. Initial experiments suggest that cosine similarity often performs better, though L2 metrics are also tested for completeness.

\subsection{ColBERT Reranking}

ColBERT \cite{khattab2020colbert} refines initial retrieval results by re-ranking passages based on fine-grained token-level similarities. This step often improves retrieval accuracy by ensuring that top-ranked documents are highly relevant to the query.

\subsection{RAPTOR}

We incorporate RAPTOR \cite{wu2021recursively, raptor2024}, which uses hierarchical summarization to structure large corpora. RAPTOR builds a tree of summaries at increasing levels of abstraction. This architecture allows retrieval at multiple scales—broad thematic overviews down to granular details—enhancing multi-hop QA and reducing context fragmentation.

%----------------- Evaluations and Results -----------------%
\section{Evaluations and Results}

\subsection{Datasets}

MyRAG was evaluated on two datasets:
\begin{itemize}
    \item \textbf{BioASQ:} Biomedical QA dataset \cite{bioasq2023} focusing on factual retrieval and precise identification of evidence.
    \item \textbf{Hugging Face Document QA:} A collection of long documents and associated queries \cite{huggingface2024docqa}, stressing reasoning across multiple segments of text.
\end{itemize}

\subsection{Retrieval Accuracies}

We measure retrieval accuracy at various $k$ values with and without re-ranking, and also compare RAG performance on final QA metrics. Table \ref{tab:retrieval_accuracies} shows retrieval accuracies at different $k$ values. On larger $k$ values

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c c}
\hline
\textbf{Embedding Model} & \textbf{Dataset} & \textbf{k=1} & \textbf{k=2} & \textbf{k=3} & \textbf{k=5} & \textbf{k=8} & \textbf{k=10} \\
\hline


\multirow{2}{*}{\texttt{NV-Embed-v2 (8-bit)}} 
 & HF QA  & 0.91 & 0.94 & 0.98 & 0.98 & 1.00 & 1.00 \\
 & BioASQ & 1.00 & 0.81 & 0.74 & 0.77 & 0.80 & 0.82 \\
\hline

\multirow{2}{*}{\texttt{all-MiniLM-L6-v2}} 
 & HF QA  & 0.57 & 0.72 & 0.74 & 0.77 & 0.82 & 0.83 \\
 & BioASQ & 0.62 & 0.58 & 0.51 & 0.57 & 0.57 & 0.61 \\
\hline

\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & HF QA  & 0.43 & 0.60 & 0.69 & 0.78 & 0.80 & 0.82 \\
 & BioASQ & 1.00 & 0.92 & 0.77 & 0.74 & 0.79 & 0.79 \\
\hline

\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & HF QA  & 0.95 & 0.98 & 0.98 & 1.00 & 1.00 & 1.00 \\
 & BioASQ & 1.00 & 0.81 & 0.72 & 0.77 & 0.81 & 0.81 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5 (8-bit)}} 
 & HF QA  & 0.88 & 0.91 & 0.91 & 0.92 & 0.94 & 0.94 \\
 & BioASQ & 0.31 & 0.42 & 0.64 & 0.68 & 0.74 & 0.78 \\
\hline
\multirow{2}{*}{\texttt{titan-embed-text-v2}} 
 & HF QA  & 0.85 & 0.94 & 0.97 & 1.00 & 1.00 & 1.00 \\
 & BioASQ & 1.00 & 0.81 & 0.69 & 0.73 & 0.77 & 0.79 \\
\hline
\end{tabular}
\caption{Top-k Retrieval Accuracy Comparison of Embedding Models}
\end{table}


\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c c}
\hline
\textbf{Embedding Model} & \textbf{Dataset} & \textbf{k=1} & \textbf{k=2} & \textbf{k=3} & \textbf{k=5} & \textbf{k=8} & \textbf{k=10} \\
\hline
\multirow{2}{*}{\texttt{NV-Embed-v2 (8-bit)}}
 & HF QA  & 0.97 & 0.98 & 0.98 & 0.98 & 1.00 & 1.00 \\
 & BioASQ & 1.00 & 0.81 & 0.74 & 0.78 & 0.80 & 0.82 \\
\hline
\multirow{2}{*}{\texttt{all-MiniLM-L6-v2}}
 & HF QA  & 0.88 & 0.89 & 0.89 & 0.91 & 0.91 & 0.91 \\
 & BioASQ & 0.85 & 0.77 & 0.72 & 0.69 & 0.72 & 0.73 \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}}
 & HF QA  & 0.92 & 0.97 & 0.97 & 0.98 & 0.98 & 0.98 \\
 & BioASQ & 1.00 & 0.88 & 0.77 & 0.76 & 0.82 & 0.82 \\
\hline

\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}}
 & HF QA  & 0.95 & 0.98 & 0.98 & 0.98 & 0.98 & 1.00 \\
 & BioASQ & 1.00 & 0.85 & 0.72 & 0.73 & 0.79 & 0.79 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5 (8-bit)}}
 & HF QA  & 0.77 & 0.77 & 0.77 & 0.78 & 0.78 & 0.82 \\
 & BioASQ & 0.92 & 0.81 & 0.72 & 0.68 & 0.70 & 0.72 \\
\hline
\multirow{2}{*}{\texttt{titan-embed-text-v2}} 
 & HF QA & 0.92 & 0.95 & 0.95 & 0.95 & 0.97 & 0.98 \\
 & BioASQ & 1.00 & 0.85 & 0.74 & 0.73 & 0.77 & 0.79 \\
\hline
\end{tabular}
\caption{Top-k Retrieval Accuracy on different of embedding models with ColBert Re-ranking}
\end{table}


\subsection{RAG Performance}

We assess end-to-end performance by feeding retrieved documents into an LLM (e.g., Llama 2, Mixtral, Gemma, or open-source Hugging Face models). Table \ref{tab:comprehensive_results} reports comprehensive QA results. We find that RAPTOR-assisted retrieval outperforms Standard RAG setups, especially on complex multi-hop questions where hierarchical summaries guide the model to more coherent context.

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c c c}
\hline

\textbf{Embedding Model} & \textbf{Dataset} & \textbf{Total} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Partially Correct} & \textbf{Correct (\%)}  \\
\hline
\multirow{2}{*}{\texttt{NV-Embed-v2 (8-bit)}} 
 & HF QA  & 65 & 64 & 1 & 0 & 98.46 \\
 & BioASQ & 13 & 9  & 1 & 3 & 69.23 \\
\hline
\multirow{2}{*}{\texttt{all-MiniLM-L6-v2}} 
 & HF QA  & 65 & 50 & 15 & 0 & 76.92 \\
 & BioASQ & 13 & 6  & 3 & 4 & 46.15  \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & HF QA  & 65 & 62 & 3 & 0 & 95.38 \\
 & BioASQ & 13 & 8  & 1 & 4 & 61.54 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & HF QA  & 65 & 64 & 1 & 0 & 98.46 \\
 & BioASQ & 13 & 9  & 0 & 4 & 69.23 \\
\hline
\multirow{2}{*}{\texttt{titan-embed-text-v2}} 
 & HF QA  & 65 & 63 & 2 & 0 & 96.92 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54\\
\hline
\end{tabular}
\caption{Evaluation Results of Standard RAG with LLM as Claude v3.5 Sonnet by Embedding Models }
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c}
\hline
\textbf{Embedding Model} & \textbf{Dataset} & \textbf{Total} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Partially Correct} & \textbf{Correct (\%)} \\
\hline
\multirow{2}{*}{\texttt{NV-Embed-v2 (8-bit)}} 
 & HF QA  & 65 & 63 & 2 & 0 & 96.92 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54 \\
\hline
\multirow{2}{*}{\texttt{all-MiniLM-L6-v2}} 
 & HF QA  & 65 & 58 & 7 & 0 & 89.23 \\
 & BioASQ & 13 & 5  & 3 & 5 & 38.46 \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & HF QA  & 65 & 63 & 2 & 0 & 96.92 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & HF QA  & 65 & 63 & 2 & 0 & 96.92 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54 \\
\hline
\multirow{2}{*}{\texttt{titan-embed-text-v2}} 
 & HF QA  & 65 & 62 & 3 & 0 & 95.38 \\
 & BioASQ & 13 & 8  & 0 & 5 & 61.54 \\
\hline
\end{tabular}
\caption{Evaluation of Standard RAG with ColBert Reranking and with LLM as Claude v3.5 Sonnet by Embedding Models}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{l l c c c c c}
\hline
\textbf{Embedding Model} & \textbf{Dataset} & \textbf{Total} & \textbf{Correct} & \textbf{Incorrect} & \textbf{Partially Correct} & \textbf{Correct (\%)} \\
\hline
\multirow{2}{*}{\texttt{mxbai-embed-large-v1}} 
 & HF QA  & 65 & 64 & 1 & 0 & 98.46 \\
 & BioASQ & 13 & 9  & 1 & 3 & 69.23 \\
\hline
\multirow{2}{*}{\texttt{stella\_en\_1.5B\_v5}} 
 & HF QA  & 65 & 65 & 0 & 0 & 100.00 \\
 & BioASQ & 13 & 10 & 0 & 3 & 76.92 \\
\hline
\end{tabular}
\caption{Evaluation Results of Raptor RAG with LLM as Claude v3.5 Sonnet by Embedding Models}
\end{table}



RAPTOR improves handling of long, complex documents by recursively summarizing and clustering information. As shown in Table \ref{tab:model_performance}, RAPTOR-enabled retrieval combinations yield higher correctness, especially on biomedical QA (BioASQ) tasks where detailed reasoning is essential.

%----------------- Challenges and Future Work -----------------%
\section{Challenges and Future Work}

\textbf{Evaluation Metrics:} Current evaluation focuses on document-level accuracy. Future work includes more fine-grained evaluation (e.g., chunk-level), automated scoring methods, and domain expert assessments to better understand retrieval quality in specialized domains.

\textbf{Document Parsing:} MyRAG currently processes textual data. Extending it to multimodal inputs (PDFs with figures, images, or even audio transcripts) would demand integrating vision and ASR models, enabling comprehensive retrieval across diverse data formats.

\textbf{Hybrid Models:} Combining strengths of different models—e.g., hybrid sparse-dense retrievers—could further enhance retrieval accuracy. Additionally, investigating more advanced reranking strategies or model-based QA refinement might yield gains.

\textbf{Multi-agent Systems:} A future direction involves multi-agent LLM architectures where one agent refines user queries and another specializes in retrieval. Iterative query clarification and refinement would ensure that only the most relevant information is retrieved for the LLM to answer.

%----------------- Conclusion -----------------%
\section{Conclusion}

This paper introduced MyRAG, an open-source RAG system that benchmarks and compares a range of retrieval architectures, embeddings, vector databases, and reranking methods. By integrating ColBERT reranking and the RAPTOR hierarchical retrieval architecture, MyRAG provides insights into building scalable and efficient retrieval-augmented generation pipelines. Experiments on BioASQ and Hugging Face Document QA datasets demonstrate that advanced retrieval methods can significantly improve both retrieval accuracy and end-to-end QA performance.

Future efforts will enhance evaluation methods, integrate multimodal capabilities, explore hybrid retrieval models, and design multi-agent query refinement to push the frontier of retrieval-augmented generation systems.

%----------------- References -----------------%
\bibliographystyle{IEEEtran}
\bibliography{references}

\appendix

% Additional details, code excerpts, and extended results can be placed here.
% Appendices for MyRag
%Put hyper parameters tables top k used, settings used , temperature used, instruction used
% Human Evaluation 
% setup Details I used inference of all local models on aws  ec2 instuance g5 2xl and test  
\end{document}